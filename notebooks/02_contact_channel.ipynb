{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdc51d4",
   "metadata": {},
   "source": [
    "# Gi·ªõi thi·ªáu\n",
    "\n",
    "File notebook n√†y thu·ªôc nh√≥m ph√¢n t√≠ch **k√™nh v√† th·ªùi ƒëi·ªÉm li√™n h·ªá** trong chi·∫øn d·ªãch marketing qua ƒëi·ªán tho·∫°i c·ªßa ng√¢n h√†ng B·ªì ƒê√†o Nha (Bank Marketing Dataset ‚Äì Additional Full).\n",
    "\n",
    "**M·ª•c ti√™u ph√¢n t√≠ch:**\n",
    "- Kh√°m ph√° **hi·ªáu qu·∫£ c·ªßa t·ª´ng k√™nh li√™n h·ªá** (`cellular` vs `telephone`) trong vi·ªác thuy·∫øt ph·ª•c kh√°ch h√†ng g·ª≠i ti·∫øt ki·ªám k·ª≥ h·∫°n.  \n",
    "- Ph√¢n t√≠ch **th·ªùi ƒëi·ªÉm g·ªçi** theo **th√°ng (`month`)** v√† **ng√†y trong tu·∫ßn (`day_of_week`)** ƒë·ªÉ x√°c ƒë·ªãnh khi n√†o kh√°ch h√†ng c√≥ xu h∆∞·ªõng ph·∫£n h·ªìi t√≠ch c·ª±c nh·∫•t.  \n",
    "- ƒê√°nh gi√° s·ª± kh√°c bi·ªát v·ªÅ **t·∫ßn su·∫•t v√† k·∫øt qu·∫£ cu·ªôc g·ªçi** nh·∫±m t√¨m ra **l·ªãch g·ªçi t·ªëi ∆∞u**, gi√∫p **gi·∫£m chi ph√≠ v√† tƒÉng t·ª∑ l·ªá chuy·ªÉn ƒë·ªïi**.\n",
    "\n",
    "**Ngu·ªìn d·ªØ li·ªáu:**  \n",
    "T·ª´ t·∫≠p `bank-additional-full.csv` (41.188 quan s√°t) do UCI Machine Learning Repository cung c·∫•p, m√¥ t·∫£ chi ti·∫øt trong proposal c·ªßa nh√≥m:contentReference[oaicite:1]{index=1}.  \n",
    "C√°c bi·∫øn ch√≠nh ƒë∆∞·ª£c s·ª≠ d·ª•ng trong file n√†y g·ªìm:\n",
    "- `contact`: k√™nh li√™n h·ªá (cellular / telephone)  \n",
    "- `month`: th√°ng g·ªçi  \n",
    "- `day_of_week`: ng√†y g·ªçi  \n",
    "- `duration`, `campaign`, `pdays`, `previous`: ƒë·∫∑c tr∆∞ng h√†nh vi cu·ªôc g·ªçi  \n",
    "- `y`: bi·∫øn m·ª•c ti√™u (kh√°ch h√†ng ƒë·ªìng √Ω ‚Äúyes‚Äù ho·∫∑c t·ª´ ch·ªëi ‚Äúno‚Äù)\n",
    "\n",
    "**K·∫øt qu·∫£ mong ƒë·ª£i:**  \n",
    "X√°c ƒë·ªãnh ƒë∆∞·ª£c:\n",
    "- K√™nh li√™n h·ªá hi·ªáu qu·∫£ nh·∫•t.  \n",
    "- Th√°ng v√† ng√†y c√≥ t·ª∑ l·ªá ‚Äúyes‚Äù cao nh·∫•t.  \n",
    "- M·ªëi quan h·ªá gi·ªØa t·∫ßn su·∫•t g·ªçi v√† kh·∫£ nƒÉng th√†nh c√¥ng.\n",
    "\n",
    "Ph·∫ßn sau s·∫Ω ti·∫øn h√†nh **l√†m s·∫°ch d·ªØ li·ªáu** v√† **kh√°m ph√° ph√¢n b·ªë c√°c bi·∫øn** tr∆∞·ªõc khi ph√¢n t√≠ch s√¢u.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a41122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init spark\n",
    "!pip install -q pyspark findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"analytics_data\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d6a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gi·∫£i ph√≥ng cache/persist c≈© (train/val/test) n·∫øu c√≤n trong b·ªô nh·ªõ\n",
    "for name in [\"train_ready\", \"val_ready\", \"test_ready\"]:\n",
    "    if name in globals():\n",
    "        df = globals()[name]\n",
    "        try:\n",
    "            df.unpersist()\n",
    "            print(f\"ƒê√£ unpersist {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Kh√¥ng th·ªÉ unpersist {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d395c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi·∫øn hi·ªán c√≥ trong globals():\n",
      "[]\n",
      "\n",
      "Cache hi·ªán c√≥ trong Spark:\n",
      "train_ready ch∆∞a t·ªìn t·∫°i trong Spark catalog.\n",
      "val_ready ch∆∞a t·ªìn t·∫°i trong Spark catalog.\n",
      "test_ready ch∆∞a t·ªìn t·∫°i trong Spark catalog.\n",
      "\n",
      "ƒê√£ d·ªçn to√†n b·ªô cache Spark.\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra c√°c bi·∫øn DataFrame ƒë√£ t·ªìn t·∫°i trong m√¥i tr∆∞·ªùng Python (global scope)\n",
    "print(\"Bi·∫øn hi·ªán c√≥ trong globals():\")\n",
    "print([v for v in [\"train_ready\", \"val_ready\", \"test_ready\"] if v in globals()])\n",
    "\n",
    "# Ki·ªÉm tra cache trong Spark (c·∫•p cluster)\n",
    "print(\"\\nCache hi·ªán c√≥ trong Spark:\")\n",
    "for name in [\"train_ready\", \"val_ready\", \"test_ready\"]:\n",
    "    try:\n",
    "        if spark.catalog.isCached(name):\n",
    "            print(f\"{name} ƒëang ƒë∆∞·ª£c cache trong Spark.\")\n",
    "        else:\n",
    "            print(f\"{name} ch∆∞a ƒë∆∞·ª£c cache trong Spark.\")\n",
    "    except:\n",
    "        print(f\"{name} ch∆∞a t·ªìn t·∫°i trong Spark catalog.\")\n",
    "\n",
    "# D·ªçn to√†n b·ªô cache Spark\n",
    "spark.catalog.clearCache()\n",
    "print(\"\\nƒê√£ d·ªçn to√†n b·ªô cache Spark.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920d805",
   "metadata": {},
   "source": [
    "### 0. L√†m s·∫°ch d·ªØ li·ªáu v√† ki·ªÉm tra ph√¢n b·ªë\n",
    "\n",
    "B·ªô d·ªØ li·ªáu ‚ÄúBank Marketing ‚Äì Additional Full‚Äù ch·ª©a th√¥ng tin 41.188 kh√°ch h√†ng ƒë∆∞·ª£c g·ªçi ƒëi·ªán trong c√°c chi·∫øn d·ªãch marketing c·ªßa m·ªôt ng√¢n h√†ng B·ªì ƒê√†o Nha (2008‚Äì2010).  \n",
    "M·ª•c ti√™u l√† d·ª± ƒëo√°n kh·∫£ nƒÉng **kh√°ch h√†ng ƒë·ªìng √Ω g·ª≠i ti·∫øt ki·ªám k·ª≥ h·∫°n (y = yes/no)** d·ª±a tr√™n th√¥ng tin li√™n h·ªá, chi·∫øn d·ªãch v√† b·ªëi c·∫£nh.\n",
    "\n",
    "·ªû ph·∫ßn n√†y, ta s·∫Ω:\n",
    "- Ki·ªÉm tra schema, s·ªë l∆∞·ª£ng d√≤ng, v√† gi√° tr·ªã null.  \n",
    "- Th·ªëng k√™ ph√¢n b·ªë s∆° b·ªô c·ªßa c√°c bi·∫øn ph√¢n lo·∫°i v√† bi·∫øn s·ªë.  \n",
    "- Ph√°t hi·ªán outlier, d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng, v√† chu·∫©n b·ªã cho EDA chi ti·∫øt ·ªü c√°c ph·∫ßn sau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6cb9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+--------+--------+-----+--------+---+\n",
      "|contact  |month|day_of_week|duration|campaign|pdays|previous|y  |\n",
      "+---------+-----+-----------+--------+--------+-----+--------+---+\n",
      "|telephone|may  |mon        |261     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |149     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |226     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |151     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |307     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |198     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |139     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |217     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |380     |1       |999  |0       |no |\n",
      "|telephone|may  |mon        |50      |1       |999  |0       |no |\n",
      "+---------+-----+-----------+--------+--------+-----+--------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/bank-additional/bank-additional-full.csv\"\n",
    "raw_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferschema\", True)\n",
    "    .option(\"sep\", \";\")\n",
    "    .csv(path)\n",
    ")\n",
    "\n",
    "cols = ['contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous']\n",
    "target_col = 'y'\n",
    "df = raw_df.select(*(cols + [target_col]))\n",
    "\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb0c42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema df: \n",
      "root\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "__________________________________________\n",
      "\n",
      " So dong df: \n",
      "41188\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"schema df: \")\n",
    "df.printSchema()\n",
    "print(\"__________________________________________\")\n",
    "\n",
    "print(\"\\n So dong df: \")\n",
    "print(df.count())\n",
    "print(\"__________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8bca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking missing values: \n",
      "contact              null = 0\n",
      "month                null = 0\n",
      "day_of_week          null = 0\n",
      "duration             null = 0\n",
      "campaign             null = 0\n",
      "pdays                null = 0\n",
      "previous             null = 0\n",
      "y                    null = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking missing values: \")\n",
    "total = df.count()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "for c in df.columns:\n",
    "    miss = df.filter(F.col(c).isNull()).count()\n",
    "    print(f\"{c:20s} null = {miss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a38e3",
   "metadata": {},
   "source": [
    "### * T·ª∑ l·ªá ph√¢n b·ªë gi·ªØa c√°c bi·∫øn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5f67e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+\n",
      "|  contact|count|ratio (%)|\n",
      "+---------+-----+---------+\n",
      "| cellular|26144|    63.47|\n",
      "|telephone|15044|    36.53|\n",
      "+---------+-----+---------+\n",
      "\n",
      "+-----+-----+---------+\n",
      "|month|count|ratio (%)|\n",
      "+-----+-----+---------+\n",
      "|  may|13769|    33.43|\n",
      "|  jul| 7174|    17.42|\n",
      "|  aug| 6178|     15.0|\n",
      "|  jun| 5318|    12.91|\n",
      "|  nov| 4101|     9.96|\n",
      "|  apr| 2632|     6.39|\n",
      "|  oct|  718|     1.74|\n",
      "|  sep|  570|     1.38|\n",
      "|  mar|  546|     1.33|\n",
      "|  dec|  182|     0.44|\n",
      "+-----+-----+---------+\n",
      "\n",
      "+-----------+-----+---------+\n",
      "|day_of_week|count|ratio (%)|\n",
      "+-----------+-----+---------+\n",
      "|        thu| 8623|    20.94|\n",
      "|        mon| 8514|    20.67|\n",
      "|        wed| 8134|    19.75|\n",
      "|        tue| 8090|    19.64|\n",
      "|        fri| 7827|     19.0|\n",
      "+-----------+-----+---------+\n",
      "\n",
      "+---+-----+---------+\n",
      "|  y|count|ratio (%)|\n",
      "+---+-----+---------+\n",
      "| no|36548|    88.73|\n",
      "|yes| 4640|    11.27|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cat_cols = ['contact', 'month', 'day_of_week', 'y']\n",
    "for c in cat_cols:\n",
    "    fre_val = df.groupBy(c).count().orderBy('count', ascending = False)\n",
    " \n",
    "    total = df.count()\n",
    "    col_expr = ((F.col('count') / total) * 100).cast(\"double\")\n",
    "    fre_val.withColumn(\"ratio (%)\", F.round(col_expr, 2)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111de2d5",
   "metadata": {},
   "source": [
    "#### Nh·∫≠n x√©t ph√¢n t√≠ch t·∫ßn su·∫•t c√°c bi·∫øn ph√¢n lo·∫°i\n",
    "\n",
    "#####  `contact` ‚Äî K√™nh li√™n h·ªá\n",
    "T·ª∑ l·ªá **cellular (63.5%)** cao h∆°n **telephone (36.5%)**.  \n",
    "ƒêi·ªÅu n√†y ph·∫£n √°nh ƒë√∫ng th·ª±c t·∫ø giai ƒëo·∫°n 2008‚Äì2010, khi ng√¢n h√†ng chuy·ªÉn d·∫ßn sang g·ªçi qua **di ƒë·ªông** ƒë·ªÉ ti·∫øp c·∫≠n kh√°ch h√†ng nhanh h∆°n.  \n",
    "\n",
    "Khi ph√¢n t√≠ch hi·ªáu qu·∫£ chi·∫øn d·ªãch, c·∫ßn ki·ªÉm tra xem **t·ª∑ l·ªá ‚Äúyes‚Äù** gi·ªØa hai k√™nh n√†y c√≥ kh√°c bi·ªát ƒë√°ng k·ªÉ kh√¥ng.  \n",
    "N·∫øu di ƒë·ªông c√≥ t·ª∑ l·ªá th√†nh c√¥ng cao h∆°n, ƒë√≥ l√† t√≠n hi·ªáu cho vi·ªác **∆∞u ti√™n k√™nh li√™n h·ªá** n√†y trong c√°c chi·∫øn d·ªãch sau.\n",
    "\n",
    "\n",
    "\n",
    "##### `month` ‚Äî Th√°ng g·ªçi\n",
    "C√°c th√°ng c√≥ nhi·ªÅu cu·ªôc g·ªçi nh·∫•t:\n",
    "- **May (33%)**\n",
    "- **July (17%)**\n",
    "- **August (15%)**\n",
    "- **June (13%)**\n",
    "\n",
    "‚Üí Chi·∫øm h∆°n **75% t·ªïng s·ªë cu·ªôc g·ªçi**.  \n",
    "C√≥ th·ªÉ ƒë√¢y l√† **chi·∫øn d·ªãch m√πa h√®**, khi ng√¢n h√†ng ƒë·∫©y m·∫°nh huy ƒë·ªông v·ªën ho·∫∑c tung s·∫£n ph·∫©m m·ªõi.\n",
    "\n",
    "C√°c th√°ng kh√°c (Oct, Sep, Mar, Dec) c√≥ t·ª∑ l·ªá r·∫•t nh·ªè ‚Üí th∆∞·ªùng l√† **off-campaign** ho·∫∑c **chi·∫øn d·ªãch th·ª≠ nghi·ªám**.  \n",
    "N√™n ki·ªÉm tra th√™m xem **t·ª∑ l·ªá ‚Äúyes‚Äù c√≥ bi·∫øn ƒë·ªông theo m√πa** kh√¥ng; v√≠ d·ª•: th√°ng May g·ªçi nhi·ªÅu nh∆∞ng hi·ªáu qu·∫£ c√≥ th·ªÉ kh√¥ng cao.\n",
    "\n",
    "\n",
    "\n",
    "#####  `day_of_week` ‚Äî Ng√†y g·ªçi\n",
    "Ph√¢n b·ªë **kh√° ƒë·ªìng ƒë·ªÅu (kho·∫£ng 19‚Äì21%)** m·ªói ng√†y.  \n",
    "ƒêi·ªÅu n√†y cho th·∫•y ng√¢n h√†ng tri·ªÉn khai chi·∫øn d·ªãch ƒë·ªÅu trong tu·∫ßn, **kh√¥ng t·∫≠p trung ri√™ng v√†o ƒë·∫ßu ho·∫∑c cu·ªëi tu·∫ßn**.\n",
    "\n",
    "ƒê√¢y l√† ƒë·∫∑c ƒëi·ªÉm t·ªët, gi√∫p **lo·∫°i b·ªè bias theo ng√†y** khi hu·∫•n luy·ªán m√¥ h√¨nh.  \n",
    "Tuy nhi√™n, v·∫´n n√™n xem th·ª≠ **th·ª© S√°u** c√≥ t·ª∑ l·ªá ‚Äúyes‚Äù cao h∆°n kh√¥ng ‚Äî v√¨ kh√°ch h√†ng cu·ªëi tu·∫ßn c√≥ th·ªÉ t√¢m l√Ω tho·∫£i m√°i h∆°n.\n",
    "\n",
    "\n",
    "\n",
    "#####  `y` ‚Äî Bi·∫øn m·ª•c ti√™u (k·∫øt qu·∫£)\n",
    "D·ªØ li·ªáu **r·∫•t m·∫•t c√¢n b·∫±ng**:  \n",
    "- **Yes:** 11.3%  \n",
    "- **No:** 88.7%\n",
    "\n",
    "ƒê√¢y l√† ƒë·∫∑c tr∆∞ng n·ªïi ti·∫øng c·ªßa **Bank Marketing Dataset**.  \n",
    "H·ªá qu·∫£: n·∫øu hu·∫•n luy·ªán m√¥ h√¨nh m√† **kh√¥ng x·ª≠ l√Ω imbalance**, m√¥ h√¨nh s·∫Ω thi√™n v·ªÅ d·ª± ƒëo√°n ‚Äúno‚Äù.\n",
    "\n",
    "‚Üí ·ªû b∆∞·ªõc m√¥ h√¨nh h√≥a, c·∫ßn **c√¢n b·∫±ng l·ªõp** b·∫±ng:\n",
    "- `class_weighting`\n",
    "- `SMOTE`\n",
    "- ho·∫∑c `resampling`\n",
    "\n",
    "ƒë·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c t√≠n hi·ªáu th·ª±c s·ª± c·ªßa nh√≥m ‚Äúyes‚Äù.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d593f196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-------------------+\n",
      "|summary|          duration|         campaign|            pdays|           previous|\n",
      "+-------+------------------+-----------------+-----------------+-------------------+\n",
      "|  count|             41188|            41188|            41188|              41188|\n",
      "|   mean| 258.2850101971448|2.567592502670681|962.4754540157328|0.17296299893172767|\n",
      "| stddev|259.27924883646455|2.770013542902331|186.9109073447414|0.49490107983928927|\n",
      "|    min|                 0|                1|                0|                  0|\n",
      "|    max|              4918|               56|              999|                  7|\n",
      "+-------+------------------+-----------------+-----------------+-------------------+\n",
      "\n",
      "[[102.0, 177.0, 312.0, 526.0, 702.0, 4918.0], [1.0, 2.0, 3.0, 5.0, 6.0, 56.0], [999.0, 999.0, 999.0, 999.0, 999.0, 999.0], [0.0, 0.0, 0.0, 1.0, 1.0, 7.0]] \n",
      "\n",
      "Percentile cua duration: \n",
      "{'25%': 102.0, '50%': 177.0, '75%': 312.0, '90%': 526.0, '95%': 702.0, '99%': 4918.0} \n",
      "\n",
      "Percentile cua campaign: \n",
      "{'25%': 1.0, '50%': 2.0, '75%': 3.0, '90%': 5.0, '95%': 6.0, '99%': 56.0} \n",
      "\n",
      "Percentile cua pdays: \n",
      "{'25%': 999.0, '50%': 999.0, '75%': 999.0, '90%': 999.0, '95%': 999.0, '99%': 999.0} \n",
      "\n",
      "Percentile cua previous: \n",
      "{'25%': 0.0, '50%': 0.0, '75%': 0.0, '90%': 1.0, '95%': 1.0, '99%': 7.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Kiem tra tan suat phan bo cua cac bien so\n",
    "num_cols = ['duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "df.select(num_cols).describe().show()\n",
    "\n",
    "percentiles = df.approxQuantile(num_cols, [0.25, 0.5, 0.75, 0.9, 0.95, 0.99], 0.01)\n",
    "print(percentiles, \"\\n\")\n",
    "\n",
    "\n",
    "for i, c in enumerate(num_cols):\n",
    "    res = dict(zip([\"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\"], percentiles[i]))\n",
    "    print(f\"Percentile cua {c}: \")\n",
    "    print(res, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5043bfe",
   "metadata": {},
   "source": [
    "#### Nh·∫≠n x√©t ph√¢n t√≠ch c√°c bi·∫øn s·ªë\n",
    "\n",
    "##### a. `duration` ‚Äî Th·ªùi l∆∞·ª£ng cu·ªôc g·ªçi\n",
    "- Trung b√¨nh kho·∫£ng **258 gi√¢y**, ƒë·ªô l·ªách chu·∫©n g·∫ßn b·∫±ng trung b√¨nh ‚Üí **ph√¢n b·ªë r·∫•t l·ªách ph·∫£i (right-skewed)**.  \n",
    "- **75%** cu·ªôc g·ªçi k√©o d√†i ‚â§ **312 gi√¢y**, nh∆∞ng **1% tr√™n c√πng** l√™n t·ªõi g·∫ßn **5000 gi√¢y (~82 ph√∫t)** ‚Üí c√≥ **outlier m·∫°nh**.  \n",
    "- ƒêi·ªÅu n√†y h·ª£p l√Ω, v√¨ ƒëa s·ªë kh√°ch h√†ng t·ª´ ch·ªëi s·ªõm; c√≤n c√°c cu·ªôc g·ªçi d√†i th∆∞·ªùng ƒë·∫øn t·ª´ kh√°ch h√†ng c√≥ h·ª©ng th√∫ ho·∫∑c nh√¢n vi√™n thuy·∫øt ph·ª•c l√¢u.  \n",
    "- Tuy nhi√™n, nh∆∞ ƒë√£ n√≥i ·ªü ph·∫ßn tr∆∞·ªõc, `duration` **kh√¥ng n√™n d√πng ƒë·ªÉ d·ª± ƒëo√°n tr·ª±c ti·∫øp**, v√¨ ƒë√¢y l√† **k·∫øt qu·∫£ sau khi g·ªçi**, kh√¥ng ph·∫£i th√¥ng tin bi·∫øt tr∆∞·ªõc.  \n",
    "  ‚Üí D√πng trong **EDA (ph√¢n t√≠ch kh√°m ph√° d·ªØ li·ªáu)** ƒë·ªÉ hi·ªÉu h√†nh vi kh√°ch h√†ng l√† h·ª£p l√Ω.\n",
    "\n",
    "\n",
    "\n",
    "##### b. `campaign` ‚Äî S·ªë l·∫ßn li√™n h·ªá trong chi·∫øn d·ªãch hi·ªán t·∫°i\n",
    "- **Trung v·ªã = 2**, **75% ‚â§ 3**, nh∆∞ng **1% cao nh·∫•t** l√™n t·ªõi **56 l·∫ßn** ‚Üí c√≥ nh·ªØng kh√°ch h√†ng b·ªã g·ªçi **h∆°n 50 l·∫ßn (!)**  \n",
    "- ƒê√¢y l√† **outlier r√µ r√†ng**, nh∆∞ng l·∫°i ch·ª©a **th√¥ng tin h√†nh vi**: chi·∫øn d·ªãch ƒë√£ c·ªë g·∫Øng ‚Äúƒëu·ªïi theo‚Äù kh√°ch h√†ng ƒë√≥.  \n",
    "- Khi m√¥ h√¨nh h√≥a, n√™n:\n",
    "  - **Gi·ªõi h·∫°n (clip)** gi√° tr·ªã t·ªëi ƒëa ·ªü m·ªôt ng∆∞·ª°ng h·ª£p l√Ω (v√≠ d·ª• 10), ho·∫∑c  \n",
    "  - **Log-transform** ƒë·ªÉ gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa c√°c gi√° tr·ªã c·ª±c l·ªõn.\n",
    "\n",
    "\n",
    "\n",
    "##### c. `pdays` ‚Äî S·ªë ng√†y t·ª´ l·∫ßn li√™n h·ªá tr∆∞·ªõc\n",
    "- T·∫•t c·∫£ c√°c percentile ƒë·ªÅu = **999**, cho th·∫•y **ph·∫ßn l·ªõn kh√°ch h√†ng ch∆∞a t·ª´ng ƒë∆∞·ª£c li√™n h·ªá tr∆∞·ªõc ƒë√¢y**.  \n",
    "- Theo m√¥ t·∫£ d·ªØ li·ªáu, **999 nghƒ©a l√† ‚Äúno previous contact‚Äù**.  \n",
    "- Bi·∫øn n√†y √≠t th√¥ng tin tr·ª±c ti·∫øp, n√™n khi x·ª≠ l√Ω c√≥ th·ªÉ:\n",
    "  - T·∫°o bi·∫øn nh·ªã ph√¢n m·ªõi: `has_contact_before = (pdays != 999)`, ho·∫∑c  \n",
    "  - Gi·ªØ nguy√™n 999 v√† ƒë·ªÉ m√¥ h√¨nh t·ª± h·ªçc (t√πy thu·∫≠t to√°n).\n",
    "\n",
    "\n",
    "\n",
    "##### d. `previous` ‚Äî S·ªë l·∫ßn li√™n h·ªá tr∆∞·ªõc chi·∫øn d·ªãch hi·ªán t·∫°i\n",
    "- **75% kh√°ch h√†ng ch∆∞a t·ª´ng ƒë∆∞·ª£c li√™n h·ªá (0)**, **95% ‚â§ 1**, ch·ªâ v√†i ng∆∞·ªùi ƒë·∫øn **7 l·∫ßn**.  \n",
    "- Nghƒ©a l√† **ƒëa s·ªë l√† kh√°ch h√†ng m·ªõi**.  \n",
    "- Bi·∫øn n√†y c√≥ **ph√¢n b·ªë r·∫•t l·ªách (one-sided)**, n√™n khi ƒë∆∞a v√†o m√¥ h√¨nh c·∫ßn:\n",
    "  - **Chu·∫©n h√≥a (scaling)** ho·∫∑c  \n",
    "  - **Binning (ph√¢n nh√≥m)** ƒë·ªÉ m√¥ h√¨nh d·ªÖ h·ªçc v√† ·ªïn ƒë·ªãnh h∆°n.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae925b0d",
   "metadata": {},
   "source": [
    "### 1. Insight v·ªÅ h√†nh vi li√™n h·ªá (k√™nh ‚Äì th·ªùi ƒëi·ªÉm ‚Äì k·∫øt qu·∫£):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd448e",
   "metadata": {},
   "source": [
    "#### 1.1 V·ªÅ k√™nh li√™n h·ªá \n",
    "‚Üí M·ª•c ti√™u: t√¨m hi·ªÉu k√™nh v√† th·ªùi ƒëi·ªÉm n√†o hi·ªáu qu·∫£ nh·∫•t.\n",
    "**C√¢u h·ªèi khai th√°c:**\n",
    "\n",
    "- K√™nh cellular c√≥ gi√∫p tƒÉng t·ª∑ l·ªá ph·∫£n h·ªìi t√≠ch c·ª±c kh√¥ng?\n",
    "\n",
    "- Li·ªáu telephone c√≥ th·ªÉ b·ªã lo·∫°i b·ªè ho·∫∑c gi·∫£m t·∫ßn su·∫•t ƒë·ªÉ ti·∫øt ki·ªám chi ph√≠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e0551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ty le kh√°ch hang say yes trong cellular la:14.74% tren tong so 26144 \n",
      "\n",
      "ty le  khach hang say yes khi su dung telephone la:5.23% tren tong so 15044 \n"
     ]
    }
   ],
   "source": [
    "#T·ª∑ l·ªá kh√°ch h√†ng \"yes\" c·ªßa t·ª´ng k√™nh\n",
    "cellular_total = df.filter(df['contact'] == 'cellular').count()\n",
    "yes_cellular = df.filter((df['contact'] == 'cellular') & (df['y'] == 'yes')).count()\n",
    "cellular_ratio = round(yes_cellular / cellular_total * 100, 2)\n",
    "print(f\"Ty le kh√°ch hang say yes trong cellular la:{cellular_ratio}% tren tong so {cellular_total} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "telephone_total = df.filter(df['contact'] == 'telephone').count()\n",
    "yes_telephone = df.filter((df['contact'] == 'telephone') & (df['y'] == 'yes')).count()\n",
    "telephone_ratio = round(yes_telephone / telephone_total * 100, 2)\n",
    "print(f\"ty le  khach hang say yes khi su dung telephone la:{telephone_ratio}% tren tong so {telephone_total} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635e7a6",
   "metadata": {},
   "source": [
    "Ph√¢n t√≠ch cho th·∫•y **k√™nh di ƒë·ªông (cellular)** c√≥ t·ª∑ l·ªá kh√°ch h√†ng ƒë·ªìng √Ω g·ª≠i ti·∫øt ki·ªám **cao g·∫•p g·∫ßn 3 l·∫ßn** so v·ªõi **ƒëi·ªán tho·∫°i b√†n (telephone)**.\n",
    "\n",
    "- **Cellular:** 14.74% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **26,144 cu·ªôc g·ªçi**.  \n",
    "- **Telephone:** 5.23% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **15,044 cu·ªôc g·ªçi**.\n",
    "\n",
    "üëâ **K·∫øt lu·∫≠n:**  \n",
    "K√™nh **di ƒë·ªông** cho th·∫•y **hi·ªáu qu·∫£ v∆∞·ª£t tr·ªôi**, c√≥ th·ªÉ do kh√°ch h√†ng **d·ªÖ ti·∫øp c·∫≠n h∆°n** v√† **ph·∫£n h·ªìi nhanh h∆°n**.  \n",
    "**ƒê·ªÅ xu·∫•t:** Ng√¢n h√†ng n√™n **∆∞u ti√™n ng√¢n s√°ch v√† nh√¢n s·ª± cho k√™nh di ƒë·ªông**, ƒë·ªìng th·ªùi **gi·∫£m t·∫ßn su·∫•t g·ªçi qua ƒëi·ªán tho·∫°i b√†n** ƒë·ªÉ **t·ªëi ∆∞u chi ph√≠ v√† n√¢ng cao t·ª∑ l·ªá chuy·ªÉn ƒë·ªïi**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9034246",
   "metadata": {},
   "source": [
    "#### 1.2 V·ªÅ th√°ng g·ªçi (month)\n",
    "\n",
    "‚Üí **M·ª•c ti√™u:** ki·ªÉm tra xem y·∫øu t·ªë **m√πa v·ª•** c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn t·ª∑ l·ªá kh√°ch h√†ng ƒë·ªìng √Ω (‚Äúyes‚Äù) hay kh√¥ng.\n",
    "\n",
    "**C√¢u h·ªèi khai th√°c:**\n",
    "\n",
    "- C√≥ ph·∫£i **th√°ng May** tuy c√≥ nhi·ªÅu cu·ªôc g·ªçi nh∆∞ng **t·ª∑ l·ªá th√†nh c√¥ng l·∫°i th·∫•p**?\n",
    "\n",
    "- **Th√°ng March** ho·∫∑c **September** c√≥ th·ªÉ l√† ‚Äú**th·ªùi ƒëi·ªÉm v√†ng**‚Äù ‚Äì √≠t cu·ªôc g·ªçi nh∆∞ng **hi·ªáu qu·∫£ cao h∆°n**?\n",
    "\n",
    "- C√≥ **m·ªëi quan h·ªá n√†o gi·ªØa s·ªë l∆∞·ª£ng cu·ªôc g·ªçi v√† t·ª∑ l·ªá th√†nh c√¥ng** (g·ªçi nhi·ªÅu ch∆∞a ch·∫Øc t·ªët h∆°n)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873f7e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-------------+\n",
      "|month|total|yes_count|yes_ratio (%)|\n",
      "+-----+-----+---------+-------------+\n",
      "|  may|13769|      886|         6.43|\n",
      "|  jul| 7174|      649|         9.05|\n",
      "|  nov| 4101|      416|        10.14|\n",
      "|  jun| 5318|      559|        10.51|\n",
      "|  aug| 6178|      655|         10.6|\n",
      "|  apr| 2632|      539|        20.48|\n",
      "|  oct|  718|      315|        43.87|\n",
      "|  sep|  570|      256|        44.91|\n",
      "|  dec|  182|       89|         48.9|\n",
      "|  mar|  546|      276|        50.55|\n",
      "+-----+-----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_df = (\n",
    "    df.groupBy('month')\n",
    "      .agg(\n",
    "          F.count('*').alias('total'),\n",
    "          F.sum((F.col('y') == 'yes').cast('int')).alias('yes_count')\n",
    "      )\n",
    "      .withColumn('yes_ratio (%)', F.round(F.col('yes_count')/F.col('total')* 100, 2))\n",
    "      .orderBy('yes_ratio (%)')\n",
    ")\n",
    "month_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e2eda",
   "metadata": {},
   "source": [
    "Ph√¢n t√≠ch cho th·∫•y c√°c chi·∫øn d·ªãch ƒë∆∞·ª£c tri·ªÉn khai m·∫°nh nh·∫•t v√†o **th√°ng May, July v√† August**, nh∆∞ng **t·ª∑ l·ªá th√†nh c√¥ng l·∫°i kh√° th·∫•p**.  \n",
    "Ng∆∞·ª£c l·∫°i, nh·ªØng th√°ng c√≥ **√≠t cu·ªôc g·ªçi** nh∆∞ **March, September, October v√† December** l·∫°i c√≥ t·ª∑ l·ªá ‚Äúyes‚Äù **r·∫•t cao** ‚Äî th·∫≠m ch√≠ **g·∫•p 5‚Äì8 l·∫ßn so v·ªõi th√°ng May**.\n",
    "\n",
    "- **Th√°ng May:** 6.43% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **13,769 cu·ªôc g·ªçi**.  \n",
    "- **Th√°ng August:** 10.6% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **6,178 cu·ªôc g·ªçi**.  \n",
    "- **Th√°ng April:** 20.48% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **2,632 cu·ªôc g·ªçi**.  \n",
    "- **Th√°ng March:** 50.55% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **546 cu·ªôc g·ªçi**.\n",
    "\n",
    "üëâ **K·∫øt lu·∫≠n:**  \n",
    "Hi·ªáu qu·∫£ chi·∫øn d·ªãch c√≥ **t√≠nh m√πa v·ª• r√µ r·ªát**.  \n",
    "Nh·ªØng th√°ng ng√¢n h√†ng **g·ªçi nhi·ªÅu (ƒë·∫∑c bi·ªát l√† May, July)** l·∫°i c√≥ **t·ª∑ l·ªá ph·∫£n h·ªìi th·∫•p**,  \n",
    "trong khi c√°c th√°ng **√≠t g·ªçi (March, September, October, December)** mang l·∫°i **t·ª∑ l·ªá th√†nh c√¥ng v∆∞·ª£t tr·ªôi**.\n",
    "\n",
    "**ƒê·ªÅ xu·∫•t:**  \n",
    "Ng√¢n h√†ng n√™n **t√°i ph√¢n b·ªï l·ªãch g·ªçi**, **tƒÉng c∆∞·ªùng chi·∫øn d·ªãch** v√†o c√°c th√°ng c√≥ hi·ªáu qu·∫£ cao,  \n",
    "ƒë·ªìng th·ªùi **gi·∫£m t·∫ßn su·∫•t** ·ªü c√°c th√°ng th·∫•p hi·ªáu qu·∫£ nh∆∞ **May‚ÄìJuly** ƒë·ªÉ **n√¢ng cao hi·ªáu su·∫•t t·ªïng th·ªÉ**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80d1f8",
   "metadata": {},
   "source": [
    "#### 1.3 V·ªÅ ng√†y trong tu·∫ßn (day_of_week)\n",
    "\n",
    "‚Üí **M·ª•c ti√™u:** x√°c ƒë·ªãnh **ng√†y n√†o trong tu·∫ßn** mang l·∫°i **t·ª∑ l·ªá kh√°ch h√†ng ƒë·ªìng √Ω cao nh·∫•t**,  \n",
    "t·ª´ ƒë√≥ h·ªó tr·ª£ ng√¢n h√†ng **l√™n l·ªãch g·ªçi t·ªëi ∆∞u** cho ƒë·ªôi ng≈© t∆∞ v·∫•n.\n",
    "\n",
    "**C√¢u h·ªèi khai th√°c:**\n",
    "\n",
    "- Li·ªáu kh√°ch h√†ng c√≥ xu h∆∞·ªõng **ƒë·ªìng √Ω nhi·ªÅu h∆°n v√†o cu·ªëi tu·∫ßn** (th·ª© NƒÉm, th·ª© S√°u) khi **t√¢m l√Ω tho·∫£i m√°i h∆°n**?\n",
    "\n",
    "- C√≥ **s·ª± kh√°c bi·ªát r√µ** gi·ªØa **ƒë·∫ßu tu·∫ßn** v√† **cu·ªëi tu·∫ßn** hay kh√¥ng?\n",
    "\n",
    "- Ng√¢n h√†ng c√≥ th·ªÉ **∆∞u ti√™n g·ªçi v√†o c√°c ng√†y ‚Äúhi·ªáu qu·∫£‚Äù h∆°n** ƒë·ªÉ **tƒÉng t·ª∑ l·ªá chuy·ªÉn ƒë·ªïi**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "985364d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---------+------------+\n",
      "|day_of_week|total|yes_count|yes_ratio(%)|\n",
      "+-----------+-----+---------+------------+\n",
      "|        thu| 8623|     1045|       12.12|\n",
      "|        tue| 8090|      953|       11.78|\n",
      "|        wed| 8134|      949|       11.67|\n",
      "|        fri| 7827|      846|       10.81|\n",
      "|        mon| 8514|      847|        9.95|\n",
      "+-----------+-----+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dayOfWeek_df = (\n",
    "    df.groupBy('day_of_week')\n",
    "      .agg(\n",
    "         F.count('*').alias('total'),\n",
    "         F.sum((F.col('y') == 'yes').cast('int')).alias('yes_count')\n",
    "      )\n",
    "      .withColumn('yes_ratio(%)', F.round((F.col('yes_count')/F.col('total') * 100), 2))\n",
    "      .orderBy('yes_ratio(%)',ascending = False)\n",
    ")\n",
    "dayOfWeek_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa71732",
   "metadata": {},
   "source": [
    "Ph√¢n t√≠ch cho th·∫•y **t·ª∑ l·ªá kh√°ch h√†ng ‚Äúsay yes‚Äù cao nh·∫•t** r∆°i v√†o **th·ª© NƒÉm (12.12%)**,  \n",
    "ti·∫øp theo l√† **th·ª© Ba (11.78%)** v√† **th·ª© T∆∞ (11.67%)**.  \n",
    "Trong khi ƒë√≥, **th·ª© Hai c√≥ t·ª∑ l·ªá th·∫•p nh·∫•t (9.95%)** ‚Äî t·ª©c l√† **ƒë·∫ßu tu·∫ßn kh√°ch h√†ng √≠t ph·∫£n h·ªìi t√≠ch c·ª±c h∆°n**.\n",
    "\n",
    "- **Th·ª© NƒÉm:** 12.12% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **8,623 cu·ªôc g·ªçi**.  \n",
    "- **Th·ª© Ba:** 11.78% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **8,090 cu·ªôc g·ªçi**.  \n",
    "- **Th·ª© Hai:** 9.95% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **8,514 cu·ªôc g·ªçi**.\n",
    "\n",
    "üëâ **K·∫øt lu·∫≠n:**  \n",
    "T·ª∑ l·ªá ph·∫£n h·ªìi t√≠ch c·ª±c c√≥ **xu h∆∞·ªõng tƒÉng d·∫ßn v·ªÅ gi·ªØa v√† cu·ªëi tu·∫ßn**, ƒë·∫°t **ƒë·ªânh v√†o th·ª© NƒÉm**.  \n",
    "ƒêi·ªÅu n√†y g·ª£i √Ω r·∫±ng **th·ªùi ƒëi·ªÉm gi·ªØa ‚Äì cu·ªëi tu·∫ßn** l√† **‚Äúkhung gi·ªù v√†ng‚Äù ƒë·ªÉ tri·ªÉn khai cu·ªôc g·ªçi**,  \n",
    "khi kh√°ch h√†ng c√≥ **t√¢m l√Ω tho·∫£i m√°i** v√† **s·∫µn s√†ng t∆∞∆°ng t√°c h∆°n**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a249a05",
   "metadata": {},
   "source": [
    "### 2: ‚ÄúT·∫ßn su·∫•t & l·ªãch s·ª≠ li√™n h·ªá.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1bf21",
   "metadata": {},
   "source": [
    "#### 2.1. T·∫ßn su·∫•t g·ªçi trong chi·∫øn d·ªãch hi·ªán t·∫°i ‚Äì campaign\n",
    "‚Üí **M·ª•c ti√™u:** xem vi·ªác **g·ªçi nhi·ªÅu h∆°n trong c√πng m·ªôt chi·∫øn d·ªãch** c√≥ l√†m **tƒÉng kh·∫£ nƒÉng kh√°ch h√†ng ‚Äúyes‚Äù** hay kh√¥ng.\n",
    "\n",
    "**C·∫ßn ph√¢n t√≠ch:**\n",
    "\n",
    "- So s√°nh **trung b√¨nh s·ªë l·∫ßn g·ªçi (campaign)** gi·ªØa nh√≥m **‚Äúyes‚Äù** v√† **‚Äúno‚Äù**.  \n",
    "- T√≠nh **t·ª∑ l·ªá ‚Äúyes‚Äù theo nh√≥m t·∫ßn su·∫•t g·ªçi**, v√≠ d·ª•: **1‚Äì2**, **3‚Äì5**, **>5 l·∫ßn**.\n",
    "\n",
    "**C√¢u h·ªèi khai th√°c:**\n",
    "\n",
    "- Li·ªáu **g·ªçi qu√° nhi·ªÅu c√≥ ph·∫£n t√°c d·ª•ng** kh√¥ng?  \n",
    "- C√≥ t·ªìn t·∫°i **ng∆∞·ª°ng ‚Äús·ªë l·∫ßn g·ªçi t·ªëi ∆∞u‚Äù** n√†o cho chi·∫øn d·ªãch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8721039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|  y|avg_calls|\n",
      "+---+---------+\n",
      "| no|     2.63|\n",
      "|yes|     2.05|\n",
      "+---+---------+\n",
      "\n",
      "+--------------+-----+---------+-------------+\n",
      "|campaign_group|total|yes_count|yes_ratio (%)|\n",
      "+--------------+-----+---------+-------------+\n",
      "|          high| 3385|      186|         5.49|\n",
      "|           low|28212|     3511|        12.45|\n",
      "|        medium| 9591|      943|         9.83|\n",
      "+--------------+-----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trung b√¨nh s·ªë l·∫ßn g·ªçi gi·ªØa hai nh√≥m yes / no\n",
    "avg_calls = df.groupBy(\"y\").agg(F.round(F.avg(\"campaign\"), 2).alias(\"avg_calls\"))\n",
    "avg_calls.show()\n",
    "\n",
    "# Ph√¢n nh√≥m t·∫ßn su·∫•t g·ªçi: √≠t - trung b√¨nh - nhi·ªÅu\n",
    "campaign_gr = (\n",
    "    df.withColumn(\n",
    "        'campaign_group',\n",
    "        F.when(F.col('campaign') <= 2, \"low\")\n",
    "        .when(F.col('campaign') <= 5, \"medium\")\n",
    "        .otherwise(\"high\")\n",
    "    )\n",
    "    .groupBy('campaign_group')\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total\"),\n",
    "        F.sum((F.col(\"y\") == \"yes\").cast(\"int\")).alias(\"yes_count\")\n",
    "    )\n",
    "    .withColumn(\"yes_ratio (%)\", F.round(F.col(\"yes_count\") / F.col(\"total\") * 100, 2))\n",
    "    .orderBy(\"campaign_group\")\n",
    ")\n",
    "\n",
    "campaign_gr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4a6df",
   "metadata": {},
   "source": [
    "Ph√¢n t√≠ch cho th·∫•y nh·ªØng kh√°ch h√†ng **ƒë·ªìng √Ω g·ª≠i ti·∫øt ki·ªám (‚Äúyes‚Äù)** ch·ªâ ƒë∆∞·ª£c g·ªçi trung b√¨nh **2.05 l·∫ßn**,  \n",
    "trong khi nh√≥m **t·ª´ ch·ªëi (‚Äúno‚Äù)** ƒë∆∞·ª£c g·ªçi trung b√¨nh **2.63 l·∫ßn**.  \n",
    "ƒêi·ªÅu n√†y g·ª£i √Ω r·∫±ng **g·ªçi qu√° nhi·ªÅu l·∫ßn kh√¥ng gi√∫p tƒÉng hi·ªáu qu·∫£**, th·∫≠m ch√≠ **c√≥ xu h∆∞·ªõng ph·∫£n t√°c d·ª•ng**.\n",
    "\n",
    "**Ph√¢n nh√≥m t·∫ßn su·∫•t g·ªçi:**\n",
    "\n",
    "- **Nh√≥m √≠t (‚â§ 2 l·∫ßn):** t·ª∑ l·ªá th√†nh c√¥ng **12.45%** tr√™n t·ªïng **28,212 cu·ªôc g·ªçi**.  \n",
    "- **Nh√≥m trung b√¨nh (3‚Äì5 l·∫ßn):** t·ª∑ l·ªá th√†nh c√¥ng **9.83%** tr√™n t·ªïng **9,591 cu·ªôc g·ªçi**.  \n",
    "- **Nh√≥m cao (>5 l·∫ßn):** t·ª∑ l·ªá th√†nh c√¥ng ch·ªâ **5.49%** tr√™n t·ªïng **3,385 cu·ªôc g·ªçi**.\n",
    "\n",
    "üëâ **K·∫øt lu·∫≠n:**  \n",
    "T·∫ßn su·∫•t g·ªçi cao **kh√¥ng l√†m tƒÉng t·ª∑ l·ªá ƒë·ªìng √Ω**; ng∆∞·ª£c l·∫°i, **c√†ng g·ªçi nhi·ªÅu kh√°ch h√†ng c√†ng √≠t ph·∫£n h·ªìi t√≠ch c·ª±c**.  \n",
    "ƒêi·ªÅu n√†y cho th·∫•y **nhi·ªÅu kh√°ch h√†ng c√≥ th·ªÉ c·∫£m th·∫•y phi·ªÅn** khi b·ªã li√™n h·ªá l·∫∑p l·∫°i trong c√πng chi·∫øn d·ªãch.\n",
    "\n",
    "**ƒê·ªÅ xu·∫•t:**  \n",
    "Ng√¢n h√†ng n√™n **gi·ªõi h·∫°n s·ªë l·∫ßn g·ªçi t·ªëi ƒëa ·ªü m·ª©c 2‚Äì3 l·∫ßn m·ªói chi·∫øn d·ªãch**,  \n",
    "v√† **chuy·ªÉn tr·ªçng t√¢m sang ch·∫•t l∆∞·ª£ng cu·ªôc g·ªçi thay v√¨ t·∫ßn su·∫•t**,  \n",
    "nh·∫±m **t·ªëi ∆∞u hi·ªáu qu·∫£ v√† gi·∫£m chi ph√≠ nh√¢n s·ª±**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7268fb2",
   "metadata": {},
   "source": [
    "#### 2.2. L·ªãch s·ª≠ li√™n h·ªá ‚Äì `pdays` v√† `has_contact_before`\n",
    "\n",
    "‚Üí **M·ª•c ti√™u:** t√¨m hi·ªÉu **t√°c ƒë·ªông c·ªßa vi·ªác t√°i li√™n h·ªá** v·ªõi kh√°ch h√†ng **ƒë√£ t·ª´ng ƒë∆∞·ª£c g·ªçi tr∆∞·ªõc ƒë√¢y**.\n",
    "\n",
    "**C·∫ßn ph√¢n t√≠ch:**\n",
    "\n",
    "- So s√°nh **t·ª∑ l·ªá ‚Äúyes‚Äù** gi·ªØa hai nh√≥m:  \n",
    "  - **ƒê√£ t·ª´ng ƒë∆∞·ª£c g·ªçi tr∆∞·ªõc** (`pdays ‚â† 999`)  \n",
    "  - **Ch∆∞a t·ª´ng ƒë∆∞·ª£c g·ªçi** (`pdays = 999`)  \n",
    "- Ki·ªÉm tra xem **kho·∫£ng c√°ch gi·ªØa hai l·∫ßn li√™n h·ªá** (`pdays` nh·ªè ‚Üí g·ªçi g·∫ßn ƒë√¢y) c√≥ **·∫£nh h∆∞·ªüng ƒë·∫øn t·ª∑ l·ªá ‚Äúyes‚Äù** hay kh√¥ng.\n",
    "\n",
    "**C√¢u h·ªèi khai th√°c:**\n",
    "\n",
    "- Vi·ªác **g·ªçi l·∫°i** c√≥ gi√∫p **tƒÉng c∆° h·ªôi th√†nh c√¥ng** kh√¥ng?  \n",
    "- N·∫øu c√≥, **sau bao l√¢u g·ªçi l·∫°i** l√† h·ª£p l√Ω?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6150bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+---------+-------------+---------------+\n",
      "|has_contact_before|total|yes_count|yes_ratio (%)|group_ratio (%)|\n",
      "+------------------+-----+---------+-------------+---------------+\n",
      "|                 0|39673|     3673|         9.26|          96.32|\n",
      "|                 1| 1515|      967|        63.83|           3.68|\n",
      "+------------------+-----+---------+-------------+---------------+\n",
      "\n",
      "+-----------+-----+---------+-------------+\n",
      "|pdays_group|total|yes_count|yes_ratio (%)|\n",
      "+-----------+-----+---------+-------------+\n",
      "|   > 5 ng√†y|  810|      522|        64.44|\n",
      "|   ‚â§ 5 ng√†y|  705|      445|        63.12|\n",
      "+-----------+-----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o bi·∫øn ƒë√°nh d·∫•u ƒë√£ t·ª´ng li√™n h·ªá tr∆∞·ªõc ƒë√≥\n",
    "df_pdays = df.withColumn(\"has_contact_before\", (F.col(\"pdays\") != 999).cast(\"int\"))\n",
    "total_count = df_pdays.count()\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng v√† t·ª∑ l·ªá \"yes\" theo nh√≥m li√™n h·ªá\n",
    "pdays_stats = (\n",
    "    df_pdays.groupBy(\"has_contact_before\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total\"),\n",
    "        F.sum((F.col(\"y\") == \"yes\").cast(\"int\")).alias(\"yes_count\")\n",
    "    )\n",
    "    .withColumn(\"yes_ratio (%)\", F.round(F.col(\"yes_count\") / F.col(\"total\") * 100, 2))\n",
    "    .withColumn(\"group_ratio (%)\", F.round(F.col(\"total\") / total_count * 100, 2))\n",
    "    .orderBy(\"has_contact_before\")\n",
    ")\n",
    "pdays_stats.show()\n",
    "\n",
    "# Ch·ªâ gi·ªØ nh·ªØng kh√°ch h√†ng ƒë√£ t·ª´ng ƒë∆∞·ª£c li√™n h·ªá (pdays != 999)\n",
    "df_recontacted = df.filter(F.col(\"pdays\") != 999)\n",
    "\n",
    "# Ph√¢n nh√≥m kho·∫£ng c√°ch g·ªçi l·∫°i\n",
    "pdays_grouped = (\n",
    "    df_recontacted.withColumn(\n",
    "        \"pdays_group\",\n",
    "        F.when(F.col(\"pdays\") <= 5, \"‚â§ 5 ng√†y\")\n",
    "         .otherwise(\"> 5 ng√†y\")\n",
    "    )\n",
    "    .groupBy(\"pdays_group\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total\"),\n",
    "        F.sum((F.col(\"y\") == \"yes\").cast(\"int\")).alias(\"yes_count\")\n",
    "    )\n",
    "    .withColumn(\"yes_ratio (%)\", F.round(F.col(\"yes_count\") / F.col(\"total\") * 100, 2))\n",
    "    .orderBy(\"pdays_group\")\n",
    ")\n",
    "pdays_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ac31a",
   "metadata": {},
   "source": [
    "Ph√¢n t√≠ch cho th·∫•y \n",
    "- **Kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c li√™n h·ªá tr∆∞·ªõc ƒë√≥** c√≥ **t·ª∑ l·ªá ƒë·ªìng √Ω r·∫•t cao (~63.8%)**,  \n",
    "cao g·∫•p nhi·ªÅu l·∫ßn so v·ªõi nh√≥m **ch∆∞a t·ª´ng ƒë∆∞·ª£c g·ªçi (9.3%)**.  \n",
    "\n",
    "- Gi·ªØa nh√≥m **ƒë∆∞·ª£c g·ªçi l·∫°i trong v√≤ng 5 ng√†y** v√† **sau 5 ng√†y**, t·ª∑ l·ªá ‚Äúyes‚Äù g·∫ßn nh∆∞ **t∆∞∆°ng ƒë∆∞∆°ng (63‚Äì64%)**.\n",
    "\n",
    "- Ch·ªâ **3.68%** kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c g·ªçi tr∆∞·ªõc ƒë√≥, nghƒ©a l√† **h∆°n 96%** l√† l·∫ßn ƒë·∫ßu ti√™n ƒë∆∞·ª£c li√™n h·ªá.  ƒêi·ªÅu n√†y x√°c nh·∫≠n chi·∫øn d·ªãch marketing c·ªßa ng√¢n h√†ng ch·ªß y·∫øu l√† **cold call**, kh√¥ng ph·∫£i **chƒÉm s√≥c kh√°ch h√†ng c≈©**.  \n",
    "\n",
    "üëâ **K·∫øt lu·∫≠n:**  \n",
    "**T√°i li√™n h·ªá kh√°ch h√†ng c≈©** l√† m·ªôt **chi·∫øn l∆∞·ª£c hi·ªáu qu·∫£ r√µ r·ªát**,  \n",
    "trong khi **kho·∫£ng c√°ch gi·ªØa hai l·∫ßn g·ªçi kh√¥ng ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ**.\n",
    "\n",
    "**ƒê·ªÅ xu·∫•t:**  \n",
    "Ng√¢n h√†ng n√™n **∆∞u ti√™n x√¢y d·ª±ng chi·∫øn d·ªãch follow-up c√≥ ch·ªçn l·ªçc**  \n",
    "thay v√¨ **t·∫≠p trung qu√° nhi·ªÅu v√†o g·ªçi m·ªõi (cold call)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a956df",
   "metadata": {},
   "source": [
    "#### 2.3. S·ªë l·∫ßn li√™n h·ªá trong c√°c chi·∫øn d·ªãch tr∆∞·ªõc ‚Äì `previous`\n",
    "\n",
    "‚Üí **M·ª•c ti√™u:** ƒë√°nh gi√° xem **l·ªãch s·ª≠ ƒë∆∞·ª£c g·ªçi nhi·ªÅu l·∫ßn tr∆∞·ªõc ƒë√¢y** c√≥ gi√∫p **tƒÉng x√°c su·∫•t kh√°ch h√†ng ‚Äúyes‚Äù** trong chi·∫øn d·ªãch hi·ªán t·∫°i hay kh√¥ng.\n",
    "\n",
    "**C·∫ßn ph√¢n t√≠ch:**\n",
    "\n",
    "- T√≠nh **t·ª∑ l·ªá ‚Äúyes‚Äù theo gi√° tr·ªã `previous`** (0, 1, 2, ‚Ä¶).  \n",
    "- Ki·ªÉm tra xem **kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c li√™n h·ªá 1‚Äì2 l·∫ßn tr∆∞·ªõc ƒë√≥** c√≥ **d·ªÖ ƒë·ªìng √Ω h∆°n** so v·ªõi nh√≥m ch∆∞a t·ª´ng li√™n h·ªá kh√¥ng.\n",
    "\n",
    "**C√¢u h·ªèi khai th√°c:**\n",
    "\n",
    "- ‚Äú**Kh√°ch h√†ng quen thu·ªôc**‚Äù c√≥ th·ª±c s·ª± **ti·ªÅm nƒÉng h∆°n** kh√¥ng?  \n",
    "- C√≥ n√™n **t·∫≠p trung v√†o nh√≥m ƒë√£ li√™n h·ªá nhi·ªÅu l·∫ßn trong qu√° kh·ª©** ƒë·ªÉ tƒÉng hi·ªáu qu·∫£ chi·∫øn d·ªãch?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d979c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+-------------+\n",
      "|previous|total|yes_count|yes_ratio (%)|\n",
      "+--------+-----+---------+-------------+\n",
      "|       0|35563|     3141|         8.83|\n",
      "|       1| 4561|      967|         21.2|\n",
      "|       2|  754|      350|        46.42|\n",
      "|       3|  216|      128|        59.26|\n",
      "|       4|   70|       38|        54.29|\n",
      "|       5|   18|       13|        72.22|\n",
      "|       6|    5|        3|         60.0|\n",
      "|       7|    1|        0|          0.0|\n",
      "+--------+-----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# T√≠nh t·ª∑ l·ªá yes theo s·ªë l·∫ßn ƒë∆∞·ª£c li√™n h·ªá trong c√°c chi·∫øn d·ªãch tr∆∞·ªõc\n",
    "previous_stats = (\n",
    "    df.groupBy(\"previous\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"total\"),\n",
    "          F.sum((F.col(\"y\") == \"yes\").cast(\"int\")).alias(\"yes_count\")\n",
    "      )\n",
    "      .withColumn(\"yes_ratio (%)\", F.round(F.col(\"yes_count\") / F.col(\"total\") * 100, 2))\n",
    "      .orderBy(\"previous\")\n",
    ")\n",
    "\n",
    "previous_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f23f3",
   "metadata": {},
   "source": [
    "Ph√¢n t√≠ch cho th·∫•y **t·ª∑ l·ªá ƒë·ªìng √Ω tƒÉng r√µ r·ªát** theo **s·ªë l·∫ßn kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c li√™n h·ªá trong qu√° kh·ª©**:\n",
    "\n",
    "- **previous = 0:** 8.83% kh√°ch h√†ng ƒë·ªìng √Ω.  \n",
    "- **previous = 1‚Äì2:** 21‚Äì46%, tƒÉng g·∫•p **2‚Äì5 l·∫ßn**.  \n",
    "- **previous ‚â• 3:** ~55‚Äì72%, d√π s·ªë m·∫´u √≠t h∆°n.\n",
    "\n",
    "üëâ **K·∫øt lu·∫≠n:**  \n",
    "**Kh√°ch h√†ng ƒë√£ t·ª´ng ƒë∆∞·ª£c li√™n h·ªá tr∆∞·ªõc ƒë√¢y** l√† nh√≥m **c√≥ ti·ªÅm nƒÉng cao**,  \n",
    "v·ªõi **x√°c su·∫•t ‚Äúyes‚Äù cao h∆°n nhi·ªÅu** so v·ªõi kh√°ch h√†ng m·ªõi.\n",
    "\n",
    "**ƒê·ªÅ xu·∫•t:**  \n",
    "T·∫≠p trung **chƒÉm s√≥c v√† t√°i li√™n h·ªá nh√≥m kh√°ch h√†ng quen thu·ªôc**,  \n",
    "thay v√¨ **d√†n tr·∫£i ngu·ªìn l·ª±c cho nh√≥m ch∆∞a t·ª´ng t∆∞∆°ng t√°c**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752c73d",
   "metadata": {},
   "source": [
    "### 3 ‚Äì Th·ªùi l∆∞·ª£ng v√† h√†nh vi cu·ªôc g·ªçi (duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee0bcf",
   "metadata": {},
   "source": [
    "#### 3.1. Th·ªùi l∆∞·ª£ng v√† h√†nh vi cu·ªôc g·ªçi ‚Äì `duration`\n",
    "\n",
    "‚Üí **M·ª•c ti√™u:** ph√¢n t√≠ch xem **th·ªùi l∆∞·ª£ng cu·ªôc g·ªçi** c√≥ li√™n quan ƒë·∫øn **kh·∫£ nƒÉng kh√°ch h√†ng ƒë·ªìng √Ω (‚Äúyes‚Äù)** hay kh√¥ng,  \n",
    "t·ª´ ƒë√≥ hi·ªÉu r√µ **m·ª©c ƒë·ªô quan t√¢m v√† t∆∞∆°ng t√°c** c·ªßa kh√°ch h√†ng trong qu√° tr√¨nh t∆∞ v·∫•n.\n",
    "\n",
    "**C·∫ßn ph√¢n t√≠ch:**\n",
    "\n",
    "- So s√°nh **th·ªùi l∆∞·ª£ng trung b√¨nh (duration)** gi·ªØa hai nh√≥m **‚Äúyes‚Äù** v√† **‚Äúno‚Äù**.  \n",
    "- T√≠nh **t·ª∑ l·ªá ‚Äúyes‚Äù theo nh√≥m ƒë·ªô d√†i cu·ªôc g·ªçi**: *ng·∫Øn*, *trung b√¨nh*, *d√†i*.\n",
    "\n",
    "**C√¢u h·ªèi khai th√°c:**\n",
    "\n",
    "- Li·ªáu **cu·ªôc g·ªçi k√©o d√†i h∆°n** c√≥ th·ª±c s·ª± **d·∫´n ƒë·∫øn kh·∫£ nƒÉng ƒë·ªìng √Ω cao h∆°n**?  \n",
    "- C√≥ th·ªÉ **x√°c ƒë·ªãnh ng∆∞·ª°ng th·ªùi l∆∞·ª£ng t·ªëi thi·ªÉu** ƒë·ªÉ nh·∫≠n bi·∫øt **cu·ªôc g·ªçi ti·ªÅm nƒÉng th√†nh c√¥ng** kh√¥ng?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c2e343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "|  y|avg_duration_sec|\n",
      "+---+----------------+\n",
      "| no|          220.84|\n",
      "|yes|          553.19|\n",
      "+---+----------------+\n",
      "\n",
      "+--------------------+-----+---------+-------------+\n",
      "|      duration_group|total|yes_count|yes_ratio (%)|\n",
      "+--------------------+-----+---------+-------------+\n",
      "|         d√†i (>300s)|11204|     3122|        27.87|\n",
      "|        ng·∫Øn (‚â§120s)|12917|      166|         1.29|\n",
      "|trung b√¨nh (121‚Äì3...|17067|     1352|         7.92|\n",
      "+--------------------+-----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Th·ªùi l∆∞·ª£ng trung b√¨nh gi·ªØa hai nh√≥m \"yes\" v√† \"no\"\n",
    "duration_avg = (\n",
    "    df.groupBy(\"y\")\n",
    "      .agg(F.round(F.avg(\"duration\"), 2).alias(\"avg_duration_sec\"))\n",
    "      .orderBy(\"y\")\n",
    ")\n",
    "duration_avg.show()\n",
    "\n",
    "# 2) T·ª∑ l·ªá \"yes\" theo nh√≥m ƒë·ªô d√†i cu·ªôc g·ªçi: ng·∫Øn (‚â§120s), trung b√¨nh (121‚Äì300s), d√†i (>300s)\n",
    "duration_bins = (\n",
    "    df.withColumn(\n",
    "        \"duration_group\",\n",
    "        F.when(F.col(\"duration\") <= 120, \"ng·∫Øn (‚â§120s)\")\n",
    "         .when(F.col(\"duration\") <= 300, \"trung b√¨nh (121‚Äì300s)\")\n",
    "         .otherwise(\"d√†i (>300s)\")\n",
    "    )\n",
    "    .groupBy(\"duration_group\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total\"),\n",
    "        F.sum((F.col(\"y\") == \"yes\").cast(\"int\")).alias(\"yes_count\")\n",
    "    )\n",
    "    .withColumn(\"yes_ratio (%)\", F.round(F.col(\"yes_count\")/F.col(\"total\")*100, 2))\n",
    "    .orderBy(\"duration_group\")\n",
    ")\n",
    "\n",
    "duration_bins.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6056e29",
   "metadata": {},
   "source": [
    "**Ph√¢n t√≠ch cho th·∫•y:**  \n",
    "Th·ªùi l∆∞·ª£ng cu·ªôc g·ªçi trung b√¨nh c·ªßa nh√≥m **‚Äúyes‚Äù** cao g·∫•p h∆°n **2 l·∫ßn** so v·ªõi nh√≥m **‚Äúno‚Äù**:\n",
    "\n",
    "- **Nh√≥m ‚Äúno‚Äù:** trung b√¨nh **220.84 gi√¢y**.  \n",
    "- **Nh√≥m ‚Äúyes‚Äù:** trung b√¨nh **553.19 gi√¢y**.\n",
    "\n",
    "**Ph√¢n nh√≥m ƒë·ªô d√†i cu·ªôc g·ªçi:**\n",
    "\n",
    "- **Ng·∫Øn (‚â§120s):** t·ª∑ l·ªá ƒë·ªìng √Ω ch·ªâ **1.29%** tr√™n t·ªïng **12,917 cu·ªôc g·ªçi**.  \n",
    "- **Trung b√¨nh (121‚Äì300s):** t·ª∑ l·ªá tƒÉng l√™n **7.92%** tr√™n **17,067 cu·ªôc g·ªçi**.  \n",
    "- **D√†i (>300s):** t·ª∑ l·ªá ‚Äúyes‚Äù v·ªçt l√™n **27.87%** tr√™n **11,204 cu·ªôc g·ªçi**.\n",
    "\n",
    "üëâ **K·∫øt lu·∫≠n:**  \n",
    "C√°c cu·ªôc g·ªçi k√©o d√†i **h∆°n 5 ph√∫t** c√≥ kh·∫£ nƒÉng th√†nh c√¥ng **cao g·∫•p 3‚Äì5 l·∫ßn** so v·ªõi c√°c cu·ªôc g·ªçi ng·∫Øn.  \n",
    "ƒêi·ªÅu n√†y cho th·∫•y **m·ª©c ƒë·ªô t∆∞∆°ng t√°c v√† th·ªùi gian t∆∞ v·∫•n** l√† **y·∫øu t·ªë quy·∫øt ƒë·ªãnh quan tr·ªçng** cho vi·ªác thuy·∫øt ph·ª•c kh√°ch h√†ng.\n",
    "\n",
    "**ƒê·ªÅ xu·∫•t:**  \n",
    "Ng√¢n h√†ng n√™n **t·∫≠p trung v√†o ch·∫•t l∆∞·ª£ng cu·ªôc g·ªçi h∆°n l√† s·ªë l∆∞·ª£ng**,  \n",
    "khuy·∫øn kh√≠ch nh√¢n vi√™n **duy tr√¨ cu·ªôc tr√≤ chuy·ªán l√¢u h∆°n** v·ªõi **nh·ªØng kh√°ch h√†ng ti·ªÅm nƒÉng**,  \n",
    "thay v√¨ **k·∫øt th√∫c s·ªõm**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f2a11",
   "metadata": {},
   "source": [
    "### 4. Ti·ªÅn x·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f4735",
   "metadata": {},
   "source": [
    "#### 4.1.a ‚Äì Chu·∫©n h√≥a bi·∫øn m·ª•c ti√™u v√† t·∫°o bi·∫øn ph·ª•\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "\n",
    "- Chuy·ªÉn `y` th√†nh d·∫°ng nh·ªã ph√¢n (`0/1`).\n",
    "- T·∫°o bi·∫øn `has_contact_before = 1` n·∫øu `pdays != 999`, `0` n·∫øu ng∆∞·ª£c l·∫°i (ƒë√£ t·ª´ng li√™n h·ªá).\n",
    "- Gi·ªõi h·∫°n gi√° tr·ªã `campaign ‚â§ 10` ƒë·ªÉ tr√°nh outlier c·ª±c l·ªõn (b·∫°n t·ª´ng ph√°t hi·ªán c√≥ ng∆∞·ªùi b·ªã g·ªçi >50 l·∫ßn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2184c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('y_binary', F.when(F.col('y') == 'yes', 1).otherwise(0))\n",
    "\n",
    "df = df.withColumn('has_contact_before', (F.col('pdays') != 999).cast('int'))\n",
    "\n",
    "df = df.withColumn('campaign_capped', F.when(F.col('campaign') > 10, 10).otherwise(F.col('campaign')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c74f4",
   "metadata": {},
   "source": [
    "#### 4.1.b ‚Äì M√£ h√≥a bi·∫øn ph√¢n lo·∫°i\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "\n",
    "- Chuy·ªÉn c√°c bi·∫øn d·∫°ng ch·ªØ (string) sang d·∫°ng s·ªë ƒë·ªÉ m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c.  \n",
    "- Trong dataset b·∫°n c√≥ ba bi·∫øn ph√¢n lo·∫°i ch√≠nh:\n",
    "  - `contact`\n",
    "  - `month`\n",
    "  - `day_of_week`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47115ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+-----+-----------+--------------+\n",
      "|contact  |contact_index|contact_vec  |month|month_index|month_vec     |\n",
      "+---------+-------------+-------------+-----+-----------+--------------+\n",
      "|telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|\n",
      "|telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|\n",
      "|telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|\n",
      "|telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|\n",
      "|telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|\n",
      "+---------+-------------+-------------+-----+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cat_cols = [\"contact\", \"month\", \"day_of_week\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_vec\") for c in cat_cols]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "df_encoded = pipeline.fit(df).transform(df)\n",
    "\n",
    "df_encoded.select(\"contact\", \"contact_index\", \"contact_vec\", \n",
    "                  \"month\", \"month_index\", \"month_vec\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca29a2a",
   "metadata": {},
   "source": [
    "#### V√¨ sao d√πng StringIndexer + OneHotEncoder\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "- C√°c m√¥ h√¨nh nh∆∞ **Logistic Regression**, **MLP** c·∫ßn d·ªØ li·ªáu d·∫°ng **vector s·ªë** ‚Äî m·ªói c·ªôt ƒë·∫°i di·ªán cho m·ªôt ƒë·∫∑c tr∆∞ng ri√™ng.  \n",
    "- D·ªØ li·ªáu ch·ªâ c√≥ √≠t gi√° tr·ªã ph√¢n lo·∫°i (`contact`, `month`, `day_of_week`), n√™n **One-Hot Encoding** kh√¥ng g√¢y tƒÉng chi·ªÅu d·ªØ li·ªáu qu√° m·ª©c.  \n",
    "- `StringIndexer` bi·∫øn chu·ªói th√†nh m√£ s·ªë ƒë·ªÉ `OneHotEncoder` hi·ªÉu ƒë∆∞·ª£c, gi√∫p pipeline **·ªïn ƒë·ªãnh v√† t√°i s·ª≠ d·ª•ng cho d·ªØ li·ªáu m·ªõi**.  \n",
    "- N·∫øu ch·ªâ d√πng **Label Encoding**, m√¥ h√¨nh s·∫Ω hi·ªÉu sai r·∫±ng c√°c gi√° tr·ªã c√≥ **th·ª© t·ª±**, l√†m sai √Ω nghƒ©a c·ªßa bi·∫øn.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868b4d7",
   "metadata": {},
   "source": [
    "#### 4.1.c ‚Äì Chia d·ªØ li·ªáu Train / Validation / Test\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "\n",
    "T√°ch d·ªØ li·ªáu th√†nh 3 ph·∫ßn ƒë·ªÉ:\n",
    "\n",
    "- **Train m√¥ h√¨nh (70%)**  \n",
    "- **Validation** ƒë·ªÉ tinh ch·ªânh tham s·ªë (**15%**)  \n",
    "- **Test** ƒë·ªÉ ƒë√°nh gi√° cu·ªëi c√πng (**15%**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf6499bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x00000230FD2F4550>\n",
      "Counts: 28831 6178 6179\n",
      "\n",
      "Train label distribution:\n",
      "+--------+-----+---------+\n",
      "|y_binary|count|ratio (%)|\n",
      "+--------+-----+---------+\n",
      "|       0|25583|    88.73|\n",
      "|       1| 3248|    11.27|\n",
      "+--------+-----+---------+\n",
      "\n",
      "\n",
      "Validation label distribution:\n",
      "+--------+-----+---------+\n",
      "|y_binary|count|ratio (%)|\n",
      "+--------+-----+---------+\n",
      "|       0| 5482|    88.73|\n",
      "|       1|  696|    11.27|\n",
      "+--------+-----+---------+\n",
      "\n",
      "\n",
      "Test label distribution:\n",
      "+--------+-----+---------+\n",
      "|y_binary|count|ratio (%)|\n",
      "+--------+-----+---------+\n",
      "|       0| 5483|    88.74|\n",
      "|       1|  696|    11.26|\n",
      "+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy('y_binary').orderBy(F.rand(seed = 42))\n",
    "print(w)\n",
    "df_strat = (\n",
    "    df_encoded.withColumn('pr', F.percent_rank().over(w))\n",
    ")\n",
    "\n",
    "#Chia ty le\n",
    "train_df = df_strat.filter(F.col('pr') < 0.7).drop('pr')\n",
    "val_df = df_strat.filter((F.col('pr') >= 0.7) & (F.col('pr') < 0.85)).drop('pr')\n",
    "test_df = df_strat.filter(F.col('pr') >= 0.85).drop('pr')\n",
    "\n",
    "# Ki·ªÉm tra quy m√¥ & ph√¢n b·ªë nh√£n\n",
    "print(\"Counts:\", train_df.count(), val_df.count(), test_df.count())\n",
    "\n",
    "for name, subset in [(\"Train\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
    "    print(f\"\\n{name} label distribution:\")\n",
    "    total = subset.count()\n",
    "    subset.groupBy(\"y_binary\").agg(\n",
    "        F.count('*').alias('count')\n",
    "    ).withColumn('ratio (%)', F.round(F.col('count')/total * 100, 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffceef3",
   "metadata": {},
   "source": [
    "#### L√Ω do ch·ªçn ph∆∞∆°ng ph√°p chia d·ªØ li·ªáu\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "- B√†i to√°n s·ª≠ d·ª•ng **Stratified Hold-out (70/15/15)** ƒë·ªÉ ƒë·∫£m b·∫£o t·ª∑ l·ªá nh√£n `yes/no` ƒë∆∞·ª£c gi·ªØ ·ªïn ƒë·ªãnh trong t·ª´ng t·∫≠p **Train**, **Validation** v√† **Test**.  \n",
    "- Ph∆∞∆°ng ph√°p n√†y ph√π h·ª£p v√¨ **d·ªØ li·ªáu ƒë·ªß l·ªõn (~41k m·∫´u, t·ª∑ l·ªá yes ‚âà 11%)**, gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c xu h∆∞·ªõng t·ªïng qu√°t m√† kh√¥ng c·∫ßn l·∫∑p nhi·ªÅu l·∫ßn nh∆∞ **K-fold**.\n",
    "\n",
    "**∆Øu ƒëi·ªÉm c·ªßa Stratified Hold-out:**\n",
    "\n",
    "- Nhanh, d·ªÖ tri·ªÉn khai trong **PySpark**.  \n",
    "- Gi·ªØ ƒë√∫ng ph√¢n ph·ªëi nh√£n cho b√†i to√°n **m·∫•t c√¢n b·∫±ng**.  \n",
    "- H·∫°n ch·∫ø ƒë∆∞·ª£c **chi ph√≠ t√≠nh to√°n** so v·ªõi K-fold, v·ªën ph·∫£i hu·∫•n luy·ªán m√¥ h√¨nh nhi·ªÅu l·∫ßn.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f151d",
   "metadata": {},
   "source": [
    "#### 4.1.d ‚Äì Class weighting (m·∫∑c ƒë·ªãnh) + tu·ª≥ ch·ªçn undersampling\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "\n",
    "Gi·∫£m thi√™n l·ªách do `yes ‚âà 11%` nh∆∞ng kh√¥ng l√†m m·∫•t d·ªØ li·ªáu quan tr·ªçng.  \n",
    "Chi·∫øn l∆∞·ª£c m·∫∑c ƒë·ªãnh: **g√°n tr·ªçng s·ªë theo l·ªõp** tr√™n t·∫≠p **train** (validation/test gi·ªØ nguy√™n ƒë·ªÉ ƒë√°nh gi√° c√¥ng b·∫±ng).\n",
    "\n",
    "#### C√¥ng th·ª©c t·ªïng qu√°t cho tr·ªçng s·ªë c√¢n b·∫±ng\n",
    "\n",
    "**C√¥ng th·ª©c:**\n",
    "\n",
    "$$\n",
    "w_i = \\frac{N}{k \\times n_i}\n",
    "$$\n",
    "\n",
    "**Trong ƒë√≥:**\n",
    "\n",
    "- $w_i$: tr·ªçng s·ªë c·ªßa l·ªõp *i*  \n",
    "- $N$: t·ªïng s·ªë m·∫´u trong t·∫≠p hu·∫•n luy·ªán  \n",
    "- $n_i$: s·ªë m·∫´u thu·ªôc l·ªõp *i*  \n",
    "- $k$: s·ªë l∆∞·ª£ng l·ªõp (v·ªõi b√†i to√°n nh·ªã ph√¢n, $k = 2$)\n",
    "\n",
    "‚Üí Khi l·ªõp ‚Äúyes‚Äù chi·∫øm √≠t, $n_{\\text{yes}}$ nh·ªè ‚Üí $w_{\\text{yes}}$ l·ªõn h∆°n $w_{\\text{no}}$.  \n",
    "N√≥i c√°ch kh√°c, m·ªói l·ªói d·ª± ƒëo√°n sai c·ªßa l·ªõp thi·ªÉu s·ªë s·∫Ω b·ªã **ph·∫°t n·∫∑ng h∆°n** trong qu√° tr√¨nh t·ªëi ∆∞u h√†m m·∫•t m√°t,  \n",
    "gi√∫p m√¥ h√¨nh h·ªçc **c√¢n b·∫±ng gi·ªØa hai l·ªõp** m√† kh√¥ng c·∫ßn thay ƒë·ªïi d·ªØ li·ªáu g·ªëc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20c8ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|y_binary| avg_w|\n",
      "+--------+------+\n",
      "|       0|0.5635|\n",
      "|       1|4.4383|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = {r['y_binary']: r['count'] for r in train_df.groupBy('y_binary').count().collect()}\n",
    "\n",
    "n_pos = cnt.get(1, 0)\n",
    "n_neg = cnt.get(0, 0)\n",
    "n_tot = n_pos + n_neg\n",
    "\n",
    "# Tr·ªçng s·ªë c√¢n b·∫±ng ki·ªÉu \"balanced\":\n",
    "# w_pos = N / (2 * N_pos), w_neg = N / (2 * N_neg)\n",
    "w_pos = float(n_tot) / (2.0 * n_pos)\n",
    "w_neg = float(n_tot) / (2.0 * n_neg)\n",
    "\n",
    "# G√°n tr·ªçng s·ªë cho TRAIN; Val/Test kh√¥ng g√°n ƒë·ªÉ ƒë√°nh gi√° trung th·ª±c\n",
    "train_w = train_df.withColumn(\n",
    "    \"class_weight\",\n",
    "    F.when(F.col(\"y_binary\") == 1, F.lit(w_pos)).otherwise(F.lit(w_neg))\n",
    ")\n",
    "\n",
    "train_w.groupBy(\"y_binary\").agg(F.round(F.avg(\"class_weight\"),4).alias(\"avg_w\")).orderBy(\"y_binary\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750a79e",
   "metadata": {},
   "source": [
    "#### Gi·∫£i th√≠ch l·ª±a ch·ªçn ph∆∞∆°ng ph√°p class weighting\n",
    "\n",
    "**L√Ω do:**\n",
    "\n",
    "- T·∫≠p d·ªØ li·ªáu ƒë·ªß l·ªõn, **kh√¥ng c·∫ßn nh√¢n b·∫£n m·∫´u thi·ªÉu s·ªë**.  \n",
    "- Nhi·ªÅu bi·∫øn ph√¢n lo·∫°i n√™n **SMOTE kh√¥ng ph√π h·ª£p**.  \n",
    "- C√°c m√¥ h√¨nh **Logistic Regression**, **CatBoost** v√† **MLP** ƒë·ªÅu h·ªó tr·ª£ weighting tr·ª±c ti·∫øp.  \n",
    "- Gi·ªØ nguy√™n to√†n b·ªô d·ªØ li·ªáu th·∫≠t, gi√∫p m√¥ h√¨nh h·ªçc **c√¢n b·∫±ng v√† ·ªïn ƒë·ªãnh h∆°n**.\n",
    "\n",
    "**K·∫øt lu·∫≠n:**\n",
    "\n",
    "S·ª≠ d·ª•ng **class weighting** gi√∫p duy tr√¨ to√†n b·ªô d·ªØ li·ªáu g·ªëc, gi·∫£m thi√™n l·ªách,  \n",
    "v√† ph√π h·ª£p v·ªõi c·∫£ ba m√¥ h√¨nh hu·∫•n luy·ªán ch√≠nh c·ªßa nh√≥m.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99813dfe",
   "metadata": {},
   "source": [
    "#### 4.1.e ‚Äì Assemble features\n",
    "\n",
    "**M·ª•c ti√™u:**  \n",
    "Gom to√†n b·ªô c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o (bao g·ªìm c·∫£ d·ªØ li·ªáu **s·ªë** v√† **one-hot encoding**) th√†nh **m·ªôt c·ªôt vector duy nh·∫•t** c√≥ t√™n `features`, ƒë·ªÉ s·∫µn s√†ng ƒë∆∞a v√†o giai ƒëo·∫°n hu·∫•n luy·ªán m√¥ h√¨nh.\n",
    "\n",
    "**L∆∞u √Ω:**  \n",
    "Kh√¥ng bao g·ªìm tr∆∞·ªùng **`duration`** trong qu√° tr√¨nh t·ªïng h·ª£p ‚Äî nh·∫±m **tr√°nh r√≤ r·ªâ th√¥ng tin** li√™n quan ƒë·∫øn k·∫øt qu·∫£ sau khi cu·ªôc g·ªçi ƒë√£ k·∫øt th√∫c.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "feature_cols = ['previous', 'has_contact_before', 'campaign_capped',\n",
    "                'contact_vec', 'month_vec', 'day_of_week_vec']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_unscaled')\n",
    "scaler    = StandardScaler(inputCol='features_unscaled', outputCol='features',\n",
    "                           withMean=True, withStd=True)\n",
    "\n",
    "# Fit ƒë√∫ng 1 l·∫ßn tr√™n TRAIN\n",
    "pipeline_scale = Pipeline(stages=[assembler, scaler])\n",
    "scale_model = pipeline_scale.fit(train_w)\n",
    "\n",
    "# Transform 3 t·∫≠p (kh√¥ng fit l·∫°i)\n",
    "train_ready = scale_model.transform(train_w).select('features','y_binary','class_weight')\n",
    "val_ready   = scale_model.transform(val_df).select('features','y_binary')\n",
    "test_ready  = scale_model.transform(test_df).select('features','y_binary')\n",
    "\n",
    "# Persist c√°i d√πng nhi·ªÅu\n",
    "train_ready = train_ready.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "_ = train_ready.count()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea494c",
   "metadata": {},
   "source": [
    "### 5. Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "\n",
    "M·ª•c ti√™u c·ªßa b∆∞·ªõc n√†y l√† **x√¢y d·ª±ng m√¥ h√¨nh h·ªçc m√°y** c√≥ kh·∫£ nƒÉng **d·ª± ƒëo√°n x√°c su·∫•t kh√°ch h√†ng s·∫Ω ƒë·ªìng √Ω tham gia s·∫£n ph·∫©m (`y = yes`)** d·ª±a tr√™n c√°c th√¥ng tin c√≥ s·∫µn nh∆∞ nh√¢n kh·∫©u h·ªçc, ngh·ªÅ nghi·ªáp, k√™nh li√™n l·∫°c v√† l·ªãch s·ª≠ chi·∫øn d·ªãch marketing.  \n",
    "\n",
    "K·∫øt qu·∫£ hu·∫•n luy·ªán gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c **m·ªëi quan h·ªá gi·ªØa ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o v√† h√†nh vi ph·∫£n h·ªìi c·ªßa kh√°ch h√†ng**, t·ª´ ƒë√≥ h·ªó tr·ª£ doanh nghi·ªáp **x√°c ƒë·ªãnh nh√≥m kh√°ch h√†ng ti·ªÅm nƒÉng nh·∫•t ƒë·ªÉ ti·∫øp c·∫≠n tr∆∞·ªõc**, gi·∫£m chi ph√≠ g·ªçi ƒëi·ªán v√† tƒÉng hi·ªáu qu·∫£ marketing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d779513",
   "metadata": {},
   "source": [
    "#### 5.1.a ‚Äì C·∫•u h√¨nh & Train m√¥ h√¨nh (d√πng `class_weight`)\n",
    "\n",
    "**M·ª•c ti√™u:**  \n",
    "Hu·∫•n luy·ªán m√¥ h√¨nh **baseline ƒë·∫ßu ti√™n** tr√™n t·∫≠p d·ªØ li·ªáu `train_ready` ƒë·ªÉ l√†m **m·ªëc so s√°nh** v·ªõi c√°c m√¥ h√¨nh sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d1425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "|rawPrediction                             |probability                             |prediction|y_binary|\n",
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "|[1.498029288056012,-1.498029288056012]    |[0.8172803675270653,0.18271963247293466]|0.0       |0       |\n",
      "|[1.262208047674275,-1.262208047674275]    |[0.7794059766980153,0.22059402330198474]|0.0       |0       |\n",
      "|[1.196569153234042,-1.196569153234042]    |[0.7679138927381268,0.23208610726187318]|0.0       |0       |\n",
      "|[0.34745760641059786,-0.34745760641059786]|[0.5860009196287119,0.4139990803712881] |0.0       |0       |\n",
      "|[-3.651301973313404,3.651301973313404]    |[0.02530057611851298,0.974699423881487] |1.0       |0       |\n",
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"y_binary\",\n",
    "    weightCol=\"class_weight\",   \n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    family=\"binomial\",\n",
    "    elasticNetParam=0.0,      \n",
    "    regParam=0.01,\n",
    "    maxIter=100,\n",
    "    standardization=True\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_ready)\n",
    "\n",
    "\n",
    "# D·ª± ƒëo√°n tr√™n t·∫≠p validation ƒë·ªÉ chu·∫©n b·ªã ƒë√°nh gi√° ·ªü b∆∞·ªõc 5.1.b\n",
    "val_pred = lr_model.transform(val_ready).select(\"rawPrediction\",\"probability\",\"prediction\",\"y_binary\")\n",
    "val_pred.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ef8cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(pred_data, label_col=\"y_binary\", pred_col=\"prediction\", raw_col=\"rawPrediction\"):\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "    cm = (\n",
    "        pred_data\n",
    "        .groupBy(label_col, pred_col)\n",
    "        .count()\n",
    "        .orderBy(label_col, pred_col)\n",
    "    )\n",
    "    cm.show()\n",
    "\n",
    "    agg = (\n",
    "        pred_data\n",
    "        .withColumn(\"tp\", F.when((F.col(pred_col)==1) & (F.col(label_col)==1), 1).otherwise(0))\n",
    "        .withColumn(\"fp\", F.when((F.col(pred_col)==1) & (F.col(label_col)==0), 1).otherwise(0))\n",
    "        .withColumn(\"tn\", F.when((F.col(pred_col)==0) & (F.col(label_col)==0), 1).otherwise(0))\n",
    "        .withColumn(\"fn\", F.when((F.col(pred_col)==0) & (F.col(label_col)==1), 1).otherwise(0))\n",
    "        .agg(F.sum(\"tp\").alias(\"tp\"),\n",
    "             F.sum(\"fp\").alias(\"fp\"),\n",
    "             F.sum(\"tn\").alias(\"tn\"),\n",
    "             F.sum(\"fn\").alias(\"fn\"),\n",
    "             F.count(\"*\").alias(\"n\"))\n",
    "        .collect()[0]\n",
    "    )\n",
    "\n",
    "    tp, fp, tn, fn, n = [agg[x] for x in [\"tp\",\"fp\",\"tn\",\"fn\",\"n\"]]\n",
    "    acc = (tp + tn) / n if n else 0.0\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    print(f\"Accuracy={acc:.4f} | Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f}\")\n",
    "\n",
    "    # ROC-AUC v√† PR-AUC n·∫øu c√≥ rawPrediction\n",
    "    if raw_col in pred_data.columns:\n",
    "        e_roc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=raw_col, metricName=\"areaUnderROC\")\n",
    "        e_pr  = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=raw_col, metricName=\"areaUnderPR\")\n",
    "        print(f\"ROC-AUC={e_roc.evaluate(pred_data):.4f} | PR-AUC={e_pr.evaluate(pred_data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c74263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n",
      "|y_binary|prediction|count|\n",
      "+--------+----------+-----+\n",
      "|       0|       0.0| 4790|\n",
      "|       0|       1.0|  692|\n",
      "|       1|       0.0|  354|\n",
      "|       1|       1.0|  342|\n",
      "+--------+----------+-----+\n",
      "\n",
      "Accuracy : 0.8307\n",
      "Precision: 0.3308\n",
      "Recall   : 0.4914\n",
      "F1-score : 0.3954\n",
      "Positive rate (true +): 0.1127\n",
      "ROC-AUC : 0.7315\n",
      "PR-AUC  : 0.3740\n"
     ]
    }
   ],
   "source": [
    "evaluation(val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d14e0",
   "metadata": {},
   "source": [
    "#### 5.1.b ‚Äì T√¨m si√™u tham s·ªë v√† t·ªëi ∆∞u ng∆∞·ª°ng d·ª± ƒëo√°n (threshold) theo F1 tr√™n validation\n",
    "\n",
    "**M·ª•c ti√™u:**  \n",
    "M√¥ h√¨nh **Logistic Regression** m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng ng∆∞·ª°ng **0.5** ƒë·ªÉ ph√¢n lo·∫°i, tuy nhi√™n v·ªõi t·∫≠p d·ªØ li·ªáu c√≥ t·ª∑ l·ªá l·ªõp ‚Äúyes‚Äù th·∫•p (~11%), ng∆∞·ª°ng n√†y th∆∞·ªùng **thi·∫øu nh·∫°y** (recall th·∫•p).  \n",
    "Do ƒë√≥, c·∫ßn **qu√©t qua nhi·ªÅu gi√° tr·ªã ng∆∞·ª°ng** (v√≠ d·ª•: t·ª´ 0.1 ƒë·∫øn 0.9) ngo√†i ra c·∫ßn t√¨m c√°c b·ªô tham s·ªë th√≠ch h·ª£p t·ªëi ∆∞u cho F1-score, sau ƒë√≥:\n",
    "\n",
    "1. T√≠nh **Precision**, **Recall** v√† **F1-score** cho t·ª´ng ng∆∞·ª°ng.  \n",
    "2. Ch·ªçn **ng∆∞·ª°ng c√≥ F1 cao nh·∫•t** l√†m gi√° tr·ªã t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bac7cd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-by-F1 params: {'regParam': 0.01, 'elasticNetParam': 1.0, 'maxIter': 50, 'thr': 0.51}\n",
      "VAL @T=0.51 -> F1=0.4031 | P=0.3476 | R=0.4799\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Grid nh·ªè, thi√™n v·ªÅ b·ªõt ‚Äúph·∫°t‚Äù ƒë·ªÉ tƒÉng recall\n",
    "reg_list = [0.0005, 0.001, 0.01, 0.05]\n",
    "en_list  = [0.0, 0.5, 1.0]     # L2 / mix / L1\n",
    "mi_list  = [50, 100]\n",
    "\n",
    "def max_f1_on_validation(model):\n",
    "    # t√≠nh p1\n",
    "    val_scored = (model.transform(val_ready)\n",
    "                        .select(\"y_binary\", vector_to_array(\"probability\").getItem(1).alias(\"p1\"))\n",
    "                        .cache())\n",
    "    # qu√©t ng∆∞·ª°ng\n",
    "    best = {\"f1\": -1}\n",
    "    for t in [round(0.02 + 0.01*i, 2) for i in range(80)]:  # 0.02..0.81\n",
    "        pred = val_scored.select(\"y_binary\", (F.col(\"p1\") >= F.lit(t)).cast(\"int\").alias(\"pred\"))\n",
    "        a = pred.agg(\n",
    "            F.sum(F.when((F.col(\"pred\")==1) & (F.col(\"y_binary\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "            F.sum(F.when((F.col(\"pred\")==1) & (F.col(\"y_binary\")==0), 1).otherwise(0)).alias(\"fp\"),\n",
    "            F.sum(F.when((F.col(\"pred\")==0) & (F.col(\"y_binary\")==0), 1).otherwise(0)).alias(\"tn\"),\n",
    "            F.sum(F.when((F.col(\"pred\")==0) & (F.col(\"y_binary\")==1), 1).otherwise(0)).alias(\"fn\")\n",
    "        ).first()\n",
    "        tp, fp, tn, fn = a.tp, a.fp, a.tn, a.fn\n",
    "        P = tp/(tp+fp) if (tp+fp) else 0.0\n",
    "        R = tp/(tp+fn) if (tp+fn) else 0.0\n",
    "        F1 = 2*P*R/(P+R) if (P+R) else 0.0\n",
    "        if F1 > best[\"f1\"]:\n",
    "            best = {\"thr\": t, \"f1\": F1, \"prec\": P, \"rec\": R}\n",
    "    return best\n",
    "\n",
    "best = None\n",
    "for rp, en, mi in product(reg_list, en_list, mi_list):\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\", labelCol=\"y_binary\", weightCol=\"class_weight\",\n",
    "        family=\"binomial\", standardization=True,\n",
    "        regParam=rp, elasticNetParam=en, maxIter=mi\n",
    "    )\n",
    "    m = lr.fit(train_ready)\n",
    "    s = max_f1_on_validation(m)\n",
    "    if (best is None) or (s[\"f1\"] > best[\"f1\"]):\n",
    "        best = {\"regParam\": rp, \"elasticNetParam\": en, \"maxIter\": mi, **s, \"model\": m}\n",
    "\n",
    "print(\"Best-by-F1 params:\",\n",
    "      {\"regParam\":best[\"regParam\"], \"elasticNetParam\":best[\"elasticNetParam\"],\n",
    "       \"maxIter\":best[\"maxIter\"], \"thr\":best[\"thr\"]})\n",
    "print(f\"VAL @T={best['thr']:.2f} -> F1={best['f1']:.4f} | P={best['prec']:.4f} | R={best['rec']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa14532",
   "metadata": {},
   "source": [
    "#### 5.1.c ‚Äì K·∫øt qu·∫£ tr√™n t·∫≠p test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff5ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "|rawPrediction                             |probability                             |prediction|y_binary|\n",
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "|[0.43308687383781813,-0.43308687383781813]|[0.6066105439912574,0.39338945600874264]|0.0       |0       |\n",
      "|[0.3674479793975853,-0.3674479793975853]  |[0.5908421766530948,0.4091578233469052] |0.0       |0       |\n",
      "|[0.43308687383781813,-0.43308687383781813]|[0.6066105439912574,0.39338945600874264]|0.0       |0       |\n",
      "|[1.812004954922738,-1.812004954922738]    |[0.8596040166539753,0.14039598334602466]|0.0       |0       |\n",
      "|[0.167012693026944,-0.167012693026944]    |[0.5416563906003695,0.4583436093996305] |0.0       |0       |\n",
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+----------+-----+\n",
      "|y_binary|prediction|count|\n",
      "+--------+----------+-----+\n",
      "|       0|       0.0| 4790|\n",
      "|       0|       1.0|  692|\n",
      "|       1|       0.0|  354|\n",
      "|       1|       1.0|  342|\n",
      "+--------+----------+-----+\n",
      "\n",
      "Accuracy : 0.8307\n",
      "Precision: 0.3308\n",
      "Recall   : 0.4914\n",
      "F1-score : 0.3954\n",
      "Positive rate (true +): 0.1127\n",
      "ROC-AUC : 0.7315\n",
      "PR-AUC  : 0.3740\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr2 = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"y_binary\",\n",
    "    weightCol=\"class_weight\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    family=\"binomial\",\n",
    "    elasticNetParam=1.0,  \n",
    "    regParam=0.01,\n",
    "    maxIter=50,\n",
    "    threshold=0.51,     \n",
    "    standardization=True\n",
    ")\n",
    "\n",
    "\n",
    "lr2_model = lr2.fit(train_ready)\n",
    "\n",
    "\n",
    "\n",
    "test_pred = lr_model.transform(test_ready).select(\"rawPrediction\",\"probability\",\"prediction\",\"y_binary\")\n",
    "test_pred.show(5, truncate=False)\n",
    "\n",
    "evaluation(val_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b42d14",
   "metadata": {},
   "source": [
    "#### 6a) Train baseline GBTClassifier tr√™n t·∫≠p train, ƒë√°nh gi√° tr√™n validation\n",
    "\n",
    "**ƒêi·ªÉm kh√°c so v·ªõi Logistic Regression:**\n",
    "\n",
    "- **GBT (Gradient Boosted Trees)** l√† m√¥ h√¨nh **phi tuy·∫øn** ‚Äî n√≥ c·ªông d·ªìn nhi·ªÅu ‚Äúc√¢y y·∫øu‚Äù (weak learners) ƒë·ªÉ t·∫°o ra m·ªôt m√¥ h√¨nh m·∫°nh h∆°n.  \n",
    "  ƒêi·ªÅu n√†y gi√∫p **b·∫Øt ƒë∆∞·ª£c c√°c t∆∞∆°ng t√°c phi tuy·∫øn** gi·ªØa c√°c bi·∫øn ƒë·∫ßu v√†o t·ªët h∆°n so v·ªõi Logistic Regression, v·ªën ch·ªâ h·ªçc tuy·∫øn t√≠nh.\n",
    "\n",
    "- Trong Spark MLlib, **GBTClassifier kh√¥ng h·ªó tr·ª£ `weightCol`**, kh√°c v·ªõi Logistic.  \n",
    "  Do ƒë√≥, ta **gi·ªØ nguy√™n quy tr√¨nh chia t·∫≠p (Stratified split)** v√† **c√°ch ƒë√°nh gi√°** nh∆∞ m√¥ h√¨nh Logistic ƒë·ªÉ ƒë·∫£m b·∫£o so s√°nh c√¥ng b·∫±ng.\n",
    "\n",
    "- **Vi·ªác scale d·ªØ li·ªáu kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn m√¥ h√¨nh c√¢y.**  \n",
    "  Tuy nhi√™n, ta v·∫´n d√πng c√°c **features ƒë√£ ƒë∆∞·ª£c assemble/scale ·ªü b∆∞·ªõc 4.1e** ƒë·ªÉ th·ªëng nh·∫•t pipeline hu·∫•n luy·ªán v√† ƒë√°nh gi√°.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "|rawPrediction                             |probability                             |prediction|y_binary|\n",
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "|[1.512224316074537,-1.512224316074537]    |[0.9536664931117149,0.04633350688828508]|0.0       |0       |\n",
      "|[1.6451784847369582,-1.6451784847369582]  |[0.9640965124583327,0.03590348754166728]|0.0       |0       |\n",
      "|[1.6451784847369582,-1.6451784847369582]  |[0.9640965124583327,0.03590348754166728]|0.0       |0       |\n",
      "|[1.2890444073637066,-1.2890444073637066]  |[0.9294380307116556,0.07056196928834435]|0.0       |0       |\n",
      "|[-0.12850445757989315,0.12850445757989315]|[0.4360991247342985,0.5639008752657015] |1.0       |0       |\n",
      "+------------------------------------------+----------------------------------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+----------+-----+\n",
      "|y_binary|prediction|count|\n",
      "+--------+----------+-----+\n",
      "|       0|       0.0| 5403|\n",
      "|       0|       1.0|   79|\n",
      "|       1|       0.0|  558|\n",
      "|       1|       1.0|  138|\n",
      "+--------+----------+-----+\n",
      "\n",
      "Accuracy : 0.8969\n",
      "Precision: 0.6359\n",
      "Recall   : 0.1983\n",
      "F1-score : 0.3023\n",
      "Positive rate (true +): 0.1127\n",
      "ROC-AUC : 0.7315\n",
      "PR-AUC  : 0.3740\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"y_binary\",\n",
    "    maxDepth=5,     \n",
    "    maxIter=60,     \n",
    "    stepSize=0.1,   \n",
    "    subsamplingRate=1.0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(train_ready)\n",
    "\n",
    "# D·ª± ƒëo√°n tr√™n validation (gi·ªëng m·ª•c 5)\n",
    "val_pred_gbt = (\n",
    "    gbt_model.transform(val_ready)\n",
    "             .select(\"rawPrediction\",\"probability\",\"prediction\",\"y_binary\")\n",
    ")\n",
    "\n",
    "val_pred_gbt.show(5, truncate=False)\n",
    "\n",
    "evaluation(val_pred_gbt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56668820",
   "metadata": {},
   "source": [
    "#### 6b) Cross-validation GBT + ch·ªçn ng∆∞·ª°ng t·ªëi ∆∞u theo F1\n",
    "\n",
    "**M·ª•c ti√™u:**  \n",
    "T·ªëi ∆∞u m√¥ h√¨nh **Gradient Boosted Trees** b·∫±ng c√°ch:\n",
    "- D√≤ **si√™u tham s·ªë (hyperparameters)** qua **3-fold cross-validation** ƒë·ªÉ t√¨m c·∫•u h√¨nh t·ªët nh·∫•t v·ªÅ `maxDepth`, `maxIter`, `stepSize`.\n",
    "- ƒê√°nh gi√° m√¥ h√¨nh theo **PR-AUC** (∆∞u ti√™n ph√π h·ª£p cho d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng).\n",
    "- Sau khi c√≥ m√¥ h√¨nh t·ªët nh·∫•t, **qu√©t nhi·ªÅu ng∆∞·ª°ng x√°c su·∫•t** ƒë·ªÉ ch·ªçn **threshold cho F1-score cao nh·∫•t** tr√™n t·∫≠p validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: maxDepth= 3 | maxIter= 100 | stepSize= 0.05\n",
      "Best thr=0.20 | F1=0.4316 | P=0.4238 | R=0.4397\n"
     ]
    }
   ],
   "source": [
    "# 6b ‚Äî CV GBT + ch·ªçn ng∆∞·ª°ng F1 (ng·∫Øn g·ªçn)\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # ho·∫∑c 100 t√πy t√†i nguy√™n\n",
    "\n",
    "val_ready.cache()\n",
    "_ = val_ready.count()\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"y_binary\", seed=42)\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(gbt.maxDepth, [3, 5, 7])\n",
    "    .addGrid(gbt.maxIter,  [60, 100])\n",
    "    .addGrid(gbt.stepSize, [0.05, 0.1])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=BinaryClassificationEvaluator(labelCol=\"y_binary\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\"),\n",
    "    numFolds=3, seed=42, parallelism=1\n",
    ")\n",
    "\n",
    "best = cv.fit(train_ready).bestModel\n",
    "print(\"Best params:\",\n",
    "      \"maxDepth=\", best.getMaxDepth(),\n",
    "      \"| maxIter=\", best.getMaxIter(),\n",
    "      \"| stepSize=\", best.getStepSize())\n",
    "\n",
    "# D·ª± ƒëo√°n tr√™n val + l·∫•y p(y=1)\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# D·ª± ƒëo√°n tr√™n val + l·∫•y p(y=1) theo c√°ch an to√†n cho VectorUDT\n",
    "val_probs = (\n",
    "    best.transform(val_ready)\n",
    "        .select(\n",
    "            vector_to_array(\"probability\").getItem(1).alias(\"p\"),\n",
    "            F.col(\"y_binary\").cast(\"int\").alias(\"y\")\n",
    "        )\n",
    "        .collect()\n",
    ")\n",
    "\n",
    "y = np.array([r[\"y\"] for r in val_probs], dtype=int)\n",
    "p = np.array([r[\"p\"] for r in val_probs], dtype=float)\n",
    "\n",
    "\n",
    "# Qu√©t ng∆∞·ª°ng 0.05..0.95 ƒë·ªÉ t·ªëi ∆∞u F1\n",
    "ths = np.linspace(0.05, 0.95, 19)\n",
    "def f1_at(t):\n",
    "    yhat = p >= t\n",
    "    yt   = y == 1\n",
    "    tp = np.sum(yhat & yt); fp = np.sum(yhat & (~yt)); fn = np.sum((~yhat) & yt)\n",
    "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
    "    return f1, prec, rec\n",
    "\n",
    "best_thr, best_f1, best_p, best_r = max(((t,)+f1_at(t) for t in ths), key=lambda x: x[1])\n",
    "print(f\"Best thr={best_thr:.2f} | F1={best_f1:.4f} | P={best_p:.4f} | R={best_r:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407e3a8",
   "metadata": {},
   "source": [
    "#### 6c) ƒê√°nh gi√° m√¥ h√¨nh GBT t·ªëi ∆∞u tr√™n t·∫≠p Test\n",
    "\n",
    "**M·ª•c ti√™u:**  \n",
    "S·ª≠ d·ª•ng m√¥ h√¨nh **GBT t·ªët nh·∫•t (`best`)** v√† **ng∆∞·ª°ng d·ª± ƒëo√°n t·ªëi ∆∞u (`best_thr`)** t·ª´ b∆∞·ªõc 6b ƒë·ªÉ:\n",
    "1. ƒê√°nh gi√° **hi·ªáu nƒÉng cu·ªëi c√πng** tr√™n t·∫≠p test ch∆∞a t·ª´ng th·∫•y trong qu√° tr√¨nh hu·∫•n luy·ªán.  \n",
    "2. B√°o c√°o c√°c ch·ªâ s·ªë ch√≠nh: **Accuracy**, **Precision**, **Recall**, **F1**, **ROC-AUC**, **PR-AUC**.  \n",
    "3. Quan s√°t **ma tr·∫≠n nh·∫ßm l·∫´n** ƒë·ªÉ hi·ªÉu r√µ m√¥ h√¨nh d·ª± ƒëo√°n t·ªët/kh√≥ ·ªü nh√≥m n√†o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC: 0.7477\n",
      "Test PR-AUC : 0.3813\n",
      "+---+--------+-----+\n",
      "|  y|pred_thr|count|\n",
      "+---+--------+-----+\n",
      "|  0|       0| 5128|\n",
      "|  0|       1|  355|\n",
      "|  1|       0|  392|\n",
      "|  1|       1|  304|\n",
      "+---+--------+-----+\n",
      "\n",
      "Threshold=0.20 | Test Acc=0.8791 | P=0.4613 | R=0.4368 | F1=0.4487\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "assert 'best' in globals() and 'best_thr' in globals(), \"C·∫ßn ch·∫°y xong 6b ƒë·ªÉ c√≥ best & best_thr.\"\n",
    "assert 'test_ready' in globals(), \"Thi·∫øu test_ready (ƒë√£ assemble + encode).\"\n",
    "\n",
    "\n",
    "test_raw = (\n",
    "    best.transform(test_ready)\n",
    "        .select(\"rawPrediction\",\"probability\",\"prediction\",\"y_binary\")\n",
    ")\n",
    "\n",
    "\n",
    "e_roc = BinaryClassificationEvaluator(labelCol=\"y_binary\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "e_pr  = BinaryClassificationEvaluator(labelCol=\"y_binary\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "print(\"Test ROC-AUC:\", round(e_roc.evaluate(test_raw), 4))\n",
    "print(\"Test PR-AUC :\", round(e_pr.evaluate(test_raw), 4))\n",
    "\n",
    "\n",
    "test_with_p = (\n",
    "    test_raw\n",
    "    .select(\n",
    "        F.col(\"y_binary\").cast(\"int\").alias(\"y\"),\n",
    "        vector_to_array(\"probability\").getItem(1).alias(\"p\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "thr = float(best_thr)\n",
    "test_scored = test_with_p.withColumn(\"pred_thr\", (F.col(\"p\") >= F.lit(thr)).cast(\"int\"))\n",
    "\n",
    "cm = test_scored.groupBy(\"y\",\"pred_thr\").count().orderBy(\"y\",\"pred_thr\")\n",
    "cm.show()\n",
    "\n",
    "\n",
    "agg = (\n",
    "    test_scored\n",
    "    .withColumn(\"tp\", F.when((F.col(\"pred_thr\")==1) & (F.col(\"y\")==1), 1).otherwise(0))\n",
    "    .withColumn(\"fp\", F.when((F.col(\"pred_thr\")==1) & (F.col(\"y\")==0), 1).otherwise(0))\n",
    "    .withColumn(\"tn\", F.when((F.col(\"pred_thr\")==0) & (F.col(\"y\")==0), 1).otherwise(0))\n",
    "    .withColumn(\"fn\", F.when((F.col(\"pred_thr\")==0) & (F.col(\"y\")==1), 1).otherwise(0))\n",
    "    .agg(F.sum(\"tp\").alias(\"tp\"), F.sum(\"fp\").alias(\"fp\"), F.sum(\"tn\").alias(\"tn\"), F.sum(\"fn\").alias(\"fn\"))\n",
    "    .collect()[0]\n",
    ")\n",
    "tp, fp, tn, fn = [int(agg[x]) for x in (\"tp\",\"fp\",\"tn\",\"fn\")]\n",
    "prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
    "rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
    "f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "acc  = (tp+tn)/(tp+tn+fp+fn)\n",
    "\n",
    "print(f\"Threshold={thr:.2f} | Test Acc={acc:.4f} | P={prec:.4f} | R={rec:.4f} | F1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1ee97",
   "metadata": {},
   "source": [
    "### K·∫øt lu·∫≠n & √ù nghƒ©a m√¥ h√¨nh\n",
    "\n",
    "#### So s√°nh hai m√¥ h√¨nh\n",
    "- **Logistic Regression (LR)** l√† m√¥ h√¨nh tuy·∫øn t√≠nh, ƒë∆°n gi·∫£n v√† d·ªÖ di·ªÖn gi·∫£i. Sau khi ƒëi·ªÅu ch·ªânh tr·ªçng s·ªë l·ªõp v√† ch·ªçn ng∆∞·ª°ng t·ªëi ∆∞u, LR ƒë·∫°t **F1 ‚âà 0.40**, **ROC-AUC ‚âà 0.73**, **PR-AUC ‚âà 0.37** tr√™n t·∫≠p validation.  \n",
    "- **Gradient Boosted Trees (GBT)** l√† m√¥ h√¨nh phi tuy·∫øn, m·∫°nh h∆°n trong vi·ªác b·∫Øt m·ªëi quan h·ªá ph·ª©c t·∫°p gi·ªØa c√°c bi·∫øn. Sau khi hi·ªáu ch·ªânh tham s·ªë v√† t·ªëi ∆∞u ng∆∞·ª°ng, GBT ƒë·∫°t **F1 ‚âà 0.43‚Äì0.45**, **ROC-AUC ‚âà 0.75**, **PR-AUC ‚âà 0.38** tr√™n test ‚Äî t·ªët h∆°n LR ·ªü h·∫ßu h·∫øt c√°c ch·ªâ s·ªë.  \n",
    "- Nh√¨n chung, **GBT c√≥ ƒë·ªô ch√≠nh x√°c v√† kh·∫£ nƒÉng t·ªïng qu√°t cao h∆°n**, ƒë·∫∑c bi·ªát trong vi·ªác nh·∫≠n di·ªán ƒë√∫ng nh√≥m kh√°ch h√†ng c√≥ kh·∫£ nƒÉng ‚Äúyes‚Äù cao h∆°n trung b√¨nh (Lift ‚âà 4√ó so v·ªõi ch·ªçn ng·∫´u nhi√™n). LR v·∫´n h·ªØu √≠ch cho m·ª•c ƒë√≠ch gi·∫£i th√≠ch v√† ki·ªÉm ch·ª©ng xu h∆∞·ªõng c·ªßa c√°c bi·∫øn.\n",
    "\n",
    "#### √ù nghƒ©a trong b√†i to√°n marketing\n",
    "- **M·ª•c ti√™u m√¥ h√¨nh:** d·ª± ƒëo√°n x√°c su·∫•t kh√°ch h√†ng s·∫Ω ƒë·ªìng √Ω tham gia s·∫£n ph·∫©m (bi·∫øn `y = yes/no`).  \n",
    "- K·∫øt qu·∫£ d·ª± ƒëo√°n gi√∫p **∆∞u ti√™n danh s√°ch kh√°ch h√†ng ti·ªÅm nƒÉng** ‚Äî thay v√¨ g·ªçi ng·∫´u nhi√™n, nh√¢n vi√™n marketing c√≥ th·ªÉ t·∫≠p trung v√†o nh√≥m c√≥ x√°c su·∫•t cao nh·∫•t, **ti·∫øt ki·ªám chi ph√≠ v√† th·ªùi gian**.  \n",
    "- Vi·ªác **ch·ªçn ng∆∞·ª°ng t·ªëi ∆∞u** (v√≠ d·ª• 0.2‚Äì0.3) cho ph√©p c√¢n b·∫±ng gi·ªØa **Precision** (ƒë·ªô ch√≠nh x√°c khi g·ªçi) v√† **Recall** (kh√¥ng b·ªè s√≥t kh√°ch h√†ng c√≥ ti·ªÅm nƒÉng).  \n",
    "- **√ù nghƒ©a ph√¢n t√≠ch:** m√¥ h√¨nh c≈©ng gi√∫p hi·ªÉu c√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh vi ‚Äúyes‚Äù, nh∆∞ ƒë·ªô tu·ªïi, t√¨nh tr·∫°ng vi·ªác l√†m, s·ªë l·∫ßn li√™n h·ªá, k√™nh li√™n l·∫°c (`contact`), v√† th·ªùi ƒëi·ªÉm g·ªçi (`month`, `day_of_week`), t·ª´ ƒë√≥ h·ªó tr·ª£ ra quy·∫øt ƒë·ªãnh marketing c√≥ c∆° s·ªü d·ªØ li·ªáu (data-driven marketing).\n",
    "\n",
    "#### Nguy√™n nh√¢n & H·∫°n ch·∫ø\n",
    "- **T·∫≠p d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng m·∫°nh:** ch·ªâ kho·∫£ng **11‚Äì12% kh√°ch h√†ng ‚Äúyes‚Äù**, khi·∫øn m√¥ h√¨nh kh√≥ h·ªçc ƒë∆∞·ª£c t√≠n hi·ªáu th·∫≠t v√† d·ªÖ b·ªã thi√™n l·ªách v·ªÅ l·ªõp ‚Äúno‚Äù.  \n",
    "- D√π ƒë√£ √°p d·ª•ng **class_weight** v√† **ch·ªçn ng∆∞·ª°ng t·ªëi ∆∞u**, nh∆∞ng **Precision v√† Recall v·∫´n gi·ªõi h·∫°n** v√¨ d·ªØ li·ªáu ch∆∞a ƒë·ªß phong ph√∫ v√† ph√¢n b·ªë ch∆∞a ƒë·ªÅu gi·ªØa c√°c nh√≥m kh√°ch h√†ng.  \n",
    "- **M·ªôt s·ªë ƒë·∫∑c tr∆∞ng c√≤n nhi·ªÖu ho·∫∑c tr√πng l·∫∑p th√¥ng tin** (v√≠ d·ª•: `previous`, `poutcome`, `campaign`) n√™n m√¥ h√¨nh kh√≥ khai th√°c ƒë∆∞·ª£c t√≠n hi·ªáu r√µ r√†ng.  \n",
    "- **C√°c y·∫øu t·ªë b√™n ngo√†i** (t√¢m l√Ω kh√°ch h√†ng, t√¨nh h√¨nh kinh t·∫ø, chi·∫øn d·ªãch marketing song song‚Ä¶) kh√¥ng ƒë∆∞·ª£c ghi nh·∫≠n trong d·ªØ li·ªáu, c≈©ng l√†m gi·∫£m kh·∫£ nƒÉng d·ª± ƒëo√°n tuy·ªát ƒë·ªëi c·ªßa m√¥ h√¨nh.\n",
    "\n",
    "**‚Üí K·∫øt lu·∫≠n:**  \n",
    "M√¥ h√¨nh GBT hi·ªán l√† l·ª±a ch·ªçn ph√π h·ª£p nh·∫•t ƒë·ªÉ tri·ªÉn khai th·ª±c t·∫ø, v√¨ ƒë·∫°t hi·ªáu nƒÉng cao v√† linh ho·∫°t trong vi·ªác ƒëi·ªÅu ch·ªânh ng∆∞·ª°ng theo chi·∫øn l∆∞·ª£c kinh doanh. Logistic Regression ƒë∆∞·ª£c gi·ªØ l√†m baseline ƒë·ªÉ theo d√µi, gi·∫£i th√≠ch v√† so s√°nh ƒë·ªãnh k·ª≥ trong qu√° tr√¨nh v·∫≠n h√†nh h·ªá th·ªëng d·ª± ƒëo√°n kh√°ch h√†ng ti·ªÅm nƒÉng.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
