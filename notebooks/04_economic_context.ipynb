{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48862ac5",
   "metadata": {},
   "source": [
    "# üß†  Economic Context & Synthesis (EDA + Modeling)\n",
    "## üéØ M·ª•c ti√™u\n",
    "Ph√¢n t√≠ch c√°c **bi·∫øn kinh t·∫ø vƒ© m√¥** ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác **kh√°ch h√†ng c√≥ g·ª≠i ti·ªÅn (y)** hay kh√¥ng.  \n",
    "Sau ƒë√≥, x√¢y d·ª±ng c√°c m√¥ h√¨nh d·ª± ƒëo√°n v·ªõi **PySpark**, x·ª≠ l√Ω d·ªØ li·ªáu **m·∫•t c√¢n b·∫±ng** b·∫±ng **SMOTE**,  v√† ƒë√°nh gi√° m√¥ h√¨nh b·∫±ng **Cross-validation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39183a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë d√≤ng: 41188\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp.var.rate: double (nullable = true)\n",
      " |-- cons.price.idx: double (nullable = true)\n",
      " |-- cons.conf.idx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nr.employed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "# T·∫°o SparkSession\n",
    "spark = SparkSession.builder.appName(\"Hao_Economic_Context_Synthesis\").getOrCreate()\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "data = spark.read.csv(\"../data/bank-additional/bank-additional-full.csv\", header=True, sep=';', inferSchema=True)\n",
    "print(\"S·ªë d√≤ng:\", data.count())\n",
    "data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "429271b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "|age|      job|marital|  education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "| 56|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 40|   admin.|married|   basic.6y|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 56| services|married|high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d3da1",
   "metadata": {},
   "source": [
    "## üîç 2Ô∏è‚É£ EDA - Ph√¢n t√≠ch ·∫£nh h∆∞·ªüng c·ªßa c√°c bi·∫øn kinh t·∫ø vƒ© m√¥\n",
    "·ªû ƒë√¢y ta t·∫≠p trung v√†o 5 bi·∫øn vƒ© m√¥:\n",
    "- `emp.var.rate` ‚Äì t·ª∑ l·ªá bi·∫øn ƒë·ªông vi·ªác l√†m  \n",
    "- `cons.price.idx` ‚Äì ch·ªâ s·ªë gi√° ti√™u d√πng  \n",
    "- `cons.conf.idx` ‚Äì ch·ªâ s·ªë ni·ªÅm tin ng∆∞·ªùi ti√™u d√πng  \n",
    "- `euribor3m` ‚Äì l√£i su·∫•t EURIBOR 3 th√°ng  \n",
    "- `nr.employed` ‚Äì s·ªë ng∆∞·ªùi c√≥ vi·ªác l√†m  \n",
    "\n",
    "Ta s·∫Ω xem x√©t **ph√¢n ph·ªëi**, **t∆∞∆°ng quan** v√† **·∫£nh h∆∞·ªüng** c·ªßa ch√∫ng t·ªõi bi·∫øn m·ª•c ti√™u `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d356058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªïi t√™n c·ªôt ƒë·ªÉ tr√°nh l·ªói c√≥ d·∫•u ch·∫•m\n",
    "data = (data\n",
    "    .withColumnRenamed(\"emp.var.rate\", \"emp_var_rate\")\n",
    "    .withColumnRenamed(\"cons.price.idx\", \"cons_price_idx\")\n",
    "    .withColumnRenamed(\"cons.conf.idx\", \"cons_conf_idx\")\n",
    "    .withColumnRenamed(\"nr.employed\", \"nr_employed\")\n",
    ")\n",
    "\n",
    "macro_cols = [\"emp_var_rate\", \"cons_price_idx\", \"cons_conf_idx\", \"euribor3m\", \"nr_employed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58b660bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1Ô∏è‚É£ Th·ªëng k√™ m√¥ t·∫£ c∆° b·∫£n ===\n",
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|emp_var_rate       |cons_price_idx    |cons_conf_idx     |euribor3m         |nr_employed      |\n",
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|count  |41188              |41188             |41188             |41188             |41188            |\n",
      "|mean   |0.08188550063178392|93.57566436828918 |-40.50260027191787|3.6212908128585366|5167.035910944004|\n",
      "|stddev |1.57095974051703   |0.5788400489541355|4.628197856174595 |1.7344474048512557|72.25152766825924|\n",
      "|min    |-3.4               |92.201            |-50.8             |0.634             |4963.6           |\n",
      "|max    |1.4                |94.767            |-26.9             |5.045             |5228.1           |\n",
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n",
      "=== 2Ô∏è‚É£ T·ª∑ l·ªá ph√¢n b·ªë bi·∫øn m·ª•c ti√™u y ===\n",
      "+---+-----+------------------+\n",
      "|  y|count|        percentage|\n",
      "+---+-----+------------------+\n",
      "| no|36548| 88.73458288821988|\n",
      "|yes| 4640|11.265417111780131|\n",
      "+---+-----+------------------+\n",
      "\n",
      "=== 3Ô∏è‚É£ Trung b√¨nh c√°c bi·∫øn vƒ© m√¥ theo t·ª´ng gi√° tr·ªã y ===\n",
      "+---+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "|y  |avg_emp_var_rate   |avg_cons_price_idx|avg_cons_conf_idx  |avg_euribor3m     |avg_nr_employed  |\n",
      "+---+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "|no |0.2488754514616543 |93.60375705922988 |-40.59309674947398 |3.8114911623072874|5176.166600086014|\n",
      "|yes|-1.2334482758620804|93.3543859913799  |-39.789784482759686|2.1231351293103327|5095.115991379181|\n",
      "+---+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, mean, stddev, min as _min, max as _max, count, corr\n",
    "from pyspark.sql.types import FloatType\n",
    "# === ƒê·ªîI T√äN C·ªòT CHO H·ª¢P L·ªÜ (n·∫øu ch∆∞a l√†m) ===\n",
    "data = (data\n",
    "    .withColumnRenamed(\"emp.var.rate\", \"emp_var_rate\")\n",
    "    .withColumnRenamed(\"cons.price.idx\", \"cons_price_idx\")\n",
    "    .withColumnRenamed(\"cons.conf.idx\", \"cons_conf_idx\")\n",
    "    .withColumnRenamed(\"nr.employed\", \"nr_employed\")\n",
    ")\n",
    "\n",
    "# Danh s√°ch c√°c bi·∫øn vƒ© m√¥\n",
    "macro_cols = [\"emp_var_rate\", \"cons_price_idx\", \"cons_conf_idx\", \"euribor3m\", \"nr_employed\"]\n",
    "\n",
    "# --- 1Ô∏è‚É£ Th·ªëng k√™ m√¥ t·∫£ c∆° b·∫£n ---\n",
    "print(\"=== 1Ô∏è‚É£ Th·ªëng k√™ m√¥ t·∫£ c∆° b·∫£n ===\")\n",
    "data.select(macro_cols).describe().show(truncate=False)\n",
    "\n",
    "# --- 2Ô∏è‚É£ T·ª∑ l·ªá c√°c l·ªõp m·ª•c ti√™u (y) ---\n",
    "print(\"=== 2Ô∏è‚É£ T·ª∑ l·ªá ph√¢n b·ªë bi·∫øn m·ª•c ti√™u y ===\")\n",
    "total_count = data.count()\n",
    "data.groupBy(\"y\").agg(\n",
    "    count(\"*\").alias(\"count\")\n",
    ").withColumn(\n",
    "    \"percentage\", (col(\"count\") / total_count * 100)\n",
    ").show()\n",
    "\n",
    "# --- 3Ô∏è‚É£ Trung b√¨nh c√°c bi·∫øn vƒ© m√¥ theo t·ª´ng gi√° tr·ªã y ---\n",
    "print(\"=== 3Ô∏è‚É£ Trung b√¨nh c√°c bi·∫øn vƒ© m√¥ theo t·ª´ng gi√° tr·ªã y ===\")\n",
    "agg_exprs = [mean(c).alias(f\"avg_{c}\") for c in macro_cols]\n",
    "data.groupBy(\"y\").agg(*agg_exprs).show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76fafa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 4Ô∏è‚É£ Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn vƒ© m√¥ v√† output ===\n",
      "+--------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
      "|Feature       |emp_var_rate       |cons_price_idx      |cons_conf_idx       |euribor3m           |nr_employed        |y_num               |\n",
      "+--------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
      "|emp_var_rate  |1.0                |0.775334170834832   |0.19604126813197284 |0.9722446711516147  |0.9069701012560353 |-0.29833442615937794|\n",
      "|cons_price_idx|0.775334170834832  |1.0                 |0.058986181748833216|0.688230107037495   |0.5220339770130168 |-0.1362112128191817 |\n",
      "|cons_conf_idx |0.19604126813197287|0.05898618174883324 |1.0                 |0.27768621966375506 |0.10051343183753894|0.05487794605319229 |\n",
      "|euribor3m     |0.9722446711516147 |0.6882301070374951  |0.27768621966375506 |1.0                 |0.9451544313982513 |-0.30777140394071995|\n",
      "|nr_employed   |0.9069701012560353 |0.5220339770130166  |0.10051343183753896 |0.9451544313982513  |1.0                |-0.3546782959214352 |\n",
      "|y_num         |-0.2983344261593779|-0.13621121281918172|0.054877946053192316|-0.30777140394071995|-0.3546782959214352|1.0                 |\n",
      "+--------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, when, col\n",
    "import math\n",
    "\n",
    "data = data.withColumn(\"y_num\", when(col(\"y\") == \"yes\", 1.0).when(col(\"y\") == \"no\", 0.0).otherwise(col(\"y\").cast(\"double\")))\n",
    "\n",
    "# --- 4Ô∏è‚É£ Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn vƒ© m√¥ v√† output ---\n",
    "print(\"=== 4Ô∏è‚É£ Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn vƒ© m√¥ v√† output ===\")\n",
    "\n",
    "# Th√™m bi·∫øn y v√†o danh s√°ch\n",
    "all_cols = macro_cols + [\"y_num\"]\n",
    "\n",
    "corr_matrix = []\n",
    "for c1 in all_cols:\n",
    "    row = []\n",
    "    for c2 in all_cols:\n",
    "        try:\n",
    "            val = data.select(corr(c1, c2).alias(\"corr\")).collect()[0][\"corr\"]\n",
    "            if val is None or math.isnan(val):\n",
    "                val = 0.0\n",
    "        except Exception:\n",
    "            val = 0.0\n",
    "        row.append(float(val))\n",
    "    corr_matrix.append((c1, *row))\n",
    "\n",
    "# T·∫°o DataFrame hi·ªÉn th·ªã ma tr·∫≠n t∆∞∆°ng quan\n",
    "columns = [\"Feature\"] + all_cols\n",
    "corr_df = spark.createDataFrame(corr_matrix, columns)\n",
    "\n",
    "corr_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c17a89",
   "metadata": {},
   "source": [
    "- Nh√≥m bi·∫øn emp.var.rate, euribor3m, nr.employed c√≥ th·ªÉ ƒë∆∞·ª£c r√∫t g·ªçn ho·∫∑c ch·ªçn 1‚Äì2 ƒë·∫°i di·ªán.\n",
    "\n",
    "- C√°c bi·∫øn c√≥ t∆∞∆°ng quan √¢m v·ªõi y\n",
    "\n",
    "- Khi c√°c ch·ªâ s·ªë kinh t·∫ø n√†y tƒÉng (bi·ªÉu hi·ªán cho t√¨nh h√¨nh kinh t·∫ø t·ªët h∆°n), kh·∫£ nƒÉng kh√°ch h√†ng ƒë·ªìng √Ω g·ª≠i ti·ªÅn (y=1) l·∫°i gi·∫£m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877fcade",
   "metadata": {},
   "source": [
    "| Bi·∫øn vƒ© m√¥ (`feature`) | √ù nghƒ©a kinh t·∫ø                 | Xu h∆∞·ªõng khi bi·∫øn tƒÉng             | ·∫¢nh h∆∞·ªüng ƒë·∫øn kh·∫£ nƒÉng g·ª≠i ti·ªÅn (`y=1`) | M·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng  | Ghi ch√∫                                                                    |\n",
    "| ---------------------- | ------------------------------- | ---------------------------------- | --------------------------------------- | ----------------- | -------------------------------------------------------------------------- |\n",
    "| **emp.var.rate**       | T·ª∑ l·ªá bi·∫øn ƒë·ªông vi·ªác l√†m        | Kinh t·∫ø ·ªïn ƒë·ªãnh h∆°n, vi·ªác l√†m tƒÉng | üîª Gi·∫£m kh·∫£ nƒÉng g·ª≠i ti·ªÅn               | **M·∫°nh (√¢m)**     | Khi t·ª∑ l·ªá vi·ªác l√†m cao, ng∆∞·ªùi d√¢n chi ti√™u nhi·ªÅu h∆°n, g·ª≠i ti·∫øt ki·ªám √≠t h∆°n |\n",
    "| **cons.price.idx**     | Ch·ªâ s·ªë gi√° ti√™u d√πng (l·∫°m ph√°t) | L·∫°m ph√°t tƒÉng                      | üîª Gi·∫£m nh·∫π kh·∫£ nƒÉng g·ª≠i ti·ªÅn           | **Y·∫øu (√¢m)**      | ·∫¢nh h∆∞·ªüng nh·ªè, th·ªÉ hi·ªán qua ch√™nh l·ªách nh·ªè gi·ªØa hai nh√≥m                   |\n",
    "| **cons.conf.idx**      | Ni·ªÅm tin ng∆∞·ªùi ti√™u d√πng        | T√¢m l√Ω ti√™u d√πng t√≠ch c·ª±c h∆°n      | ‚ö™ G·∫ßn nh∆∞ kh√¥ng ·∫£nh h∆∞·ªüng               | **R·∫•t y·∫øu**       | Ph√¢n b·ªë hai nh√≥m g·∫ßn nh∆∞ tr√πng nhau                                        |\n",
    "| **euribor3m**          | L√£i su·∫•t Euribor 3 th√°ng        | L√£i su·∫•t th·ªã tr∆∞·ªùng tƒÉng           | üîª Gi·∫£m m·∫°nh kh·∫£ nƒÉng g·ª≠i ti·ªÅn          | **M·∫°nh (√¢m)**     | Khi l√£i su·∫•t cao, kh√°ch h√†ng ∆∞u ti√™n ƒë·∫ßu t∆∞ h∆°n l√† g·ª≠i ti·∫øt ki·ªám           |\n",
    "| **nr.employed**        | S·ªë l∆∞·ª£ng ng∆∞·ªùi c√≥ vi·ªác l√†m      | Vi·ªác l√†m nhi·ªÅu h∆°n                 | üîª Gi·∫£m kh·∫£ nƒÉng g·ª≠i ti·ªÅn               | **Kh√° m·∫°nh (√¢m)** | Ph·∫£n √°nh m·ªëi li√™n h·ªá gi·ªØa th·ªã tr∆∞·ªùng lao ƒë·ªông v√† h√†nh vi ti·∫øt ki·ªám         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856a8e5",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 3Ô∏è‚É£ X·ª¨ L√ù M·∫§T C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\n",
    "D·ªØ li·ªáu b·ªã **m·∫•t c√¢n b·∫±ng** khi t·ª∑ l·ªá kh√°ch h√†ng g·ª≠i ti·ªÅn (`y=1`) r·∫•t nh·ªè so v·ªõi `y=0`.  \n",
    "Ta d√πng **SMOTE (Synthetic Minority Oversampling Technique)** ƒë·ªÉ t·∫°o th√™m m·∫´u thi·ªÉu s·ªë nh√¢n t·∫°o.  \n",
    "- ∆Øu ƒëi·ªÉm: Gi·ªØ l·∫°i to√†n b·ªô d·ªØ li·ªáu g·ªëc, tr√°nh m·∫•t th√¥ng tin.  \n",
    "- Nh∆∞·ª£c ƒëi·ªÉm: C√≥ th·ªÉ t·∫°o nhi·ªÖu n·∫øu d·ªØ li·ªáu thi·ªÉu s·ªë kh√¥ng ph√¢n b·ªë t·ªët.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7881d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== T·ª∑ l·ªá nh√£n ban ƒë·∫ßu ===\n",
      "+---+-----+\n",
      "|  y|count|\n",
      "+---+-----+\n",
      "| no|36548|\n",
      "|yes| 4640|\n",
      "+---+-----+\n",
      "\n",
      "S·ªë l∆∞·ª£ng ban ƒë·∫ßu -> YES: 4640, NO: 36548\n",
      "=== D·ªØ li·ªáu sau oversampling (SMOTE th·ªß c√¥ng) ===\n",
      "+---+-----+---------+\n",
      "|  y|count|t·ª∑ l·ªá (%)|\n",
      "+---+-----+---------+\n",
      "| no|36548|     50.0|\n",
      "|yes|36548|     50.0|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Row, SparkSession\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "\n",
    "# --- 1Ô∏è‚É£ Ki·ªÉm tra t·ª∑ l·ªá nh√£n ---\n",
    "print(\"=== T·ª∑ l·ªá nh√£n ban ƒë·∫ßu ===\")\n",
    "data.groupBy(\"y\").count().show()\n",
    "\n",
    "# --- 2Ô∏è‚É£ Ph√¢n t√°ch d·ªØ li·ªáu ---\n",
    "yes_df = data.filter(col(\"y\") == \"yes\")\n",
    "no_df  = data.filter(col(\"y\") == \"no\")\n",
    "\n",
    "yes_count = yes_df.count()\n",
    "no_count  = no_df.count()\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng ban ƒë·∫ßu -> YES: {yes_count}, NO: {no_count}\")\n",
    "\n",
    "# --- 3Ô∏è‚É£ N·∫øu m·∫•t c√¢n b·∫±ng, th·ª±c hi·ªán oversampling ---\n",
    "if yes_count < no_count:\n",
    "    diff = no_count - yes_count\n",
    "    yes_rows = yes_df.collect()\n",
    "    new_rows = []\n",
    "\n",
    "    for i in range(diff):\n",
    "        r1, r2 = random.sample(yes_rows, 2)\n",
    "        new_data = {}\n",
    "        for c in data.columns:\n",
    "            if c == \"y\":\n",
    "                new_data[c] = \"yes\"\n",
    "            else:\n",
    "                try:\n",
    "                    v1 = float(r1[c])\n",
    "                    v2 = float(r2[c])\n",
    "                    alpha = random.random()\n",
    "                    new_data[c] = v1 + alpha * (v2 - v1)\n",
    "                except Exception:\n",
    "                    new_data[c] = r1[c]\n",
    "        new_rows.append(Row(**new_data))\n",
    "\n",
    "    # --- üîß Chuy·ªÉn schema ƒë·ªÉ ch·∫•p nh·∫≠n DoubleType ---\n",
    "    new_schema = StructType([\n",
    "        StructField(f.name,\n",
    "                    DoubleType() if isinstance(f.dataType, IntegerType) else f.dataType)\n",
    "        for f in data.schema\n",
    "    ])\n",
    "\n",
    "    new_df = spark.createDataFrame(new_rows, schema=new_schema)\n",
    "\n",
    "    # --- üîß Union theo t√™n c·ªôt (t·ª± √©p ki·ªÉu t∆∞∆°ng th√≠ch) ---\n",
    "    data_balanced = data.unionByName(new_df)\n",
    "else:\n",
    "    data_balanced = data\n",
    "\n",
    "# --- 4Ô∏è‚É£ Ki·ªÉm tra k·∫øt qu·∫£ sau c√¢n b·∫±ng ---\n",
    "print(\"=== D·ªØ li·ªáu sau oversampling (SMOTE th·ªß c√¥ng) ===\")\n",
    "data_balanced.groupBy(\"y\").count().withColumn(\n",
    "    \"t·ª∑ l·ªá (%)\", F.round(col(\"count\") / data_balanced.count() * 100, 2)\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b669d",
   "metadata": {},
   "source": [
    "## ü§ñ 4Ô∏è‚É£ X√ÇY D·ª∞NG M√î H√åNH C∆† B·∫¢N\n",
    "Ta b·∫Øt ƒë·∫ßu b·∫±ng c√°c **m√¥ h√¨nh c∆° b·∫£n** ƒë·ªÉ thi·∫øt l·∫≠p baseline:  \n",
    "1. **Logistic Regression** ‚Äì ƒë∆°n gi·∫£n, d·ªÖ gi·∫£i th√≠ch, d√πng l√†m baseline.  \n",
    "2. **Random Forest** ‚Äì b·∫Øt ƒë∆∞·ª£c quan h·ªá phi tuy·∫øn gi·ªØa c√°c bi·∫øn.  \n",
    "3. **Gradient Boosted Trees (GBT)** ‚Äì h·ªçc s√¢u h∆°n m·ªëi quan h·ªá vƒ© m√¥.  \n",
    "\n",
    "Vi·ªác ƒë√°nh gi√° d√πng ch·ªâ s·ªë **Accuracy, Precision, Recall, F1-score**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "108e945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "\n",
    "# ƒê·ªïi t√™n ƒë·ªÉ tr√°nh l·ªói k√Ω t·ª± \".\"\n",
    "data_balanced = (data_balanced\n",
    "    .withColumnRenamed(\"emp.var.rate\", \"emp_var_rate\")\n",
    "    .withColumnRenamed(\"cons.price.idx\", \"cons_price_idx\")\n",
    "    .withColumnRenamed(\"cons.conf.idx\", \"cons_conf_idx\")\n",
    "    .withColumnRenamed(\"nr.employed\", \"nr_employed\")\n",
    ")\n",
    "\n",
    "# Vector features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"emp_var_rate\", \"cons_price_idx\", \"cons_conf_idx\", \"euribor3m\", \"nr_employed\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "final_data = assembler.transform(data_balanced).select(\"features\", \"y\")\n",
    "\n",
    "# Encode nh√£n (PySpark y√™u c·∫ßu d·∫°ng s·ªë)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
    "final_data = indexer.fit(final_data).transform(final_data)\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6ff4de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√¥ h√¨nh: Logistic Regression ===\n",
      "\n",
      "Confusion Matrix (Logistic Regression):\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     5275   |     2010\n",
      "Actual=1     |     1894   |     5277\n",
      "\n",
      "Precision: 0.7242, Recall: 0.7359, F1: 0.7300, Accuracy: 0.7299\n",
      "\n",
      "=== M√¥ h√¨nh: Random Forest ===\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     6691   |      594\n",
      "Actual=1     |      849   |     6322\n",
      "\n",
      "Precision: 0.9141, Recall: 0.8816, F1: 0.8976, Accuracy: 0.9002\n",
      "\n",
      "=== M√¥ h√¨nh: Gradient Boosted Trees ===\n",
      "\n",
      "Confusion Matrix (Gradient Boosted Trees):\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     6691   |      594\n",
      "Actual=1     |      722   |     6449\n",
      "\n",
      "Precision: 0.9157, Recall: 0.8993, F1: 0.9074, Accuracy: 0.9090\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- Kh·ªüi t·∫°o m√¥ h√¨nh ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
    "    \"Random Forest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50),\n",
    "    \"Gradient Boosted Trees\": GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "}\n",
    "\n",
    "# --- H√†m t·ª± t√≠nh c√°c ch·ªâ s·ªë ---\n",
    "def compute_metrics(df):\n",
    "    TP = df.filter((col(\"label\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "    FP = df.filter((col(\"label\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "    FN = df.filter((col(\"label\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "    TN = df.filter((col(\"label\") == 0) & (col(\"prediction\") == 0)).count()\n",
    "    \n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1, accuracy, TP, FP, FN, TN\n",
    "\n",
    "# --- Hu·∫•n luy·ªán & ƒë√°nh gi√° ---\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== M√¥ h√¨nh: {name} ===\")\n",
    "    model_fit = model.fit(train_data)\n",
    "    pred = model_fit.transform(test_data)\n",
    "    \n",
    "    precision, recall, f1, accuracy, TP, FP, FN, TN = compute_metrics(pred)\n",
    "\n",
    "    # --- Hi·ªÉn th·ªã Confusion Matrix d·∫°ng b·∫£ng ---\n",
    "    print(f\"\\nConfusion Matrix ({name}):\")\n",
    "    print(\"                 Pred=0     Pred=1\")\n",
    "    print(f\"Actual=0     |   {TN:6d}   |   {FP:6d}\")\n",
    "    print(f\"Actual=1     |   {FN:6d}   |   {TP:6d}\")\n",
    "\n",
    "    print(f\"\\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    results.append((name, precision, recall, f1, accuracy, TP, FP, FN, TN))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a58be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== T·ªïng h·ª£p k·∫øt qu·∫£ ===\n",
      "+----------------------+------------------+------------------+------------------+------------------+----+----+----+----+\n",
      "|Model                 |Precision         |Recall            |F1                |Accuracy          |TP  |FP  |FN  |TN  |\n",
      "+----------------------+------------------+------------------+------------------+------------------+----+----+----+----+\n",
      "|Logistic Regression   |0.7241663235899547|0.7358806303165528|0.7299764836076912|0.7299391256225789|5277|2010|1894|5275|\n",
      "|Random Forest         |0.9141122035858879|0.8816064705062056|0.897565130971818 |0.9001798561151079|6322|594 |849 |6691|\n",
      "|Gradient Boosted Trees|0.9156609399403663|0.8993166922326036|0.9074152244266218|0.9089651355838406|6449|594 |722 |6691|\n",
      "+----------------------+------------------+------------------+------------------+------------------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- T·∫°o b·∫£ng t·ªïng h·ª£p ---\n",
    "print(\"\\n=== T·ªïng h·ª£p k·∫øt qu·∫£ ===\")\n",
    "summary_df = spark.createDataFrame(\n",
    "    results, \n",
    "    [\"Model\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\", \"TP\", \"FP\", \"FN\", \"TN\"]\n",
    ")\n",
    "summary_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa977d6d",
   "metadata": {},
   "source": [
    "## üìä 5Ô∏è‚É£ K·∫æT QU·∫¢ ƒê√ÅNH GI√Å\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c542bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ƒê·ªô quan tr·ªçng (Logistic Regression) ===\n",
      "emp_var_rate        : 0.273858\n",
      "cons_price_idx      : 0.532919\n",
      "cons_conf_idx       : 0.061193\n",
      "euribor3m           : 0.216819\n",
      "nr_employed         : 0.008131\n",
      "\n",
      "=== ƒê·ªô quan tr·ªçng (Random Forest) ===\n",
      "emp_var_rate        : 0.076077\n",
      "cons_price_idx      : 0.046925\n",
      "cons_conf_idx       : 0.256145\n",
      "euribor3m           : 0.237378\n",
      "nr_employed         : 0.383475\n",
      "\n",
      "=== ƒê·ªô quan tr·ªçng (Gradient Boosted Trees) ===\n",
      "emp_var_rate        : 0.239829\n",
      "cons_price_idx      : 0.125229\n",
      "cons_conf_idx       : 0.293558\n",
      "euribor3m           : 0.073672\n",
      "nr_employed         : 0.267712\n",
      "\n",
      "=== T·ªïng k·∫øt bi·∫øn quan tr·ªçng nh·∫•t ===\n",
      "M√¥ h√¨nh                        Bi·∫øn quan tr·ªçng nh·∫•t         Gi√° tr·ªã\n",
      "Logistic Regression            cons_price_idx              0.532919\n",
      "Random Forest                  nr_employed                 0.383475\n",
      "Gradient Boosted Trees         cons_conf_idx               0.293558\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\"emp_var_rate\", \"cons_price_idx\", \"cons_conf_idx\", \"euribor3m\", \"nr_employed\"]\n",
    "importance_summary = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    fitted = model.fit(train_data)\n",
    "    if hasattr(fitted, \"featureImportances\"):\n",
    "        importances = list(fitted.featureImportances)\n",
    "    elif hasattr(fitted, \"coefficients\"):\n",
    "        importances = [abs(x) for x in fitted.coefficients]\n",
    "    else:\n",
    "        importances = [0.0]*len(feature_cols)\n",
    "    \n",
    "    pairs = list(zip(feature_cols, importances))\n",
    "    print(f\"\\n=== ƒê·ªô quan tr·ªçng ({name}) ===\")\n",
    "    for col_name, imp in pairs:\n",
    "        print(f\"{col_name:<20}: {imp:.6f}\")\n",
    "    \n",
    "    max_idx = importances.index(max(importances, key=abs))\n",
    "    importance_summary.append((name, feature_cols[max_idx], importances[max_idx]))\n",
    "\n",
    "# T·ªïng k·∫øt\n",
    "print(\"\\n=== T·ªïng k·∫øt bi·∫øn quan tr·ªçng nh·∫•t ===\")\n",
    "print(f\"{'M√¥ h√¨nh':<30} {'Bi·∫øn quan tr·ªçng nh·∫•t':<25} {'Gi√° tr·ªã':>10}\")\n",
    "for row in importance_summary:\n",
    "    print(f\"{row[0]:<30} {row[1]:<25} {row[2]:>10.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb41158",
   "metadata": {},
   "source": [
    "##  Ph√¢n t√≠ch theo th·ªùi gian v√† bi·∫øn kinh t·∫ø vƒ© m√¥\n",
    "\n",
    "M·ª•c ti√™u c·ªßa ph·∫ßn n√†y l√† m·ªü r·ªông EDA b·∫±ng c√°ch xem **xu h∆∞·ªõng th·ªùi gian** ·∫£nh h∆∞·ªüng th·∫ø n√†o ƒë·∫øn kh·∫£ nƒÉng kh√°ch h√†ng ƒëƒÉng k√Ω g·ª≠i ti·ªÅn (`y = yes`).  \n",
    "C√°c b∆∞·ªõc ch√≠nh:\n",
    "1. T·∫°o th√™m c√°c bi·∫øn th·ªùi gian (th√°ng s·ªë, m√πa).  \n",
    "2. Ph√¢n t√≠ch t·ª∑ l·ªá ƒëƒÉng k√Ω (`conversion rate`) theo **th√°ng** v√† **m√πa**.  \n",
    "3. K·∫øt h·ª£p c√°c bi·∫øn kinh t·∫ø vƒ© m√¥ ƒë·ªÉ t√¨m hi·ªÉu **trong th·ªùi ƒëi·ªÉm t·ª∑ l·ªá g·ª≠i ti·ªÅn cao, c√°c bi·∫øn kinh t·∫ø vƒ© m√¥ c√≥ ƒë·∫∑c ƒëi·ªÉm g√¨**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5b213b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ th√™m bi·∫øn month_num v√† season\n",
      "+-----+---------+------+\n",
      "|month|month_num|season|\n",
      "+-----+---------+------+\n",
      "|  0.0|     NULL|winter|\n",
      "+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1Ô∏è‚É£ T·∫°o th√™m c√°c bi·∫øn th·ªùi gian (th√°ng s·ªë, m√πa) ---\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Map th√°ng sang s·ªë\n",
    "month_mapping = {\n",
    "    \"jan\": 1, \"feb\": 2, \"mar\": 3, \"apr\": 4,\n",
    "    \"may\": 5, \"jun\": 6, \"jul\": 7, \"aug\": 8,\n",
    "    \"sep\": 9, \"oct\": 10, \"nov\": 11, \"dec\": 12\n",
    "}\n",
    "\n",
    "# Bi·∫øn th√°ng d·∫°ng s·ªë\n",
    "month_expr = None\n",
    "for m, n in month_mapping.items():\n",
    "    expr = when(col(\"month\") == m, n)\n",
    "    month_expr = expr if month_expr is None else month_expr.when(col(\"month\") == m, n)\n",
    "month_expr = month_expr.otherwise(None)\n",
    "\n",
    "data = data.withColumn(\"month_num\", month_expr)\n",
    "\n",
    "# T·∫°o bi·∫øn m√πa (xu√¢n, h·∫°, thu, ƒë√¥ng)\n",
    "data = data.withColumn(\n",
    "    \"season\",\n",
    "    when(col(\"month_num\").isin(3,4,5), \"spring\")\n",
    "    .when(col(\"month_num\").isin(6,7,8), \"summer\")\n",
    "    .when(col(\"month_num\").isin(9,10,11), \"autumn\")\n",
    "    .otherwise(\"winter\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ th√™m bi·∫øn month_num v√† season\")\n",
    "data.select(\"month\", \"month_num\", \"season\").distinct().orderBy(\"month_num\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79dddf",
   "metadata": {},
   "source": [
    "###  Ph√¢n t√≠ch t·ª∑ l·ªá ƒëƒÉng k√Ω theo th·ªùi gian\n",
    "Xem x√©t ph√¢n b·ªë bi·∫øn m·ª•c ti√™u `y` theo **th√°ng** v√† **m√πa** ƒë·ªÉ x√°c ƒë·ªãnh giai ƒëo·∫°n n√†o c√≥ nhi·ªÅu kh√°ch h√†ng g·ª≠i ti·ªÅn nh·∫•t.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5f7e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== T·ª∑ l·ªá kh√°ch h√†ng ƒëƒÉng k√Ω theo th√°ng ===\n",
      "+-----+-----+---------+------------------+\n",
      "|month|total|yes_count|conversion_rate   |\n",
      "+-----+-----+---------+------------------+\n",
      "|0.0  |41188|4640     |11.265417111780131|\n",
      "+-----+-----+---------+------------------+\n",
      "\n",
      "=== T·ª∑ l·ªá kh√°ch h√†ng ƒëƒÉng k√Ω theo m√πa ===\n",
      "+------+-----+---------+------------------+\n",
      "|season|total|yes_count|conversion_rate   |\n",
      "+------+-----+---------+------------------+\n",
      "|winter|41188|4640     |11.265417111780131|\n",
      "+------+-----+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "# --- 2Ô∏è‚É£ T·ª∑ l·ªá ƒëƒÉng k√Ω theo th√°ng ---\n",
    "print(\"=== T·ª∑ l·ªá kh√°ch h√†ng ƒëƒÉng k√Ω theo th√°ng ===\")\n",
    "conv_by_month = (\n",
    "    data.groupBy(\"month\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total\"),\n",
    "        sum(when(col(\"y\") == \"yes\", 1).otherwise(0)).alias(\"yes_count\")\n",
    "    )\n",
    "    .withColumn(\"conversion_rate\", col(\"yes_count\") / col(\"total\") * 100)\n",
    "    .orderBy(\"month\")\n",
    ")\n",
    "conv_by_month.show(truncate=False)\n",
    "\n",
    "# --- 3Ô∏è‚É£ T·ª∑ l·ªá ƒëƒÉng k√Ω theo m√πa ---\n",
    "print(\"=== T·ª∑ l·ªá kh√°ch h√†ng ƒëƒÉng k√Ω theo m√πa ===\")\n",
    "conv_by_season = (\n",
    "    data.groupBy(\"season\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total\"),\n",
    "        sum(when(col(\"y\") == \"yes\", 1).otherwise(0)).alias(\"yes_count\")\n",
    "    )\n",
    "    .withColumn(\"conversion_rate\", col(\"yes_count\") / col(\"total\") * 100)\n",
    "    .orderBy(\"season\")\n",
    ")\n",
    "conv_by_season.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6108dae",
   "metadata": {},
   "source": [
    "###  Li√™n h·ªá gi·ªØa t·ª∑ l·ªá ƒëƒÉng k√Ω v√† c√°c bi·∫øn kinh t·∫ø vƒ© m√¥\n",
    "\n",
    "B∆∞·ªõc n√†y gi√∫p hi·ªÉu r√µ **khi t·ª∑ l·ªá g·ª≠i ti·ªÅn cao**, c√°c y·∫øu t·ªë nh∆∞ **l√£i su·∫•t, vi·ªác l√†m, l·∫°m ph√°t, ni·ªÅm tin ti√™u d√πng** c√≥ gi√° tr·ªã nh∆∞ th·∫ø n√†o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a4f69e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== So s√°nh t·ª∑ l·ªá ƒëƒÉng k√Ω v√† ƒë·∫∑c tr∆∞ng kinh t·∫ø t·ª´ng th√°ng ===\n",
      "+-----+-----+---------+------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|month|total|yes_count|conversion_rate   |avg_emp_var_rate   |avg_cons_price_idx|avg_cons_conf_idx |avg_euribor3m     |avg_nr_employed  |\n",
      "+-----+-----+---------+------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|0.0  |41188|4640     |11.265417111780131|0.08188550063178392|93.57566436828918 |-40.50260027191787|3.6212908128585366|5167.035910944004|\n",
      "+-----+-----+---------+------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "macro_cols = [\"emp_var_rate\", \"cons_price_idx\", \"cons_conf_idx\", \"euribor3m\", \"nr_employed\"]\n",
    "\n",
    "# --- 4Ô∏è‚É£ Trung b√¨nh bi·∫øn vƒ© m√¥ theo t·ª´ng th√°ng ---\n",
    "macro_by_month = data.groupBy(\"month\").agg(\n",
    "    *[mean(c).alias(f\"avg_{c}\") for c in macro_cols]\n",
    ")\n",
    "\n",
    "# --- 5Ô∏è‚É£ K·∫øt h·ª£p v·ªõi conversion rate ---\n",
    "result = conv_by_month.join(macro_by_month, on=\"month\", how=\"inner\")\n",
    "\n",
    "# --- 6Ô∏è‚É£ Hi·ªÉn th·ªã k·∫øt qu·∫£ theo th·ª© t·ª± conversion gi·∫£m d·∫ßn ---\n",
    "print(\"=== So s√°nh t·ª∑ l·ªá ƒëƒÉng k√Ω v√† ƒë·∫∑c tr∆∞ng kinh t·∫ø t·ª´ng th√°ng ===\")\n",
    "result.orderBy(col(\"conversion_rate\").desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2fb2b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω d·ªØ li·ªáu ‚Äî t·∫•t c·∫£ bi·∫øn ƒë√£ numeric v√† kh√¥ng null.\n",
      "+---------------------------------------+---+-----+\n",
      "|features                               |y  |label|\n",
      "+---------------------------------------+---+-----+\n",
      "|[1.1,93.994,-36.4,4.857,5191.0,0.0,1.0]|no |0.0  |\n",
      "|[1.1,93.994,-36.4,4.857,5191.0,0.0,1.0]|no |0.0  |\n",
      "|[1.1,93.994,-36.4,4.857,5191.0,0.0,1.0]|no |0.0  |\n",
      "+---------------------------------------+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# --- Th√™m th√°ng & m√πa ---\n",
    "data_balanced = data_balanced.withColumn(\"month\", col(\"month\").cast(\"int\"))\n",
    "\n",
    "data_balanced = data_balanced.withColumn(\n",
    "    \"season\",\n",
    "    when(col(\"month\").isin(12, 1, 2), lit(\"Winter\"))\n",
    "    .when(col(\"month\").isin(3, 4, 5), lit(\"Spring\"))\n",
    "    .when(col(\"month\").isin(6, 7, 8), lit(\"Summer\"))\n",
    "    .when(col(\"month\").isin(9, 10, 11), lit(\"Autumn\"))\n",
    "    .otherwise(lit(\"Unknown\"))\n",
    ")\n",
    "\n",
    "# --- ƒê·∫£m b·∫£o kh√¥ng c√≥ null ---\n",
    "data_balanced = data_balanced.na.fill({\n",
    "    \"emp_var_rate\": 0.0,\n",
    "    \"cons_price_idx\": 0.0,\n",
    "    \"cons_conf_idx\": 0.0,\n",
    "    \"euribor3m\": 0.0,\n",
    "    \"nr_employed\": 0.0,\n",
    "    \"month\": 0,\n",
    "    \"season\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# --- Encode c·ªôt season ---\n",
    "season_indexer = StringIndexer(\n",
    "    inputCol=\"season\", outputCol=\"season_index\", handleInvalid=\"keep\"\n",
    ")\n",
    "data_encoded = season_indexer.fit(data_balanced).transform(data_balanced)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"season_index\"], outputCols=[\"season_vec\"])\n",
    "data_encoded = encoder.fit(data_encoded).transform(data_encoded)\n",
    "\n",
    "# --- √âp ki·ªÉu t·∫•t c·∫£ c·ªôt numeric ---\n",
    "numeric_cols = [\"emp_var_rate\", \"cons_price_idx\", \"cons_conf_idx\", \"euribor3m\", \"nr_employed\", \"month\"]\n",
    "for col_name in numeric_cols:\n",
    "    data_encoded = data_encoded.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "\n",
    "# --- Assemble features ---\n",
    "feature_cols = numeric_cols + [\"season_vec\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "final_data = assembler.transform(data_encoded).select(\"features\", \"y\")\n",
    "\n",
    "# --- Encode nh√£n ---\n",
    "indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
    "final_data = indexer.fit(final_data).transform(final_data)\n",
    "\n",
    "# --- T√°ch t·∫≠p train/test ---\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω d·ªØ li·ªáu ‚Äî t·∫•t c·∫£ bi·∫øn ƒë√£ numeric v√† kh√¥ng null.\")\n",
    "final_data.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dce971",
   "metadata": {},
   "source": [
    "##  Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi Cross Validation\n",
    "√Åp d·ª•ng Logistic Regression, Random Forest v√† Gradient Boosted Trees ‚Äî th√™m tuning si√™u tham s·ªë.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9cbdbda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training model: Logistic Regression\n",
      "\n",
      "Confusion Matrix (Logistic Regression):\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     5275   |     2010\n",
      "Actual=1     |     1883   |     5288\n",
      "\n",
      "Precision: 0.7246, Recall: 0.7374, F1: 0.7309, Accuracy: 0.7307, AUC: 0.8048\n",
      "\n",
      "üöÄ Training model: Random Forest\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     6874   |      411\n",
      "Actual=1     |      813   |     6358\n",
      "\n",
      "Precision: 0.9393, Recall: 0.8866, F1: 0.9122, Accuracy: 0.9153, AUC: 0.9610\n",
      "\n",
      "üöÄ Training model: Gradient Boosted Trees\n",
      "\n",
      "Confusion Matrix (Gradient Boosted Trees):\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     6796   |      489\n",
      "Actual=1     |      748   |     6423\n",
      "\n",
      "Precision: 0.9293, Recall: 0.8957, F1: 0.9122, Accuracy: 0.9144, AUC: 0.9628\n",
      "\n",
      "‚úÖ ƒê√£ train v√† l∆∞u xong t·∫•t c·∫£ m√¥ h√¨nh. Bi·∫øn `best_models` v√† `cv_results` ƒë√£ s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Kh·ªüi t·∫°o m√¥ h√¨nh ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
    "    \"Random Forest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\"),\n",
    "    \"Gradient Boosted Trees\": GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "}\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# --- H√†m t√≠nh ch·ªâ s·ªë ---\n",
    "def compute_metrics(df):\n",
    "    TP = df.filter((col(\"label\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "    FP = df.filter((col(\"label\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "    FN = df.filter((col(\"label\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "    TN = df.filter((col(\"label\") == 0) & (col(\"prediction\") == 0)).count()\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "\n",
    "    return TP, FP, FN, TN, precision, recall, f1, accuracy\n",
    "\n",
    "\n",
    "# --- Th∆∞ m·ª•c l∆∞u model ---\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "cv_results = {}\n",
    "best_models = {}\n",
    "\n",
    "# --- Train t·∫•t c·∫£ m√¥ h√¨nh ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüöÄ Training model: {name}\")\n",
    "\n",
    "    # --- L∆∞·ªõi tham s·ªë ---\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        grid = ParamGridBuilder() \\\n",
    "            .addGrid(model.regParam, [0.01, 0.1, 1.0]) \\\n",
    "            .addGrid(model.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "            .build()\n",
    "    elif isinstance(model, RandomForestClassifier):\n",
    "        grid = ParamGridBuilder() \\\n",
    "            .addGrid(model.numTrees, [30, 50, 100]) \\\n",
    "            .addGrid(model.maxDepth, [5, 10]) \\\n",
    "            .build()\n",
    "    elif isinstance(model, GBTClassifier):\n",
    "        grid = ParamGridBuilder() \\\n",
    "            .addGrid(model.maxDepth, [3, 5]) \\\n",
    "            .addGrid(model.maxIter, [20, 40]) \\\n",
    "            .build()\n",
    "    else:\n",
    "        grid = ParamGridBuilder().build()\n",
    "\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3,\n",
    "        parallelism=2\n",
    "    )\n",
    "\n",
    "    # --- Train v√† d·ª± ƒëo√°n ---\n",
    "    cv_model = cv.fit(train_data)\n",
    "    preds = cv_model.transform(test_data)\n",
    "\n",
    "    auc_per_param = list(cv_model.avgMetrics)\n",
    "    auc_best = evaluator.evaluate(preds)\n",
    "    TP, FP, FN, TN, precision, recall, f1, accuracy = compute_metrics(preds)\n",
    "\n",
    "    # --- L∆∞u k·∫øt qu·∫£ ---\n",
    "    cv_results[name] = {\n",
    "        \"model\": cv_model.bestModel,\n",
    "        \"preds\": preds,\n",
    "        \"auc_best\": auc_best,\n",
    "        \"auc_list\": auc_per_param,\n",
    "        \"metrics\": {\n",
    "            \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "            \"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models[name] = cv_model.bestModel\n",
    "\n",
    "    # --- In confusion matrix v√† metric ---\n",
    "    print(f\"\\nConfusion Matrix ({name}):\")\n",
    "    print(\"                 Pred=0     Pred=1\")\n",
    "    print(f\"Actual=0     |   {TN:6d}   |   {FP:6d}\")\n",
    "    print(f\"Actual=1     |   {FN:6d}   |   {TP:6d}\")\n",
    "    print(f\"\\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc_best:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ train v√† l∆∞u xong t·∫•t c·∫£ m√¥ h√¨nh. Bi·∫øn `best_models` v√† `cv_results` ƒë√£ s·∫µn s√†ng.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c789b8a",
   "metadata": {},
   "source": [
    "###  K·∫øt qu·∫£ t·ªïng h·ª£p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0842f682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Qu√° tr√¨nh tuning AUC t·ª´ng m√¥ h√¨nh ===\n",
      "\n",
      "\n",
      "üëâ Logistic Regression\n",
      "  - L·∫ßn 1: AUC = 0.8028\n",
      "  - L·∫ßn 2: AUC = 0.8013\n",
      "  - L·∫ßn 3: AUC = 0.8003\n",
      "  - L·∫ßn 4: AUC = 0.7986\n",
      "  - L·∫ßn 5: AUC = 0.7971\n",
      "  - L·∫ßn 6: AUC = 0.7987\n",
      "  - L·∫ßn 7: AUC = 0.7908\n",
      "  - L·∫ßn 8: AUC = 0.5000\n",
      "  - L·∫ßn 9: AUC = 0.5000\n",
      "‚úÖ Best AUC = 0.8048\n",
      "\n",
      "üëâ Random Forest\n",
      "  - L·∫ßn 1: AUC = 0.9368\n",
      "  - L·∫ßn 2: AUC = 0.9599\n",
      "  - L·∫ßn 3: AUC = 0.9379\n",
      "  - L·∫ßn 4: AUC = 0.9593\n",
      "  - L·∫ßn 5: AUC = 0.9384\n",
      "  - L·∫ßn 6: AUC = 0.9595\n",
      "‚úÖ Best AUC = 0.9610\n",
      "\n",
      "üëâ Gradient Boosted Trees\n",
      "  - L·∫ßn 1: AUC = 0.9503\n",
      "  - L·∫ßn 2: AUC = 0.9568\n",
      "  - L·∫ßn 3: AUC = 0.9605\n",
      "  - L·∫ßn 4: AUC = 0.9614\n",
      "‚úÖ Best AUC = 0.9628\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Qu√° tr√¨nh tuning AUC t·ª´ng m√¥ h√¨nh ===\\n\")\n",
    "\n",
    "for name, info in cv_results.items():\n",
    "    print(f\"\\nüëâ {name}\")\n",
    "    auc_list = info[\"auc_list\"]\n",
    "    for i, auc_val in enumerate(auc_list):\n",
    "        print(f\"  - L·∫ßn {i+1}: AUC = {auc_val:.4f}\")\n",
    "    print(f\"‚úÖ Best AUC = {info['auc_best']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97368585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix t·ª´ng m√¥ h√¨nh ===\n",
      "\n",
      "\n",
      "üìò Logistic Regression\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     5275   |     2010\n",
      "Actual=1     |     1883   |     5288\n",
      "\n",
      "Precision: 0.7246, Recall: 0.7374, F1: 0.7309, Accuracy: 0.7307\n",
      "\n",
      "üìò Random Forest\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     6874   |      411\n",
      "Actual=1     |      813   |     6358\n",
      "\n",
      "Precision: 0.9393, Recall: 0.8866, F1: 0.9122, Accuracy: 0.9153\n",
      "\n",
      "üìò Gradient Boosted Trees\n",
      "                 Pred=0     Pred=1\n",
      "Actual=0     |     6796   |      489\n",
      "Actual=1     |      748   |     6423\n",
      "\n",
      "Precision: 0.9293, Recall: 0.8957, F1: 0.9122, Accuracy: 0.9144\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Confusion Matrix t·ª´ng m√¥ h√¨nh ===\\n\")\n",
    "\n",
    "for name, info in cv_results.items():\n",
    "    m = info[\"metrics\"]\n",
    "    print(f\"\\nüìò {name}\")\n",
    "    print(\"                 Pred=0     Pred=1\")\n",
    "    print(f\"Actual=0     |   {m['TN']:6d}   |   {m['FP']:6d}\")\n",
    "    print(f\"Actual=1     |   {m['FN']:6d}   |   {m['TP']:6d}\")\n",
    "    print(f\"\\nPrecision: {m['precision']:.4f}, Recall: {m['recall']:.4f}, F1: {m['f1']:.4f}, Accuracy: {m['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b87b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== T·ªïng h·ª£p k·∫øt qu·∫£ t·∫•t c·∫£ m√¥ h√¨nh ===\n",
      "\n",
      "Model                     | AUC    | Precision  | Recall   | F1     | Acc   \n",
      "----------------------------------------------------------------------\n",
      "Logistic Regression       | 0.8048 | 0.7246     | 0.7374  | 0.7309 | 0.7307\n",
      "Random Forest             | 0.9610 | 0.9393     | 0.8866  | 0.9122 | 0.9153\n",
      "Gradient Boosted Trees    | 0.9628 | 0.9293     | 0.8957  | 0.9122 | 0.9144\n"
     ]
    }
   ],
   "source": [
    "print(\"=== T·ªïng h·ª£p k·∫øt qu·∫£ t·∫•t c·∫£ m√¥ h√¨nh ===\\n\")\n",
    "print(f\"{'Model':25s} | {'AUC':6s} | {'Precision':10s} | {'Recall':8s} | {'F1':6s} | {'Acc':6s}\")\n",
    "print(\"-\"*70)\n",
    "for name, info in cv_results.items():\n",
    "    m = info[\"metrics\"]\n",
    "    print(f\"{name:25s} | {info['auc_best']:.4f} | {m['precision']:.4f}     | {m['recall']:.4f}  | {m['f1']:.4f} | {m['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de3207",
   "metadata": {},
   "source": [
    "###  Ph√¢n t√≠ch t·∫ßm quan tr·ªçng c·ªßa bi·∫øn sau khi th√™m \"th√°ng\" v√† \"m√πa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06a7fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫°o feature_importance_df (ch·ª©a t·∫•t c·∫£ m√¥ h√¨nh).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "importance_rows = []\n",
    "\n",
    "for name, fitted in best_models.items():\n",
    "    if hasattr(fitted, \"featureImportances\"):  # RandomForest / GBT\n",
    "        importances = list(fitted.featureImportances)\n",
    "        for feat, imp in zip(feature_cols, importances):\n",
    "            importance_rows.append(Row(model=name, feature=feat, importance=float(imp)))\n",
    "\n",
    "    elif hasattr(fitted, \"coefficients\"):  # Logistic Regression\n",
    "        for feat, imp in zip(feature_cols, fitted.coefficients):\n",
    "            importance_rows.append(Row(model=name, feature=feat, importance=float(abs(imp))))\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {name}: Kh√¥ng h·ªó tr·ª£ tr√≠ch xu·∫•t feature importance.\")\n",
    "\n",
    "# --- T·∫°o DataFrame Spark ---\n",
    "feature_importance_df = spark.createDataFrame(importance_rows)\n",
    "print(\"‚úÖ ƒê√£ t·∫°o feature_importance_df (ch·ª©a t·∫•t c·∫£ m√¥ h√¨nh).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51997e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature Importance: Logistic Regression\n",
      "+-------------------+--------------+--------------------+\n",
      "|model              |feature       |importance          |\n",
      "+-------------------+--------------+--------------------+\n",
      "|Logistic Regression|cons_price_idx|0.431698656457129   |\n",
      "|Logistic Regression|emp_var_rate  |0.24618010540976207 |\n",
      "|Logistic Regression|euribor3m     |0.21827262608142817 |\n",
      "|Logistic Regression|cons_conf_idx |0.05621535092980314 |\n",
      "|Logistic Regression|nr_employed   |0.007634176901973183|\n",
      "|Logistic Regression|month         |0.0                 |\n",
      "|Logistic Regression|season_vec    |0.0                 |\n",
      "+-------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Feature Importance: Logistic Regression\")\n",
    "feature_importance_df.filter(col(\"model\") == \"Logistic Regression\") \\\n",
    "    .orderBy(col(\"importance\").desc()) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95b1162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature Importance: Random Forest\n",
      "+-------------+--------------+-------------------+\n",
      "|model        |feature       |importance         |\n",
      "+-------------+--------------+-------------------+\n",
      "|Random Forest|nr_employed   |0.2632992099402898 |\n",
      "|Random Forest|euribor3m     |0.24891744827229528|\n",
      "|Random Forest|cons_conf_idx |0.19121900709587356|\n",
      "|Random Forest|emp_var_rate  |0.18997014400303414|\n",
      "|Random Forest|cons_price_idx|0.1065941906885072 |\n",
      "|Random Forest|month         |0.0                |\n",
      "|Random Forest|season_vec    |0.0                |\n",
      "+-------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Feature Importance: Random Forest\")\n",
    "feature_importance_df.filter(col(\"model\") == \"Random Forest\") \\\n",
    "    .orderBy(col(\"importance\").desc()) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d1fb496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature Importance: Gradient Boosted Trees\n",
      "+----------------------+--------------+-------------------+\n",
      "|model                 |feature       |importance         |\n",
      "+----------------------+--------------+-------------------+\n",
      "|Gradient Boosted Trees|cons_conf_idx |0.2805864919125758 |\n",
      "|Gradient Boosted Trees|nr_employed   |0.26725407529419015|\n",
      "|Gradient Boosted Trees|emp_var_rate  |0.23827387194195473|\n",
      "|Gradient Boosted Trees|cons_price_idx|0.13602102954187395|\n",
      "|Gradient Boosted Trees|euribor3m     |0.07786453130940538|\n",
      "|Gradient Boosted Trees|month         |0.0                |\n",
      "|Gradient Boosted Trees|season_vec    |0.0                |\n",
      "+----------------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Feature Importance: Gradient Boosted Trees\")\n",
    "feature_importance_df.filter(col(\"model\") == \"Gradient Boosted Trees\") \\\n",
    "    .orderBy(col(\"importance\").desc()) \\\n",
    "    .show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
