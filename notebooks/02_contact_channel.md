```python
#init spark
!pip install -q pyspark findspark

import findspark
findspark.init()
import pyspark
findspark.find()
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("analytics_data").getOrCreate()

```


```python
path = "../data/bank-additional/bank-additional-full.csv"
raw_df = (
    spark.read
    .option("header", True)
    .option("inferschema", True)
    .option("sep", ";")
    .csv(path)
)

cols = ['contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous']
target_col = 'y'
df = raw_df.select(*(cols + [target_col]))

df.show(10, truncate=False)
```

    +---------+-----+-----------+--------+--------+-----+--------+---+
    |contact  |month|day_of_week|duration|campaign|pdays|previous|y  |
    +---------+-----+-----------+--------+--------+-----+--------+---+
    |telephone|may  |mon        |261     |1       |999  |0       |no |
    |telephone|may  |mon        |149     |1       |999  |0       |no |
    |telephone|may  |mon        |226     |1       |999  |0       |no |
    |telephone|may  |mon        |151     |1       |999  |0       |no |
    |telephone|may  |mon        |307     |1       |999  |0       |no |
    |telephone|may  |mon        |198     |1       |999  |0       |no |
    |telephone|may  |mon        |139     |1       |999  |0       |no |
    |telephone|may  |mon        |217     |1       |999  |0       |no |
    |telephone|may  |mon        |380     |1       |999  |0       |no |
    |telephone|may  |mon        |50      |1       |999  |0       |no |
    +---------+-----+-----------+--------+--------+-----+--------+---+
    only showing top 10 rows
    
    


```python
print("schema df: ")
df.printSchema()
print("__________________________________________")

print("\n So dong df: ")
print(df.count())
print("__________________________________________")

```

    schema df: 
    root
     |-- contact: string (nullable = true)
     |-- month: string (nullable = true)
     |-- day_of_week: string (nullable = true)
     |-- duration: integer (nullable = true)
     |-- campaign: integer (nullable = true)
     |-- pdays: integer (nullable = true)
     |-- previous: integer (nullable = true)
     |-- y: string (nullable = true)
    
    __________________________________________
    
     So dong df: 
    41188
    __________________________________________
    


```python
print("Checking missing values: ")
total = df.count()

from pyspark.sql import functions as F
for c in df.columns:
    miss = df.filter(F.col(c).isNull()).count()
    print(f"{c:20s} null = {miss}")
```

    Checking missing values: 
    contact              null = 0
    month                null = 0
    day_of_week          null = 0
    duration             null = 0
    campaign             null = 0
    pdays                null = 0
    previous             null = 0
    y                    null = 0
    

### * T·ª∑ l·ªá ph√¢n b·ªë gi·ªØa c√°c bi·∫øn


```python

cat_cols = ['contact', 'month', 'day_of_week', 'y']
for c in cat_cols:
    fre_val = df.groupBy(c).count().orderBy('count', ascending = False)
 
    total = df.count()
    col_expr = ((F.col('count') / total) * 100).cast("double")
    fre_val.withColumn("ratio (%)", F.round(col_expr, 2)).show()

```

    +---------+-----+---------+
    |  contact|count|ratio (%)|
    +---------+-----+---------+
    | cellular|26144|    63.47|
    |telephone|15044|    36.53|
    +---------+-----+---------+
    
    +-----+-----+---------+
    |month|count|ratio (%)|
    +-----+-----+---------+
    |  may|13769|    33.43|
    |  jul| 7174|    17.42|
    |  aug| 6178|     15.0|
    |  jun| 5318|    12.91|
    |  nov| 4101|     9.96|
    |  apr| 2632|     6.39|
    |  oct|  718|     1.74|
    |  sep|  570|     1.38|
    |  mar|  546|     1.33|
    |  dec|  182|     0.44|
    +-----+-----+---------+
    
    +-----------+-----+---------+
    |day_of_week|count|ratio (%)|
    +-----------+-----+---------+
    |        thu| 8623|    20.94|
    |        mon| 8514|    20.67|
    |        wed| 8134|    19.75|
    |        tue| 8090|    19.64|
    |        fri| 7827|     19.0|
    +-----------+-----+---------+
    
    +---+-----+---------+
    |  y|count|ratio (%)|
    +---+-----+---------+
    | no|36548|    88.73|
    |yes| 4640|    11.27|
    +---+-----+---------+
    
    

### Nh·∫≠n x√©t ph√¢n t√≠ch t·∫ßn su·∫•t c√°c bi·∫øn ph√¢n lo·∫°i

#### 1. `contact` ‚Äî K√™nh li√™n h·ªá
T·ª∑ l·ªá **cellular (63.5%)** cao h∆°n **telephone (36.5%)**.  
ƒêi·ªÅu n√†y ph·∫£n √°nh ƒë√∫ng th·ª±c t·∫ø giai ƒëo·∫°n 2008‚Äì2010, khi ng√¢n h√†ng chuy·ªÉn d·∫ßn sang g·ªçi qua **di ƒë·ªông** ƒë·ªÉ ti·∫øp c·∫≠n kh√°ch h√†ng nhanh h∆°n.  

Khi ph√¢n t√≠ch hi·ªáu qu·∫£ chi·∫øn d·ªãch, c·∫ßn ki·ªÉm tra xem **t·ª∑ l·ªá ‚Äúyes‚Äù** gi·ªØa hai k√™nh n√†y c√≥ kh√°c bi·ªát ƒë√°ng k·ªÉ kh√¥ng.  
N·∫øu di ƒë·ªông c√≥ t·ª∑ l·ªá th√†nh c√¥ng cao h∆°n, ƒë√≥ l√† t√≠n hi·ªáu cho vi·ªác **∆∞u ti√™n k√™nh li√™n h·ªá** n√†y trong c√°c chi·∫øn d·ªãch sau.

---

#### 2. `month` ‚Äî Th√°ng g·ªçi
C√°c th√°ng c√≥ nhi·ªÅu cu·ªôc g·ªçi nh·∫•t:
- **May (33%)**
- **July (17%)**
- **August (15%)**
- **June (13%)**

‚Üí Chi·∫øm h∆°n **75% t·ªïng s·ªë cu·ªôc g·ªçi**.  
C√≥ th·ªÉ ƒë√¢y l√† **chi·∫øn d·ªãch m√πa h√®**, khi ng√¢n h√†ng ƒë·∫©y m·∫°nh huy ƒë·ªông v·ªën ho·∫∑c tung s·∫£n ph·∫©m m·ªõi.

C√°c th√°ng kh√°c (Oct, Sep, Mar, Dec) c√≥ t·ª∑ l·ªá r·∫•t nh·ªè ‚Üí th∆∞·ªùng l√† **off-campaign** ho·∫∑c **chi·∫øn d·ªãch th·ª≠ nghi·ªám**.  
N√™n ki·ªÉm tra th√™m xem **t·ª∑ l·ªá ‚Äúyes‚Äù c√≥ bi·∫øn ƒë·ªông theo m√πa** kh√¥ng; v√≠ d·ª•: th√°ng May g·ªçi nhi·ªÅu nh∆∞ng hi·ªáu qu·∫£ c√≥ th·ªÉ kh√¥ng cao.

---

#### 3. `day_of_week` ‚Äî Ng√†y g·ªçi
Ph√¢n b·ªë **kh√° ƒë·ªìng ƒë·ªÅu (kho·∫£ng 19‚Äì21%)** m·ªói ng√†y.  
ƒêi·ªÅu n√†y cho th·∫•y ng√¢n h√†ng tri·ªÉn khai chi·∫øn d·ªãch ƒë·ªÅu trong tu·∫ßn, **kh√¥ng t·∫≠p trung ri√™ng v√†o ƒë·∫ßu ho·∫∑c cu·ªëi tu·∫ßn**.

ƒê√¢y l√† ƒë·∫∑c ƒëi·ªÉm t·ªët, gi√∫p **lo·∫°i b·ªè bias theo ng√†y** khi hu·∫•n luy·ªán m√¥ h√¨nh.  
Tuy nhi√™n, v·∫´n n√™n xem th·ª≠ **th·ª© S√°u** c√≥ t·ª∑ l·ªá ‚Äúyes‚Äù cao h∆°n kh√¥ng ‚Äî v√¨ kh√°ch h√†ng cu·ªëi tu·∫ßn c√≥ th·ªÉ t√¢m l√Ω tho·∫£i m√°i h∆°n.

---

#### 4. `y` ‚Äî Bi·∫øn m·ª•c ti√™u (k·∫øt qu·∫£)
D·ªØ li·ªáu **r·∫•t m·∫•t c√¢n b·∫±ng**:  
- **Yes:** 11.3%  
- **No:** 88.7%

ƒê√¢y l√† ƒë·∫∑c tr∆∞ng n·ªïi ti·∫øng c·ªßa **Bank Marketing Dataset**.  
H·ªá qu·∫£: n·∫øu hu·∫•n luy·ªán m√¥ h√¨nh m√† **kh√¥ng x·ª≠ l√Ω imbalance**, m√¥ h√¨nh s·∫Ω thi√™n v·ªÅ d·ª± ƒëo√°n ‚Äúno‚Äù.

‚Üí ·ªû b∆∞·ªõc m√¥ h√¨nh h√≥a, c·∫ßn **c√¢n b·∫±ng l·ªõp** b·∫±ng:
- `class_weighting`
- `SMOTE`
- ho·∫∑c `resampling`

ƒë·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c t√≠n hi·ªáu th·ª±c s·ª± c·ªßa nh√≥m ‚Äúyes‚Äù.



```python
#Kiem tra tan suat phan bo cua cac bien so
num_cols = ['duration', 'campaign', 'pdays', 'previous']

df.select(num_cols).describe().show()

percentiles = df.approxQuantile(num_cols, [0.25, 0.5, 0.75, 0.9, 0.95, 0.99], 0.01)
print(percentiles, "\n")


for i, c in enumerate(num_cols):
    res = dict(zip(["25%", "50%", "75%", "90%", "95%", "99%"], percentiles[i]))
    print(f"Percentile cua {c}: ")
    print(res, "\n")
```

    +-------+------------------+-----------------+-----------------+-------------------+
    |summary|          duration|         campaign|            pdays|           previous|
    +-------+------------------+-----------------+-----------------+-------------------+
    |  count|             41188|            41188|            41188|              41188|
    |   mean| 258.2850101971448|2.567592502670681|962.4754540157328|0.17296299893172767|
    | stddev|259.27924883646455|2.770013542902331|186.9109073447414|0.49490107983928927|
    |    min|                 0|                1|                0|                  0|
    |    max|              4918|               56|              999|                  7|
    +-------+------------------+-----------------+-----------------+-------------------+
    
    [[102.0, 177.0, 312.0, 526.0, 702.0, 4918.0], [1.0, 2.0, 3.0, 5.0, 6.0, 56.0], [999.0, 999.0, 999.0, 999.0, 999.0, 999.0], [0.0, 0.0, 0.0, 1.0, 1.0, 7.0]] 
    
    Percentile cua duration: 
    {'25%': 102.0, '50%': 177.0, '75%': 312.0, '90%': 526.0, '95%': 702.0, '99%': 4918.0} 
    
    Percentile cua campaign: 
    {'25%': 1.0, '50%': 2.0, '75%': 3.0, '90%': 5.0, '95%': 6.0, '99%': 56.0} 
    
    Percentile cua pdays: 
    {'25%': 999.0, '50%': 999.0, '75%': 999.0, '90%': 999.0, '95%': 999.0, '99%': 999.0} 
    
    Percentile cua previous: 
    {'25%': 0.0, '50%': 0.0, '75%': 0.0, '90%': 1.0, '95%': 1.0, '99%': 7.0} 
    
    

### Nh·∫≠n x√©t ph√¢n t√≠ch c√°c bi·∫øn s·ªë

#### 1. `duration` ‚Äî Th·ªùi l∆∞·ª£ng cu·ªôc g·ªçi
- Trung b√¨nh kho·∫£ng **258 gi√¢y**, ƒë·ªô l·ªách chu·∫©n g·∫ßn b·∫±ng trung b√¨nh ‚Üí **ph√¢n b·ªë r·∫•t l·ªách ph·∫£i (right-skewed)**.  
- **75%** cu·ªôc g·ªçi k√©o d√†i ‚â§ **312 gi√¢y**, nh∆∞ng **1% tr√™n c√πng** l√™n t·ªõi g·∫ßn **5000 gi√¢y (~82 ph√∫t)** ‚Üí c√≥ **outlier m·∫°nh**.  
- ƒêi·ªÅu n√†y h·ª£p l√Ω, v√¨ ƒëa s·ªë kh√°ch h√†ng t·ª´ ch·ªëi s·ªõm; c√≤n c√°c cu·ªôc g·ªçi d√†i th∆∞·ªùng ƒë·∫øn t·ª´ kh√°ch h√†ng c√≥ h·ª©ng th√∫ ho·∫∑c nh√¢n vi√™n thuy·∫øt ph·ª•c l√¢u.  
- Tuy nhi√™n, nh∆∞ ƒë√£ n√≥i ·ªü ph·∫ßn tr∆∞·ªõc, `duration` **kh√¥ng n√™n d√πng ƒë·ªÉ d·ª± ƒëo√°n tr·ª±c ti·∫øp**, v√¨ ƒë√¢y l√† **k·∫øt qu·∫£ sau khi g·ªçi**, kh√¥ng ph·∫£i th√¥ng tin bi·∫øt tr∆∞·ªõc.  
  ‚Üí D√πng trong **EDA (ph√¢n t√≠ch kh√°m ph√° d·ªØ li·ªáu)** ƒë·ªÉ hi·ªÉu h√†nh vi kh√°ch h√†ng l√† h·ª£p l√Ω.



#### 2. `campaign` ‚Äî S·ªë l·∫ßn li√™n h·ªá trong chi·∫øn d·ªãch hi·ªán t·∫°i
- **Trung v·ªã = 2**, **75% ‚â§ 3**, nh∆∞ng **1% cao nh·∫•t** l√™n t·ªõi **56 l·∫ßn** ‚Üí c√≥ nh·ªØng kh√°ch h√†ng b·ªã g·ªçi **h∆°n 50 l·∫ßn (!)**  
- ƒê√¢y l√† **outlier r√µ r√†ng**, nh∆∞ng l·∫°i ch·ª©a **th√¥ng tin h√†nh vi**: chi·∫øn d·ªãch ƒë√£ c·ªë g·∫Øng ‚Äúƒëu·ªïi theo‚Äù kh√°ch h√†ng ƒë√≥.  
- Khi m√¥ h√¨nh h√≥a, n√™n:
  - **Gi·ªõi h·∫°n (clip)** gi√° tr·ªã t·ªëi ƒëa ·ªü m·ªôt ng∆∞·ª°ng h·ª£p l√Ω (v√≠ d·ª• 10), ho·∫∑c  
  - **Log-transform** ƒë·ªÉ gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa c√°c gi√° tr·ªã c·ª±c l·ªõn.



#### 3. `pdays` ‚Äî S·ªë ng√†y t·ª´ l·∫ßn li√™n h·ªá tr∆∞·ªõc
- T·∫•t c·∫£ c√°c percentile ƒë·ªÅu = **999**, cho th·∫•y **ph·∫ßn l·ªõn kh√°ch h√†ng ch∆∞a t·ª´ng ƒë∆∞·ª£c li√™n h·ªá tr∆∞·ªõc ƒë√¢y**.  
- Theo m√¥ t·∫£ d·ªØ li·ªáu, **999 nghƒ©a l√† ‚Äúno previous contact‚Äù**.  
- Bi·∫øn n√†y √≠t th√¥ng tin tr·ª±c ti·∫øp, n√™n khi x·ª≠ l√Ω c√≥ th·ªÉ:
  - T·∫°o bi·∫øn nh·ªã ph√¢n m·ªõi: `has_contact_before = (pdays != 999)`, ho·∫∑c  
  - Gi·ªØ nguy√™n 999 v√† ƒë·ªÉ m√¥ h√¨nh t·ª± h·ªçc (t√πy thu·∫≠t to√°n).



#### 4. `previous` ‚Äî S·ªë l·∫ßn li√™n h·ªá tr∆∞·ªõc chi·∫øn d·ªãch hi·ªán t·∫°i
- **75% kh√°ch h√†ng ch∆∞a t·ª´ng ƒë∆∞·ª£c li√™n h·ªá (0)**, **95% ‚â§ 1**, ch·ªâ v√†i ng∆∞·ªùi ƒë·∫øn **7 l·∫ßn**.  
- Nghƒ©a l√† **ƒëa s·ªë l√† kh√°ch h√†ng m·ªõi**.  
- Bi·∫øn n√†y c√≥ **ph√¢n b·ªë r·∫•t l·ªách (one-sided)**, n√™n khi ƒë∆∞a v√†o m√¥ h√¨nh c·∫ßn:
  - **Chu·∫©n h√≥a (scaling)** ho·∫∑c  
  - **Binning (ph√¢n nh√≥m)** ƒë·ªÉ m√¥ h√¨nh d·ªÖ h·ªçc v√† ·ªïn ƒë·ªãnh h∆°n.
---

### 1. Insight v·ªÅ h√†nh vi li√™n h·ªá (k√™nh ‚Äì th·ªùi ƒëi·ªÉm ‚Äì k·∫øt qu·∫£):

#### 1.1 V·ªÅ k√™nh li√™n h·ªá 
‚Üí M·ª•c ti√™u: t√¨m hi·ªÉu k√™nh v√† th·ªùi ƒëi·ªÉm n√†o hi·ªáu qu·∫£ nh·∫•t.
**C√¢u h·ªèi khai th√°c:**

- K√™nh cellular c√≥ gi√∫p tƒÉng t·ª∑ l·ªá ph·∫£n h·ªìi t√≠ch c·ª±c kh√¥ng?

- Li·ªáu telephone c√≥ th·ªÉ b·ªã lo·∫°i b·ªè ho·∫∑c gi·∫£m t·∫ßn su·∫•t ƒë·ªÉ ti·∫øt ki·ªám chi ph√≠?


```python
#T·ª∑ l·ªá kh√°ch h√†ng "yes" c·ªßa t·ª´ng k√™nh
cellular_total = df.filter(df['contact'] == 'cellular').count()
yes_cellular = df.filter((df['contact'] == 'cellular') & (df['y'] == 'yes')).count()
cellular_ratio = round(yes_cellular / cellular_total * 100, 2)
print(f"Ty le kh√°ch hang say yes trong cellular la:{cellular_ratio}% tren tong so {cellular_total} \n")



telephone_total = df.filter(df['contact'] == 'telephone').count()
yes_telephone = df.filter((df['contact'] == 'telephone') & (df['y'] == 'yes')).count()
telephone_ratio = round(yes_telephone / telephone_total * 100, 2)
print(f"ty le  khach hang say yes khi su dung telephone la:{telephone_ratio}% tren tong so {telephone_total} ")


```

    Ty le kh√°ch hang say yes trong cellular la:14.74% tren tong so 26144 
    
    ty le  khach hang say yes khi su dung telephone la:5.23% tren tong so 15044 
    

Ph√¢n t√≠ch cho th·∫•y **k√™nh di ƒë·ªông (cellular)** c√≥ t·ª∑ l·ªá kh√°ch h√†ng ƒë·ªìng √Ω g·ª≠i ti·∫øt ki·ªám **cao g·∫•p g·∫ßn 3 l·∫ßn** so v·ªõi **ƒëi·ªán tho·∫°i b√†n (telephone)**.

- **Cellular:** 14.74% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **26,144 cu·ªôc g·ªçi**.  
- **Telephone:** 5.23% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **15,044 cu·ªôc g·ªçi**.

üëâ **K·∫øt lu·∫≠n:**  
K√™nh **di ƒë·ªông** cho th·∫•y **hi·ªáu qu·∫£ v∆∞·ª£t tr·ªôi**, c√≥ th·ªÉ do kh√°ch h√†ng **d·ªÖ ti·∫øp c·∫≠n h∆°n** v√† **ph·∫£n h·ªìi nhanh h∆°n**.  
**ƒê·ªÅ xu·∫•t:** Ng√¢n h√†ng n√™n **∆∞u ti√™n ng√¢n s√°ch v√† nh√¢n s·ª± cho k√™nh di ƒë·ªông**, ƒë·ªìng th·ªùi **gi·∫£m t·∫ßn su·∫•t g·ªçi qua ƒëi·ªán tho·∫°i b√†n** ƒë·ªÉ **t·ªëi ∆∞u chi ph√≠ v√† n√¢ng cao t·ª∑ l·ªá chuy·ªÉn ƒë·ªïi**.

---

#### 1.2 V·ªÅ th√°ng g·ªçi (month)

‚Üí **M·ª•c ti√™u:** ki·ªÉm tra xem y·∫øu t·ªë **m√πa v·ª•** c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn t·ª∑ l·ªá kh√°ch h√†ng ƒë·ªìng √Ω (‚Äúyes‚Äù) hay kh√¥ng.

**C√¢u h·ªèi khai th√°c:**

- C√≥ ph·∫£i **th√°ng May** tuy c√≥ nhi·ªÅu cu·ªôc g·ªçi nh∆∞ng **t·ª∑ l·ªá th√†nh c√¥ng l·∫°i th·∫•p**?

- **Th√°ng March** ho·∫∑c **September** c√≥ th·ªÉ l√† ‚Äú**th·ªùi ƒëi·ªÉm v√†ng**‚Äù ‚Äì √≠t cu·ªôc g·ªçi nh∆∞ng **hi·ªáu qu·∫£ cao h∆°n**?

- C√≥ **m·ªëi quan h·ªá n√†o gi·ªØa s·ªë l∆∞·ª£ng cu·ªôc g·ªçi v√† t·ª∑ l·ªá th√†nh c√¥ng** (g·ªçi nhi·ªÅu ch∆∞a ch·∫Øc t·ªët h∆°n)?



```python
month_df = (
    df.groupBy('month')
      .agg(
          F.count('*').alias('total'),
          F.sum((F.col('y') == 'yes').cast('int')).alias('yes_count')
      )
      .withColumn('yes_ratio (%)', F.round(F.col('yes_count')/F.col('total')* 100, 2))
      .orderBy('yes_ratio (%)')
)
month_df.show()
```

    +-----+-----+---------+-------------+
    |month|total|yes_count|yes_ratio (%)|
    +-----+-----+---------+-------------+
    |  may|13769|      886|         6.43|
    |  jul| 7174|      649|         9.05|
    |  nov| 4101|      416|        10.14|
    |  jun| 5318|      559|        10.51|
    |  aug| 6178|      655|         10.6|
    |  apr| 2632|      539|        20.48|
    |  oct|  718|      315|        43.87|
    |  sep|  570|      256|        44.91|
    |  dec|  182|       89|         48.9|
    |  mar|  546|      276|        50.55|
    +-----+-----+---------+-------------+
    
    

Ph√¢n t√≠ch cho th·∫•y c√°c chi·∫øn d·ªãch ƒë∆∞·ª£c tri·ªÉn khai m·∫°nh nh·∫•t v√†o **th√°ng May, July v√† August**, nh∆∞ng **t·ª∑ l·ªá th√†nh c√¥ng l·∫°i kh√° th·∫•p**.  
Ng∆∞·ª£c l·∫°i, nh·ªØng th√°ng c√≥ **√≠t cu·ªôc g·ªçi** nh∆∞ **March, September, October v√† December** l·∫°i c√≥ t·ª∑ l·ªá ‚Äúyes‚Äù **r·∫•t cao** ‚Äî th·∫≠m ch√≠ **g·∫•p 5‚Äì8 l·∫ßn so v·ªõi th√°ng May**.

- **Th√°ng May:** 6.43% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **13,769 cu·ªôc g·ªçi**.  
- **Th√°ng August:** 10.6% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **6,178 cu·ªôc g·ªçi**.  
- **Th√°ng April:** 20.48% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **2,632 cu·ªôc g·ªçi**.  
- **Th√°ng March:** 50.55% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **546 cu·ªôc g·ªçi**.

üëâ **K·∫øt lu·∫≠n:**  
Hi·ªáu qu·∫£ chi·∫øn d·ªãch c√≥ **t√≠nh m√πa v·ª• r√µ r·ªát**.  
Nh·ªØng th√°ng ng√¢n h√†ng **g·ªçi nhi·ªÅu (ƒë·∫∑c bi·ªát l√† May, July)** l·∫°i c√≥ **t·ª∑ l·ªá ph·∫£n h·ªìi th·∫•p**,  
trong khi c√°c th√°ng **√≠t g·ªçi (March, September, October, December)** mang l·∫°i **t·ª∑ l·ªá th√†nh c√¥ng v∆∞·ª£t tr·ªôi**.

**ƒê·ªÅ xu·∫•t:**  
Ng√¢n h√†ng n√™n **t√°i ph√¢n b·ªï l·ªãch g·ªçi**, **tƒÉng c∆∞·ªùng chi·∫øn d·ªãch** v√†o c√°c th√°ng c√≥ hi·ªáu qu·∫£ cao,  
ƒë·ªìng th·ªùi **gi·∫£m t·∫ßn su·∫•t** ·ªü c√°c th√°ng th·∫•p hi·ªáu qu·∫£ nh∆∞ **May‚ÄìJuly** ƒë·ªÉ **n√¢ng cao hi·ªáu su·∫•t t·ªïng th·ªÉ**.

---

#### 1.3 V·ªÅ ng√†y trong tu·∫ßn (day_of_week)

‚Üí **M·ª•c ti√™u:** x√°c ƒë·ªãnh **ng√†y n√†o trong tu·∫ßn** mang l·∫°i **t·ª∑ l·ªá kh√°ch h√†ng ƒë·ªìng √Ω cao nh·∫•t**,  
t·ª´ ƒë√≥ h·ªó tr·ª£ ng√¢n h√†ng **l√™n l·ªãch g·ªçi t·ªëi ∆∞u** cho ƒë·ªôi ng≈© t∆∞ v·∫•n.

**C√¢u h·ªèi khai th√°c:**

- Li·ªáu kh√°ch h√†ng c√≥ xu h∆∞·ªõng **ƒë·ªìng √Ω nhi·ªÅu h∆°n v√†o cu·ªëi tu·∫ßn** (th·ª© NƒÉm, th·ª© S√°u) khi **t√¢m l√Ω tho·∫£i m√°i h∆°n**?

- C√≥ **s·ª± kh√°c bi·ªát r√µ** gi·ªØa **ƒë·∫ßu tu·∫ßn** v√† **cu·ªëi tu·∫ßn** hay kh√¥ng?

- Ng√¢n h√†ng c√≥ th·ªÉ **∆∞u ti√™n g·ªçi v√†o c√°c ng√†y ‚Äúhi·ªáu qu·∫£‚Äù h∆°n** ƒë·ªÉ **tƒÉng t·ª∑ l·ªá chuy·ªÉn ƒë·ªïi**?



```python
dayOfWeek_df = (
    df.groupBy('day_of_week')
      .agg(
         F.count('*').alias('total'),
         F.sum((F.col('y') == 'yes').cast('int')).alias('yes_count')
      )
      .withColumn('yes_ratio(%)', F.round((F.col('yes_count')/F.col('total') * 100), 2))
      .orderBy('yes_ratio(%)',ascending = False)
)
dayOfWeek_df.show()
```

    +-----------+-----+---------+------------+
    |day_of_week|total|yes_count|yes_ratio(%)|
    +-----------+-----+---------+------------+
    |        thu| 8623|     1045|       12.12|
    |        tue| 8090|      953|       11.78|
    |        wed| 8134|      949|       11.67|
    |        fri| 7827|      846|       10.81|
    |        mon| 8514|      847|        9.95|
    +-----------+-----+---------+------------+
    
    

Ph√¢n t√≠ch cho th·∫•y **t·ª∑ l·ªá kh√°ch h√†ng ‚Äúsay yes‚Äù cao nh·∫•t** r∆°i v√†o **th·ª© NƒÉm (12.12%)**,  
ti·∫øp theo l√† **th·ª© Ba (11.78%)** v√† **th·ª© T∆∞ (11.67%)**.  
Trong khi ƒë√≥, **th·ª© Hai c√≥ t·ª∑ l·ªá th·∫•p nh·∫•t (9.95%)** ‚Äî t·ª©c l√† **ƒë·∫ßu tu·∫ßn kh√°ch h√†ng √≠t ph·∫£n h·ªìi t√≠ch c·ª±c h∆°n**.

- **Th·ª© NƒÉm:** 12.12% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **8,623 cu·ªôc g·ªçi**.  
- **Th·ª© Ba:** 11.78% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **8,090 cu·ªôc g·ªçi**.  
- **Th·ª© Hai:** 9.95% kh√°ch h√†ng ƒë·ªìng √Ω tr√™n t·ªïng **8,514 cu·ªôc g·ªçi**.

üëâ **K·∫øt lu·∫≠n:**  
T·ª∑ l·ªá ph·∫£n h·ªìi t√≠ch c·ª±c c√≥ **xu h∆∞·ªõng tƒÉng d·∫ßn v·ªÅ gi·ªØa v√† cu·ªëi tu·∫ßn**, ƒë·∫°t **ƒë·ªânh v√†o th·ª© NƒÉm**.  
ƒêi·ªÅu n√†y g·ª£i √Ω r·∫±ng **th·ªùi ƒëi·ªÉm gi·ªØa ‚Äì cu·ªëi tu·∫ßn** l√† **‚Äúkhung gi·ªù v√†ng‚Äù ƒë·ªÉ tri·ªÉn khai cu·ªôc g·ªçi**,  
khi kh√°ch h√†ng c√≥ **t√¢m l√Ω tho·∫£i m√°i** v√† **s·∫µn s√†ng t∆∞∆°ng t√°c h∆°n**.

---

### 2: ‚ÄúT·∫ßn su·∫•t & l·ªãch s·ª≠ li√™n h·ªá.‚Äù

#### 2.1. T·∫ßn su·∫•t g·ªçi trong chi·∫øn d·ªãch hi·ªán t·∫°i ‚Äì campaign
‚Üí **M·ª•c ti√™u:** xem vi·ªác **g·ªçi nhi·ªÅu h∆°n trong c√πng m·ªôt chi·∫øn d·ªãch** c√≥ l√†m **tƒÉng kh·∫£ nƒÉng kh√°ch h√†ng ‚Äúyes‚Äù** hay kh√¥ng.

**C·∫ßn ph√¢n t√≠ch:**

- So s√°nh **trung b√¨nh s·ªë l·∫ßn g·ªçi (campaign)** gi·ªØa nh√≥m **‚Äúyes‚Äù** v√† **‚Äúno‚Äù**.  
- T√≠nh **t·ª∑ l·ªá ‚Äúyes‚Äù theo nh√≥m t·∫ßn su·∫•t g·ªçi**, v√≠ d·ª•: **1‚Äì2**, **3‚Äì5**, **>5 l·∫ßn**.

**C√¢u h·ªèi khai th√°c:**

- Li·ªáu **g·ªçi qu√° nhi·ªÅu c√≥ ph·∫£n t√°c d·ª•ng** kh√¥ng?  
- C√≥ t·ªìn t·∫°i **ng∆∞·ª°ng ‚Äús·ªë l·∫ßn g·ªçi t·ªëi ∆∞u‚Äù** n√†o cho chi·∫øn d·ªãch?


```python
# Trung b√¨nh s·ªë l·∫ßn g·ªçi gi·ªØa hai nh√≥m yes / no
avg_calls = df.groupBy("y").agg(F.round(F.avg("campaign"), 2).alias("avg_calls"))
avg_calls.show()

# Ph√¢n nh√≥m t·∫ßn su·∫•t g·ªçi: √≠t - trung b√¨nh - nhi·ªÅu
campaign_gr = (
    df.withColumn(
        'campaign_group',
        F.when(F.col('campaign') <= 2, "low")
        .when(F.col('campaign') <= 5, "medium")
        .otherwise("high")
    )
    .groupBy('campaign_group')
    .agg(
        F.count("*").alias("total"),
        F.sum((F.col("y") == "yes").cast("int")).alias("yes_count")
    )
    .withColumn("yes_ratio (%)", F.round(F.col("yes_count") / F.col("total") * 100, 2))
    .orderBy("campaign_group")
)

campaign_gr.show()
```

    +---+---------+
    |  y|avg_calls|
    +---+---------+
    | no|     2.63|
    |yes|     2.05|
    +---+---------+
    
    +--------------+-----+---------+-------------+
    |campaign_group|total|yes_count|yes_ratio (%)|
    +--------------+-----+---------+-------------+
    |          high| 3385|      186|         5.49|
    |           low|28212|     3511|        12.45|
    |        medium| 9591|      943|         9.83|
    +--------------+-----+---------+-------------+
    
    

Ph√¢n t√≠ch cho th·∫•y nh·ªØng kh√°ch h√†ng **ƒë·ªìng √Ω g·ª≠i ti·∫øt ki·ªám (‚Äúyes‚Äù)** ch·ªâ ƒë∆∞·ª£c g·ªçi trung b√¨nh **2.05 l·∫ßn**,  
trong khi nh√≥m **t·ª´ ch·ªëi (‚Äúno‚Äù)** ƒë∆∞·ª£c g·ªçi trung b√¨nh **2.63 l·∫ßn**.  
ƒêi·ªÅu n√†y g·ª£i √Ω r·∫±ng **g·ªçi qu√° nhi·ªÅu l·∫ßn kh√¥ng gi√∫p tƒÉng hi·ªáu qu·∫£**, th·∫≠m ch√≠ **c√≥ xu h∆∞·ªõng ph·∫£n t√°c d·ª•ng**.

**Ph√¢n nh√≥m t·∫ßn su·∫•t g·ªçi:**

- **Nh√≥m √≠t (‚â§ 2 l·∫ßn):** t·ª∑ l·ªá th√†nh c√¥ng **12.45%** tr√™n t·ªïng **28,212 cu·ªôc g·ªçi**.  
- **Nh√≥m trung b√¨nh (3‚Äì5 l·∫ßn):** t·ª∑ l·ªá th√†nh c√¥ng **9.83%** tr√™n t·ªïng **9,591 cu·ªôc g·ªçi**.  
- **Nh√≥m cao (>5 l·∫ßn):** t·ª∑ l·ªá th√†nh c√¥ng ch·ªâ **5.49%** tr√™n t·ªïng **3,385 cu·ªôc g·ªçi**.

üëâ **K·∫øt lu·∫≠n:**  
T·∫ßn su·∫•t g·ªçi cao **kh√¥ng l√†m tƒÉng t·ª∑ l·ªá ƒë·ªìng √Ω**; ng∆∞·ª£c l·∫°i, **c√†ng g·ªçi nhi·ªÅu kh√°ch h√†ng c√†ng √≠t ph·∫£n h·ªìi t√≠ch c·ª±c**.  
ƒêi·ªÅu n√†y cho th·∫•y **nhi·ªÅu kh√°ch h√†ng c√≥ th·ªÉ c·∫£m th·∫•y phi·ªÅn** khi b·ªã li√™n h·ªá l·∫∑p l·∫°i trong c√πng chi·∫øn d·ªãch.

**ƒê·ªÅ xu·∫•t:**  
Ng√¢n h√†ng n√™n **gi·ªõi h·∫°n s·ªë l·∫ßn g·ªçi t·ªëi ƒëa ·ªü m·ª©c 2‚Äì3 l·∫ßn m·ªói chi·∫øn d·ªãch**,  
v√† **chuy·ªÉn tr·ªçng t√¢m sang ch·∫•t l∆∞·ª£ng cu·ªôc g·ªçi thay v√¨ t·∫ßn su·∫•t**,  
nh·∫±m **t·ªëi ∆∞u hi·ªáu qu·∫£ v√† gi·∫£m chi ph√≠ nh√¢n s·ª±**.

---

#### 2.2. L·ªãch s·ª≠ li√™n h·ªá ‚Äì `pdays` v√† `has_contact_before`

‚Üí **M·ª•c ti√™u:** t√¨m hi·ªÉu **t√°c ƒë·ªông c·ªßa vi·ªác t√°i li√™n h·ªá** v·ªõi kh√°ch h√†ng **ƒë√£ t·ª´ng ƒë∆∞·ª£c g·ªçi tr∆∞·ªõc ƒë√¢y**.

**C·∫ßn ph√¢n t√≠ch:**

- So s√°nh **t·ª∑ l·ªá ‚Äúyes‚Äù** gi·ªØa hai nh√≥m:  
  - **ƒê√£ t·ª´ng ƒë∆∞·ª£c g·ªçi tr∆∞·ªõc** (`pdays ‚â† 999`)  
  - **Ch∆∞a t·ª´ng ƒë∆∞·ª£c g·ªçi** (`pdays = 999`)  
- Ki·ªÉm tra xem **kho·∫£ng c√°ch gi·ªØa hai l·∫ßn li√™n h·ªá** (`pdays` nh·ªè ‚Üí g·ªçi g·∫ßn ƒë√¢y) c√≥ **·∫£nh h∆∞·ªüng ƒë·∫øn t·ª∑ l·ªá ‚Äúyes‚Äù** hay kh√¥ng.

**C√¢u h·ªèi khai th√°c:**

- Vi·ªác **g·ªçi l·∫°i** c√≥ gi√∫p **tƒÉng c∆° h·ªôi th√†nh c√¥ng** kh√¥ng?  
- N·∫øu c√≥, **sau bao l√¢u g·ªçi l·∫°i** l√† h·ª£p l√Ω?



```python
# T·∫°o bi·∫øn ƒë√°nh d·∫•u ƒë√£ t·ª´ng li√™n h·ªá tr∆∞·ªõc ƒë√≥
df_pdays = df.withColumn("has_contact_before", (F.col("pdays") != 999).cast("int"))
total_count = df_pdays.count()

# ƒê·∫øm s·ªë l∆∞·ª£ng v√† t·ª∑ l·ªá "yes" theo nh√≥m li√™n h·ªá
pdays_stats = (
    df_pdays.groupBy("has_contact_before")
    .agg(
        F.count("*").alias("total"),
        F.sum((F.col("y") == "yes").cast("int")).alias("yes_count")
    )
    .withColumn("yes_ratio (%)", F.round(F.col("yes_count") / F.col("total") * 100, 2))
    .withColumn("group_ratio (%)", F.round(F.col("total") / total_count * 100, 2))
    .orderBy("has_contact_before")
)
pdays_stats.show()

# Ch·ªâ gi·ªØ nh·ªØng kh√°ch h√†ng ƒë√£ t·ª´ng ƒë∆∞·ª£c li√™n h·ªá (pdays != 999)
df_recontacted = df.filter(F.col("pdays") != 999)

# Ph√¢n nh√≥m kho·∫£ng c√°ch g·ªçi l·∫°i
pdays_grouped = (
    df_recontacted.withColumn(
        "pdays_group",
        F.when(F.col("pdays") <= 5, "‚â§ 5 ng√†y")
         .otherwise("> 5 ng√†y")
    )
    .groupBy("pdays_group")
    .agg(
        F.count("*").alias("total"),
        F.sum((F.col("y") == "yes").cast("int")).alias("yes_count")
    )
    .withColumn("yes_ratio (%)", F.round(F.col("yes_count") / F.col("total") * 100, 2))
    .orderBy("pdays_group")
)
pdays_grouped.show()

```

    +------------------+-----+---------+-------------+---------------+
    |has_contact_before|total|yes_count|yes_ratio (%)|group_ratio (%)|
    +------------------+-----+---------+-------------+---------------+
    |                 0|39673|     3673|         9.26|          96.32|
    |                 1| 1515|      967|        63.83|           3.68|
    +------------------+-----+---------+-------------+---------------+
    
    +-----------+-----+---------+-------------+
    |pdays_group|total|yes_count|yes_ratio (%)|
    +-----------+-----+---------+-------------+
    |   > 5 ng√†y|  810|      522|        64.44|
    |   ‚â§ 5 ng√†y|  705|      445|        63.12|
    +-----------+-----+---------+-------------+
    
    

Ph√¢n t√≠ch cho th·∫•y 
- **Kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c li√™n h·ªá tr∆∞·ªõc ƒë√≥** c√≥ **t·ª∑ l·ªá ƒë·ªìng √Ω r·∫•t cao (~63.8%)**,  
cao g·∫•p nhi·ªÅu l·∫ßn so v·ªõi nh√≥m **ch∆∞a t·ª´ng ƒë∆∞·ª£c g·ªçi (9.3%)**.  

- Gi·ªØa nh√≥m **ƒë∆∞·ª£c g·ªçi l·∫°i trong v√≤ng 5 ng√†y** v√† **sau 5 ng√†y**, t·ª∑ l·ªá ‚Äúyes‚Äù g·∫ßn nh∆∞ **t∆∞∆°ng ƒë∆∞∆°ng (63‚Äì64%)**.

- Ch·ªâ **3.68%** kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c g·ªçi tr∆∞·ªõc ƒë√≥, nghƒ©a l√† **h∆°n 96%** l√† l·∫ßn ƒë·∫ßu ti√™n ƒë∆∞·ª£c li√™n h·ªá.  ƒêi·ªÅu n√†y x√°c nh·∫≠n chi·∫øn d·ªãch marketing c·ªßa ng√¢n h√†ng ch·ªß y·∫øu l√† **cold call**, kh√¥ng ph·∫£i **chƒÉm s√≥c kh√°ch h√†ng c≈©**.  

üëâ **K·∫øt lu·∫≠n:**  
**T√°i li√™n h·ªá kh√°ch h√†ng c≈©** l√† m·ªôt **chi·∫øn l∆∞·ª£c hi·ªáu qu·∫£ r√µ r·ªát**,  
trong khi **kho·∫£ng c√°ch gi·ªØa hai l·∫ßn g·ªçi kh√¥ng ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ**.

**ƒê·ªÅ xu·∫•t:**  
Ng√¢n h√†ng n√™n **∆∞u ti√™n x√¢y d·ª±ng chi·∫øn d·ªãch follow-up c√≥ ch·ªçn l·ªçc**  
thay v√¨ **t·∫≠p trung qu√° nhi·ªÅu v√†o g·ªçi m·ªõi (cold call)**.

---

#### 2.3. S·ªë l·∫ßn li√™n h·ªá trong c√°c chi·∫øn d·ªãch tr∆∞·ªõc ‚Äì `previous`

‚Üí **M·ª•c ti√™u:** ƒë√°nh gi√° xem **l·ªãch s·ª≠ ƒë∆∞·ª£c g·ªçi nhi·ªÅu l·∫ßn tr∆∞·ªõc ƒë√¢y** c√≥ gi√∫p **tƒÉng x√°c su·∫•t kh√°ch h√†ng ‚Äúyes‚Äù** trong chi·∫øn d·ªãch hi·ªán t·∫°i hay kh√¥ng.

**C·∫ßn ph√¢n t√≠ch:**

- T√≠nh **t·ª∑ l·ªá ‚Äúyes‚Äù theo gi√° tr·ªã `previous`** (0, 1, 2, ‚Ä¶).  
- Ki·ªÉm tra xem **kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c li√™n h·ªá 1‚Äì2 l·∫ßn tr∆∞·ªõc ƒë√≥** c√≥ **d·ªÖ ƒë·ªìng √Ω h∆°n** so v·ªõi nh√≥m ch∆∞a t·ª´ng li√™n h·ªá kh√¥ng.

**C√¢u h·ªèi khai th√°c:**

- ‚Äú**Kh√°ch h√†ng quen thu·ªôc**‚Äù c√≥ th·ª±c s·ª± **ti·ªÅm nƒÉng h∆°n** kh√¥ng?  
- C√≥ n√™n **t·∫≠p trung v√†o nh√≥m ƒë√£ li√™n h·ªá nhi·ªÅu l·∫ßn trong qu√° kh·ª©** ƒë·ªÉ tƒÉng hi·ªáu qu·∫£ chi·∫øn d·ªãch?



```python
from pyspark.sql import functions as F

# T√≠nh t·ª∑ l·ªá yes theo s·ªë l·∫ßn ƒë∆∞·ª£c li√™n h·ªá trong c√°c chi·∫øn d·ªãch tr∆∞·ªõc
previous_stats = (
    df.groupBy("previous")
      .agg(
          F.count("*").alias("total"),
          F.sum((F.col("y") == "yes").cast("int")).alias("yes_count")
      )
      .withColumn("yes_ratio (%)", F.round(F.col("yes_count") / F.col("total") * 100, 2))
      .orderBy("previous")
)

previous_stats.show()

```

    +--------+-----+---------+-------------+
    |previous|total|yes_count|yes_ratio (%)|
    +--------+-----+---------+-------------+
    |       0|35563|     3141|         8.83|
    |       1| 4561|      967|         21.2|
    |       2|  754|      350|        46.42|
    |       3|  216|      128|        59.26|
    |       4|   70|       38|        54.29|
    |       5|   18|       13|        72.22|
    |       6|    5|        3|         60.0|
    |       7|    1|        0|          0.0|
    +--------+-----+---------+-------------+
    
    

Ph√¢n t√≠ch cho th·∫•y **t·ª∑ l·ªá ƒë·ªìng √Ω tƒÉng r√µ r·ªát** theo **s·ªë l·∫ßn kh√°ch h√†ng t·ª´ng ƒë∆∞·ª£c li√™n h·ªá trong qu√° kh·ª©**:

- **previous = 0:** 8.83% kh√°ch h√†ng ƒë·ªìng √Ω.  
- **previous = 1‚Äì2:** 21‚Äì46%, tƒÉng g·∫•p **2‚Äì5 l·∫ßn**.  
- **previous ‚â• 3:** ~55‚Äì72%, d√π s·ªë m·∫´u √≠t h∆°n.

üëâ **K·∫øt lu·∫≠n:**  
**Kh√°ch h√†ng ƒë√£ t·ª´ng ƒë∆∞·ª£c li√™n h·ªá tr∆∞·ªõc ƒë√¢y** l√† nh√≥m **c√≥ ti·ªÅm nƒÉng cao**,  
v·ªõi **x√°c su·∫•t ‚Äúyes‚Äù cao h∆°n nhi·ªÅu** so v·ªõi kh√°ch h√†ng m·ªõi.

**ƒê·ªÅ xu·∫•t:**  
T·∫≠p trung **chƒÉm s√≥c v√† t√°i li√™n h·ªá nh√≥m kh√°ch h√†ng quen thu·ªôc**,  
thay v√¨ **d√†n tr·∫£i ngu·ªìn l·ª±c cho nh√≥m ch∆∞a t·ª´ng t∆∞∆°ng t√°c**.

----

### 3 ‚Äì Th·ªùi l∆∞·ª£ng v√† h√†nh vi cu·ªôc g·ªçi (duration)

#### 3.1. Th·ªùi l∆∞·ª£ng v√† h√†nh vi cu·ªôc g·ªçi ‚Äì `duration`

‚Üí **M·ª•c ti√™u:** ph√¢n t√≠ch xem **th·ªùi l∆∞·ª£ng cu·ªôc g·ªçi** c√≥ li√™n quan ƒë·∫øn **kh·∫£ nƒÉng kh√°ch h√†ng ƒë·ªìng √Ω (‚Äúyes‚Äù)** hay kh√¥ng,  
t·ª´ ƒë√≥ hi·ªÉu r√µ **m·ª©c ƒë·ªô quan t√¢m v√† t∆∞∆°ng t√°c** c·ªßa kh√°ch h√†ng trong qu√° tr√¨nh t∆∞ v·∫•n.

**C·∫ßn ph√¢n t√≠ch:**

- So s√°nh **th·ªùi l∆∞·ª£ng trung b√¨nh (duration)** gi·ªØa hai nh√≥m **‚Äúyes‚Äù** v√† **‚Äúno‚Äù**.  
- T√≠nh **t·ª∑ l·ªá ‚Äúyes‚Äù theo nh√≥m ƒë·ªô d√†i cu·ªôc g·ªçi**: *ng·∫Øn*, *trung b√¨nh*, *d√†i*.

**C√¢u h·ªèi khai th√°c:**

- Li·ªáu **cu·ªôc g·ªçi k√©o d√†i h∆°n** c√≥ th·ª±c s·ª± **d·∫´n ƒë·∫øn kh·∫£ nƒÉng ƒë·ªìng √Ω cao h∆°n**?  
- C√≥ th·ªÉ **x√°c ƒë·ªãnh ng∆∞·ª°ng th·ªùi l∆∞·ª£ng t·ªëi thi·ªÉu** ƒë·ªÉ nh·∫≠n bi·∫øt **cu·ªôc g·ªçi ti·ªÅm nƒÉng th√†nh c√¥ng** kh√¥ng?



```python
# Th·ªùi l∆∞·ª£ng trung b√¨nh gi·ªØa hai nh√≥m "yes" v√† "no"
duration_avg = (
    df.groupBy("y")
      .agg(F.round(F.avg("duration"), 2).alias("avg_duration_sec"))
      .orderBy("y")
)
duration_avg.show()

# 2) T·ª∑ l·ªá "yes" theo nh√≥m ƒë·ªô d√†i cu·ªôc g·ªçi: ng·∫Øn (‚â§120s), trung b√¨nh (121‚Äì300s), d√†i (>300s)
duration_bins = (
    df.withColumn(
        "duration_group",
        F.when(F.col("duration") <= 120, "ng·∫Øn (‚â§120s)")
         .when(F.col("duration") <= 300, "trung b√¨nh (121‚Äì300s)")
         .otherwise("d√†i (>300s)")
    )
    .groupBy("duration_group")
    .agg(
        F.count("*").alias("total"),
        F.sum((F.col("y") == "yes").cast("int")).alias("yes_count")
    )
    .withColumn("yes_ratio (%)", F.round(F.col("yes_count")/F.col("total")*100, 2))
    .orderBy("duration_group")
)

duration_bins.show()





```

    +---+----------------+
    |  y|avg_duration_sec|
    +---+----------------+
    | no|          220.84|
    |yes|          553.19|
    +---+----------------+
    
    +--------------------+-----+---------+-------------+
    |      duration_group|total|yes_count|yes_ratio (%)|
    +--------------------+-----+---------+-------------+
    |         d√†i (>300s)|11204|     3122|        27.87|
    |        ng·∫Øn (‚â§120s)|12917|      166|         1.29|
    |trung b√¨nh (121‚Äì3...|17067|     1352|         7.92|
    +--------------------+-----+---------+-------------+
    
    

**Ph√¢n t√≠ch cho th·∫•y:**  
Th·ªùi l∆∞·ª£ng cu·ªôc g·ªçi trung b√¨nh c·ªßa nh√≥m **‚Äúyes‚Äù** cao g·∫•p h∆°n **2 l·∫ßn** so v·ªõi nh√≥m **‚Äúno‚Äù**:

- **Nh√≥m ‚Äúno‚Äù:** trung b√¨nh **220.84 gi√¢y**.  
- **Nh√≥m ‚Äúyes‚Äù:** trung b√¨nh **553.19 gi√¢y**.

**Ph√¢n nh√≥m ƒë·ªô d√†i cu·ªôc g·ªçi:**

- **Ng·∫Øn (‚â§120s):** t·ª∑ l·ªá ƒë·ªìng √Ω ch·ªâ **1.29%** tr√™n t·ªïng **12,917 cu·ªôc g·ªçi**.  
- **Trung b√¨nh (121‚Äì300s):** t·ª∑ l·ªá tƒÉng l√™n **7.92%** tr√™n **17,067 cu·ªôc g·ªçi**.  
- **D√†i (>300s):** t·ª∑ l·ªá ‚Äúyes‚Äù v·ªçt l√™n **27.87%** tr√™n **11,204 cu·ªôc g·ªçi**.

üëâ **K·∫øt lu·∫≠n:**  
C√°c cu·ªôc g·ªçi k√©o d√†i **h∆°n 5 ph√∫t** c√≥ kh·∫£ nƒÉng th√†nh c√¥ng **cao g·∫•p 3‚Äì5 l·∫ßn** so v·ªõi c√°c cu·ªôc g·ªçi ng·∫Øn.  
ƒêi·ªÅu n√†y cho th·∫•y **m·ª©c ƒë·ªô t∆∞∆°ng t√°c v√† th·ªùi gian t∆∞ v·∫•n** l√† **y·∫øu t·ªë quy·∫øt ƒë·ªãnh quan tr·ªçng** cho vi·ªác thuy·∫øt ph·ª•c kh√°ch h√†ng.

**ƒê·ªÅ xu·∫•t:**  
Ng√¢n h√†ng n√™n **t·∫≠p trung v√†o ch·∫•t l∆∞·ª£ng cu·ªôc g·ªçi h∆°n l√† s·ªë l∆∞·ª£ng**,  
khuy·∫øn kh√≠ch nh√¢n vi√™n **duy tr√¨ cu·ªôc tr√≤ chuy·ªán l√¢u h∆°n** v·ªõi **nh·ªØng kh√°ch h√†ng ti·ªÅm nƒÉng**,  
thay v√¨ **k·∫øt th√∫c s·ªõm**.

---

### 4. Ti·ªÅn x·ª≠ l√Ω

#### 4.1.a ‚Äì Chu·∫©n h√≥a bi·∫øn m·ª•c ti√™u v√† t·∫°o bi·∫øn ph·ª•

**M·ª•c ti√™u:**

- Chuy·ªÉn `y` th√†nh d·∫°ng nh·ªã ph√¢n (`0/1`).
- T·∫°o bi·∫øn `has_contact_before = 1` n·∫øu `pdays != 999`, `0` n·∫øu ng∆∞·ª£c l·∫°i (ƒë√£ t·ª´ng li√™n h·ªá).
- Gi·ªõi h·∫°n gi√° tr·ªã `campaign ‚â§ 10` ƒë·ªÉ tr√°nh outlier c·ª±c l·ªõn (b·∫°n t·ª´ng ph√°t hi·ªán c√≥ ng∆∞·ªùi b·ªã g·ªçi >50 l·∫ßn).



```python
df = df.withColumn('y_binary', F.when(F.col('y') == 'yes', 1).otherwise(0))

df = df.withColumn('has_contact_before', (F.col('pdays') != 999).cast('int'))

df = df.withColumn('campaign_capped', F.when(F.col('campaign') > 10, 10).otherwise(F.col('campaign')))
```

#### 4.1.b ‚Äì M√£ h√≥a bi·∫øn ph√¢n lo·∫°i

**M·ª•c ti√™u:**

- Chuy·ªÉn c√°c bi·∫øn d·∫°ng ch·ªØ (string) sang d·∫°ng s·ªë ƒë·ªÉ m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c.  
- Trong dataset b·∫°n c√≥ ba bi·∫øn ph√¢n lo·∫°i ch√≠nh:
  - `contact`
  - `month`
  - `day_of_week`



```python
from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline

cat_cols = ["contact", "month", "day_of_week"]

# T·∫°o c√°c b∆∞·ªõc index v√† encode cho t·ª´ng bi·∫øn
indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_index", handleInvalid="keep") for c in cat_cols]
encoders = [OneHotEncoder(inputCol=f"{c}_index", outputCol=f"{c}_vec") for c in cat_cols]

pipeline = Pipeline(stages=indexers + encoders)
df_encoded = pipeline.fit(df).transform(df)

df_encoded.select("contact", "contact_index", "contact_vec", 
                  "month", "month_index", "month_vec").show(5, truncate=False)
```

    +---------+-------------+-------------+-----+-----------+--------------+
    |contact  |contact_index|contact_vec  |month|month_index|month_vec     |
    +---------+-------------+-------------+-----+-----------+--------------+
    |telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|
    |telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|
    |telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|
    |telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|
    |telephone|1.0          |(2,[1],[1.0])|may  |0.0        |(10,[0],[1.0])|
    +---------+-------------+-------------+-----+-----------+--------------+
    only showing top 5 rows
    
    

#### V√¨ sao d√πng StringIndexer + OneHotEncoder

**Gi·∫£i th√≠ch:**

- C√°c m√¥ h√¨nh nh∆∞ **Logistic Regression**, **MLP** c·∫ßn d·ªØ li·ªáu d·∫°ng **vector s·ªë** ‚Äî m·ªói c·ªôt ƒë·∫°i di·ªán cho m·ªôt ƒë·∫∑c tr∆∞ng ri√™ng.  
- D·ªØ li·ªáu ch·ªâ c√≥ √≠t gi√° tr·ªã ph√¢n lo·∫°i (`contact`, `month`, `day_of_week`), n√™n **One-Hot Encoding** kh√¥ng g√¢y tƒÉng chi·ªÅu d·ªØ li·ªáu qu√° m·ª©c.  
- `StringIndexer` bi·∫øn chu·ªói th√†nh m√£ s·ªë ƒë·ªÉ `OneHotEncoder` hi·ªÉu ƒë∆∞·ª£c, gi√∫p pipeline **·ªïn ƒë·ªãnh v√† t√°i s·ª≠ d·ª•ng cho d·ªØ li·ªáu m·ªõi**.  
- N·∫øu ch·ªâ d√πng **Label Encoding**, m√¥ h√¨nh s·∫Ω hi·ªÉu sai r·∫±ng c√°c gi√° tr·ªã c√≥ **th·ª© t·ª±**, l√†m sai √Ω nghƒ©a c·ªßa bi·∫øn.
---

#### 4.1.c ‚Äì Chia d·ªØ li·ªáu Train / Validation / Test

**M·ª•c ti√™u:**

T√°ch d·ªØ li·ªáu th√†nh 3 ph·∫ßn ƒë·ªÉ:

- **Train m√¥ h√¨nh (70%)**  
- **Validation** ƒë·ªÉ tinh ch·ªânh tham s·ªë (**15%**)  
- **Test** ƒë·ªÉ ƒë√°nh gi√° cu·ªëi c√πng (**15%**)



```python
from pyspark.sql.window import Window

w = Window.partitionBy('y_binary').orderBy(F.rand(seed = 42))
print(w)
df_strat = (
    df_encoded.withColumn('pr', F.percent_rank().over(w))
)

#Chia ty le
train_df = df_strat.filter(F.col('pr') < 0.7).drop('pr')
val_df = df_strat.filter((F.col('pr') >= 0.7) & (F.col('pr') < 0.85)).drop('pr')
test_df = df_strat.filter(F.col('pr') >= 0.85).drop('pr')

# Ki·ªÉm tra quy m√¥ & ph√¢n b·ªë nh√£n
print("Counts:", train_df.count(), val_df.count(), test_df.count())

for name, subset in [("Train", train_df), ("Validation", val_df), ("Test", test_df)]:
    print(f"\n{name} label distribution:")
    total = subset.count()
    subset.groupBy("y_binary").agg(
        F.count('*').alias('count')
    ).withColumn('ratio (%)', F.round(F.col('count')/total * 100, 2)).show()
```

    <pyspark.sql.window.WindowSpec object at 0x000001627647FCA0>
    Counts: 28831 6178 6179
    
    Train label distribution:
    +--------+-----+---------+
    |y_binary|count|ratio (%)|
    +--------+-----+---------+
    |       0|25583|    88.73|
    |       1| 3248|    11.27|
    +--------+-----+---------+
    
    
    Validation label distribution:
    +--------+-----+---------+
    |y_binary|count|ratio (%)|
    +--------+-----+---------+
    |       0| 5482|    88.73|
    |       1|  696|    11.27|
    +--------+-----+---------+
    
    
    Test label distribution:
    +--------+-----+---------+
    |y_binary|count|ratio (%)|
    +--------+-----+---------+
    |       0| 5483|    88.74|
    |       1|  696|    11.26|
    +--------+-----+---------+
    
    

#### L√Ω do ch·ªçn ph∆∞∆°ng ph√°p chia d·ªØ li·ªáu

**Gi·∫£i th√≠ch:**

- B√†i to√°n s·ª≠ d·ª•ng **Stratified Hold-out (70/15/15)** ƒë·ªÉ ƒë·∫£m b·∫£o t·ª∑ l·ªá nh√£n `yes/no` ƒë∆∞·ª£c gi·ªØ ·ªïn ƒë·ªãnh trong t·ª´ng t·∫≠p **Train**, **Validation** v√† **Test**.  
- Ph∆∞∆°ng ph√°p n√†y ph√π h·ª£p v√¨ **d·ªØ li·ªáu ƒë·ªß l·ªõn (~41k m·∫´u, t·ª∑ l·ªá yes ‚âà 11%)**, gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c xu h∆∞·ªõng t·ªïng qu√°t m√† kh√¥ng c·∫ßn l·∫∑p nhi·ªÅu l·∫ßn nh∆∞ **K-fold**.

**∆Øu ƒëi·ªÉm c·ªßa Stratified Hold-out:**

- Nhanh, d·ªÖ tri·ªÉn khai trong **PySpark**.  
- Gi·ªØ ƒë√∫ng ph√¢n ph·ªëi nh√£n cho b√†i to√°n **m·∫•t c√¢n b·∫±ng**.  
- H·∫°n ch·∫ø ƒë∆∞·ª£c **chi ph√≠ t√≠nh to√°n** so v·ªõi K-fold, v·ªën ph·∫£i hu·∫•n luy·ªán m√¥ h√¨nh nhi·ªÅu l·∫ßn.
---


#### 4.1.d ‚Äì Class weighting (m·∫∑c ƒë·ªãnh) + tu·ª≥ ch·ªçn undersampling

**M·ª•c ti√™u:**

Gi·∫£m thi√™n l·ªách do `yes ‚âà 11%` nh∆∞ng kh√¥ng l√†m m·∫•t d·ªØ li·ªáu quan tr·ªçng.  
Chi·∫øn l∆∞·ª£c m·∫∑c ƒë·ªãnh: **g√°n tr·ªçng s·ªë theo l·ªõp** tr√™n t·∫≠p **train** (validation/test gi·ªØ nguy√™n ƒë·ªÉ ƒë√°nh gi√° c√¥ng b·∫±ng).

#### C√¥ng th·ª©c t·ªïng qu√°t cho tr·ªçng s·ªë c√¢n b·∫±ng

**C√¥ng th·ª©c:**

$$
w_i = \frac{N}{k \times n_i}
$$

**Trong ƒë√≥:**

- $w_i$: tr·ªçng s·ªë c·ªßa l·ªõp *i*  
- $N$: t·ªïng s·ªë m·∫´u trong t·∫≠p hu·∫•n luy·ªán  
- $n_i$: s·ªë m·∫´u thu·ªôc l·ªõp *i*  
- $k$: s·ªë l∆∞·ª£ng l·ªõp (v·ªõi b√†i to√°n nh·ªã ph√¢n, $k = 2$)

‚Üí Khi l·ªõp ‚Äúyes‚Äù chi·∫øm √≠t, $n_{\text{yes}}$ nh·ªè ‚Üí $w_{\text{yes}}$ l·ªõn h∆°n $w_{\text{no}}$.  
N√≥i c√°ch kh√°c, m·ªói l·ªói d·ª± ƒëo√°n sai c·ªßa l·ªõp thi·ªÉu s·ªë s·∫Ω b·ªã **ph·∫°t n·∫∑ng h∆°n** trong qu√° tr√¨nh t·ªëi ∆∞u h√†m m·∫•t m√°t,  
gi√∫p m√¥ h√¨nh h·ªçc **c√¢n b·∫±ng gi·ªØa hai l·ªõp** m√† kh√¥ng c·∫ßn thay ƒë·ªïi d·ªØ li·ªáu g·ªëc.



```python
cnt = {r['y_binary']: r['count'] for r in train_df.groupBy('y_binary').count().collect()}

n_pos = cnt.get(1, 0)
n_neg = cnt.get(0, 0)
n_tot = n_pos + n_neg

# Tr·ªçng s·ªë c√¢n b·∫±ng ki·ªÉu "balanced":
# w_pos = N / (2 * N_pos), w_neg = N / (2 * N_neg)
w_pos = float(n_tot) / (2.0 * n_pos)
w_neg = float(n_tot) / (2.0 * n_neg)

# G√°n tr·ªçng s·ªë cho TRAIN; Val/Test kh√¥ng g√°n ƒë·ªÉ ƒë√°nh gi√° trung th·ª±c
train_w = train_df.withColumn(
    "class_weight",
    F.when(F.col("y_binary") == 1, F.lit(w_pos)).otherwise(F.lit(w_neg))
)

train_w.groupBy("y_binary").agg(F.round(F.avg("class_weight"),4).alias("avg_w")).orderBy("y_binary").show()

```

    +--------+------+
    |y_binary| avg_w|
    +--------+------+
    |       0|0.5635|
    |       1|4.4383|
    +--------+------+
    
    

#### Gi·∫£i th√≠ch l·ª±a ch·ªçn ph∆∞∆°ng ph√°p class weighting

**L√Ω do:**

- T·∫≠p d·ªØ li·ªáu ƒë·ªß l·ªõn, **kh√¥ng c·∫ßn nh√¢n b·∫£n m·∫´u thi·ªÉu s·ªë**.  
- Nhi·ªÅu bi·∫øn ph√¢n lo·∫°i n√™n **SMOTE kh√¥ng ph√π h·ª£p**.  
- C√°c m√¥ h√¨nh **Logistic Regression**, **CatBoost** v√† **MLP** ƒë·ªÅu h·ªó tr·ª£ weighting tr·ª±c ti·∫øp.  
- Gi·ªØ nguy√™n to√†n b·ªô d·ªØ li·ªáu th·∫≠t, gi√∫p m√¥ h√¨nh h·ªçc **c√¢n b·∫±ng v√† ·ªïn ƒë·ªãnh h∆°n**.

**K·∫øt lu·∫≠n:**

S·ª≠ d·ª•ng **class weighting** gi√∫p duy tr√¨ to√†n b·ªô d·ªØ li·ªáu g·ªëc, gi·∫£m thi√™n l·ªách,  
v√† ph√π h·ª£p v·ªõi c·∫£ ba m√¥ h√¨nh hu·∫•n luy·ªán ch√≠nh c·ªßa nh√≥m.

---



#### 4.1.e ‚Äì Assemble features

**M·ª•c ti√™u:**  
Gom to√†n b·ªô c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o (bao g·ªìm c·∫£ d·ªØ li·ªáu **s·ªë** v√† **one-hot encoding**) th√†nh **m·ªôt c·ªôt vector duy nh·∫•t** c√≥ t√™n `features`, ƒë·ªÉ s·∫µn s√†ng ƒë∆∞a v√†o giai ƒëo·∫°n hu·∫•n luy·ªán m√¥ h√¨nh.

**L∆∞u √Ω:**  
Kh√¥ng bao g·ªìm tr∆∞·ªùng **`duration`** trong qu√° tr√¨nh t·ªïng h·ª£p ‚Äî nh·∫±m **tr√°nh r√≤ r·ªâ th√¥ng tin** li√™n quan ƒë·∫øn k·∫øt qu·∫£ sau khi cu·ªôc g·ªçi ƒë√£ k·∫øt th√∫c.



```python
from pyspark.ml.feature import VectorAssembler, StandardScaler

# 1Ô∏è‚É£ Gom ƒë·∫∑c tr∆∞ng
feature_cols = ['previous', 'has_contact_before', 'campaign_capped',
                'contact_vec', 'month_vec', 'day_of_week_vec']

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol='features_unscaled'
)

# 2Ô∏è‚É£ Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng
scaler = StandardScaler(
    inputCol='features_unscaled',
    outputCol='features',
    withMean=True,    # tr·ª´ trung b√¨nh
    withStd=True      # chia ƒë·ªô l·ªách chu·∫©n
)

# 3Ô∏è‚É£ Fit-transform tr√™n TRAIN
from pyspark.ml import Pipeline
pipeline_scale = Pipeline(stages=[assembler, scaler])

train_ready = pipeline_scale.fit(train_w).transform(train_w).select('features', 'y_binary', 'class_weight')
val_ready   = pipeline_scale.fit(train_w).transform(val_df).select('features', 'y_binary')
test_ready  = pipeline_scale.fit(train_w).transform(test_df).select('features', 'y_binary')

train_ready.cache()
val_ready.cache()
test_ready.cache()

```




    DataFrame[features: vector, y_binary: int]



### 5 Hu·∫•n luy·ªán m√¥ h√¨nh 


#### 5.1.a ‚Äì C·∫•u h√¨nh & Train m√¥ h√¨nh (d√πng `class_weight`)

**M·ª•c ti√™u:**  
Hu·∫•n luy·ªán m√¥ h√¨nh **baseline ƒë·∫ßu ti√™n** tr√™n t·∫≠p d·ªØ li·ªáu `train_ready` ƒë·ªÉ l√†m **m·ªëc so s√°nh** v·ªõi c√°c m√¥ h√¨nh sau.


```python
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(
    featuresCol="features",
    labelCol="y_binary",
    weightCol="class_weight",   # s·ª≠ d·ª•ng tr·ªçng s·ªë t·ª´ 4.1d
    predictionCol="prediction",
    probabilityCol="probability",
    rawPredictionCol="rawPrediction",
    family="binomial",
    elasticNetParam=0.0,        # L2
    regParam=0.01,
    maxIter=100,
    standardization=True
)

lr_model = lr.fit(train_ready)


# D·ª± ƒëo√°n tr√™n t·∫≠p validation ƒë·ªÉ chu·∫©n b·ªã ƒë√°nh gi√° ·ªü b∆∞·ªõc 5.1.b
val_pred = lr_model.transform(val_ready).select("rawPrediction","probability","prediction","y_binary")
val_pred.show(5, truncate=False)

```

    +----------------------------------------+----------------------------------------+----------+--------+
    |rawPrediction                           |probability                             |prediction|y_binary|
    +----------------------------------------+----------------------------------------+----------+--------+
    |[-2.260313747331963,2.260313747331963]  |[0.09446352753522903,0.905536472464771] |1.0       |1       |
    |[0.1530807568359773,-0.1530807568359773]|[0.5381956295230149,0.4618043704769851] |0.0       |1       |
    |[-1.1563470484992502,1.1563470484992502]|[0.23933167899981153,0.7606683210001884]|1.0       |1       |
    |[-0.8614046541319255,0.8614046541319255]|[0.29704595671759115,0.7029540432824088]|1.0       |1       |
    |[-2.0081656447688454,2.0081656447688454]|[0.11834824403131385,0.8816517559686862]|1.0       |1       |
    +----------------------------------------+----------------------------------------+----------+--------+
    only showing top 5 rows
    
    


```python
from pyspark.sql import functions as F
from pyspark.ml.evaluation import BinaryClassificationEvaluator

def evaluation(pred_data):

    cm = (
        pred_data
        .groupBy("y_binary", "prediction")
        .count()
        .orderBy("y_binary", "prediction")
    )

    cm.show()

    # 2) T√≠nh TP, FP, TN, FN v√† c√°c metric c∆° b·∫£n
    agg_row = (
        pred_data
        .withColumn("tp", F.when((F.col("prediction")==1) & (F.col("y_binary")==1), 1).otherwise(0))
        .withColumn("fp", F.when((F.col("prediction")==1) & (F.col("y_binary")==0), 1).otherwise(0))
        .withColumn("tn", F.when((F.col("prediction")==0) & (F.col("y_binary")==0), 1).otherwise(0))
        .withColumn("fn", F.when((F.col("prediction")==0) & (F.col("y_binary")==1), 1).otherwise(0))
        .agg(F.sum("tp").alias("tp"),
            F.sum("fp").alias("fp"),
            F.sum("tn").alias("tn"),
            F.sum("fn").alias("fn"),
            F.count("*").alias("n"))
        .collect()[0]
    )

    tp, fp, tn, fn, n = [agg_row[x] for x in ["tp","fp","tn","fn","n"]]

    accuracy  = (tp + tn) / n if n else 0.0
    precision = tp / (tp + fp) if (tp + fp) else 0.0
    recall    = tp / (tp + fn) if (tp + fn) else 0.0
    f1        = 2*precision*recall/(precision+recall) if (precision+recall) else 0.0
    pos_rate  = (tp + fn) / n if n else 0.0  # t·ª∑ l·ªá d∆∞∆°ng th·∫≠t (m·∫•t c√¢n b·∫±ng)

    print(f"Accuracy : {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall   : {recall:.4f}")
    print(f"F1-score : {f1:.4f}")
    print(f"Positive rate (true +): {pos_rate:.4f}")

    # 3) ROC-AUC v√† PR-AUC (d√πng rawPrediction)
    e_roc = BinaryClassificationEvaluator(
        labelCol="y_binary",
        rawPredictionCol="rawPrediction",
        metricName="areaUnderROC"
    )
    e_pr  = BinaryClassificationEvaluator(
        labelCol="y_binary",
        rawPredictionCol="rawPrediction",
        metricName="areaUnderPR"
    )

    roc_auc = e_roc.evaluate(val_pred)
    pr_auc  = e_pr.evaluate(val_pred)

    print(f"ROC-AUC : {roc_auc:.4f}")
    print(f"PR-AUC  : {pr_auc:.4f}")

evaluation(val_pred)


```

    +--------+----------+-----+
    |y_binary|prediction|count|
    +--------+----------+-----+
    |       0|       0.0| 4790|
    |       0|       1.0|  692|
    |       1|       0.0|  354|
    |       1|       1.0|  342|
    +--------+----------+-----+
    
    Accuracy : 0.8307
    Precision: 0.3308
    Recall   : 0.4914
    F1-score : 0.3954
    Positive rate (true +): 0.1127
    ROC-AUC : 0.7315
    PR-AUC  : 0.3740
    

#### 5.1.b ‚Äì T√¨m si√™u tham s·ªë v√† t·ªëi ∆∞u ng∆∞·ª°ng d·ª± ƒëo√°n (threshold) theo F1 tr√™n validation

**M·ª•c ti√™u:**  
M√¥ h√¨nh **Logistic Regression** m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng ng∆∞·ª°ng **0.5** ƒë·ªÉ ph√¢n lo·∫°i, tuy nhi√™n v·ªõi t·∫≠p d·ªØ li·ªáu c√≥ t·ª∑ l·ªá l·ªõp ‚Äúyes‚Äù th·∫•p (~11%), ng∆∞·ª°ng n√†y th∆∞·ªùng **thi·∫øu nh·∫°y** (recall th·∫•p).  
Do ƒë√≥, c·∫ßn **qu√©t qua nhi·ªÅu gi√° tr·ªã ng∆∞·ª°ng** (v√≠ d·ª•: t·ª´ 0.1 ƒë·∫øn 0.9), sau ƒë√≥:

1. T√≠nh **Precision**, **Recall** v√† **F1-score** cho t·ª´ng ng∆∞·ª°ng.  
2. Ch·ªçn **ng∆∞·ª°ng c√≥ F1 cao nh·∫•t** l√†m gi√° tr·ªã t·ªëi ∆∞u.


```python
from itertools import product
from pyspark.sql import functions as F
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.functions import vector_to_array

# Grid nh·ªè, thi√™n v·ªÅ b·ªõt ‚Äúph·∫°t‚Äù ƒë·ªÉ tƒÉng recall
reg_list = [0.0005, 0.001, 0.01, 0.05]
en_list  = [0.0, 0.5, 1.0]     # L2 / mix / L1
mi_list  = [50, 100]

def max_f1_on_validation(model):
    # t√≠nh p1
    val_scored = (model.transform(val_ready)
                        .select("y_binary", vector_to_array("probability").getItem(1).alias("p1"))
                        .cache())
    # qu√©t ng∆∞·ª°ng
    best = {"f1": -1}
    for t in [round(0.02 + 0.01*i, 2) for i in range(80)]:  # 0.02..0.81
        pred = val_scored.select("y_binary", (F.col("p1") >= F.lit(t)).cast("int").alias("pred"))
        a = pred.agg(
            F.sum(F.when((F.col("pred")==1) & (F.col("y_binary")==1), 1).otherwise(0)).alias("tp"),
            F.sum(F.when((F.col("pred")==1) & (F.col("y_binary")==0), 1).otherwise(0)).alias("fp"),
            F.sum(F.when((F.col("pred")==0) & (F.col("y_binary")==0), 1).otherwise(0)).alias("tn"),
            F.sum(F.when((F.col("pred")==0) & (F.col("y_binary")==1), 1).otherwise(0)).alias("fn")
        ).first()
        tp, fp, tn, fn = a.tp, a.fp, a.tn, a.fn
        P = tp/(tp+fp) if (tp+fp) else 0.0
        R = tp/(tp+fn) if (tp+fn) else 0.0
        F1 = 2*P*R/(P+R) if (P+R) else 0.0
        if F1 > best["f1"]:
            best = {"thr": t, "f1": F1, "prec": P, "rec": R}
    return best

best = None
for rp, en, mi in product(reg_list, en_list, mi_list):
    lr = LogisticRegression(
        featuresCol="features", labelCol="y_binary", weightCol="class_weight",
        family="binomial", standardization=True,
        regParam=rp, elasticNetParam=en, maxIter=mi
    )
    m = lr.fit(train_ready)
    s = max_f1_on_validation(m)
    if (best is None) or (s["f1"] > best["f1"]):
        best = {"regParam": rp, "elasticNetParam": en, "maxIter": mi, **s, "model": m}

print("Best-by-F1 params:",
      {"regParam":best["regParam"], "elasticNetParam":best["elasticNetParam"],
       "maxIter":best["maxIter"], "thr":best["thr"]})
print(f"VAL @T={best['thr']:.2f} -> F1={best['f1']:.4f} | P={best['prec']:.4f} | R={best['rec']:.4f}")

```

#### 5.1.c ‚Äì K·∫øt qu·∫£ tr√™n t·∫≠p test



```python
from pyspark.ml.classification import LogisticRegression

lr2 = LogisticRegression(
    featuresCol="features",
    labelCol="y_binary",
    weightCol="class_weight",
    predictionCol="prediction",
    probabilityCol="probability",
    rawPredictionCol="rawPrediction",
    family="binomial",
    elasticNetParam=1.0,  
    regParam=0.01,
    maxIter=50,
    threshold=0.51,       # ƒë√∫ng t√™n tham s·ªë
    standardization=True
)


lr2_model = lr2.fit(train_ready)


# D·ª± ƒëo√°n tr√™n t·∫≠p validation ƒë·ªÉ chu·∫©n b·ªã ƒë√°nh gi√° ·ªü b∆∞·ªõc 5.1.b
test_pred = lr_model.transform(test_ready).select("rawPrediction","probability","prediction","y_binary")
test_pred.show(5, truncate=False)

evaluation(val_pred)

```

    +------------------------------------------+-----------------------------------------+----------+--------+
    |rawPrediction                             |probability                              |prediction|y_binary|
    +------------------------------------------+-----------------------------------------+----------+--------+
    |[-2.2837631534996956,2.2837631534996956]  |[0.09247664682062362,0.9075233531793764] |1.0       |1       |
    |[1.2387586415065526,-1.2387586415065526]  |[0.7753478639442255,0.2246521360557745]  |0.0       |1       |
    |[0.05602751008121537,-0.05602751008121537]|[0.5140032146088256,0.4859967853911744]  |0.0       |1       |
    |[-3.4337992113305296,3.4337992113305296]  |[0.031255691700562804,0.9687443082994371]|1.0       |1       |
    |[-3.8138147904097583,3.8138147904097583]  |[0.021587544912117226,0.9784124550878828]|1.0       |1       |
    +------------------------------------------+-----------------------------------------+----------+--------+
    only showing top 5 rows
    
    +--------+----------+-----+
    |y_binary|prediction|count|
    +--------+----------+-----+
    |       0|       0.0| 4790|
    |       0|       1.0|  692|
    |       1|       0.0|  354|
    |       1|       1.0|  342|
    +--------+----------+-----+
    
    Accuracy : 0.8307
    Precision: 0.3308
    Recall   : 0.4914
    F1-score : 0.3954
    Positive rate (true +): 0.1127
    ROC-AUC : 0.7315
    PR-AUC  : 0.3740
    


    The Kernel crashed while executing code in the current cell or a previous cell. 
    

    Please review the code in the cell(s) to identify a possible cause of the failure. 
    

    Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. 
    

    View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.

