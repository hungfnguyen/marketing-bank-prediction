{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Customer Profiles (Hung)\n",
    "Baseline: PySpark Logistic Regression (khong dung 'duration' de tranh leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pathlib import Path\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 16:42:44 WARN Utils: Your hostname, hung resolves to a loopback address: 127.0.1.1; using 192.168.1.29 instead (on interface wlp0s20f3)\n",
      "25/10/15 16:42:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/15 16:42:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('bdml_baseline_hung').getOrCreate()\n",
    "# Tim duong dan tuyet doi toi file du lieu bat ke CWD hien tai la gi\n",
    "rel = Path('data/bank-additional/bank-additional-full.csv')\n",
    "data_path = None\n",
    "# Uu tien PROJECT_ROOT neu duoc cai dat\n",
    "if os.environ.get('PROJECT_ROOT'):\n",
    "    cand = Path(os.environ['PROJECT_ROOT']) / rel\n",
    "    if cand.exists():\n",
    "        data_path = str(cand.resolve())\n",
    "# Thu lan luot CWD va cac thu muc cha\n",
    "if data_path is None:\n",
    "    cwd = Path.cwd()\n",
    "    for base in [cwd] + list(cwd.parents):\n",
    "        cand = base / rel\n",
    "        if cand.exists():\n",
    "            data_path = str(cand.resolve())\n",
    "            break\n",
    "if data_path is None:\n",
    "    raise FileNotFoundError(f'Khong tim thay {rel} tu {Path.cwd()} â€” vui long kiem tra cau truc thu muc')\n",
    "\n",
    "df = spark.read.csv(data_path, header=True, sep=';', inferSchema=True)\n",
    "# Tao nhan 'label' va chi dung dac diem khach hang (personal)\n",
    "df = df.withColumn('label', when(col('y')=='yes', lit(1.0)).otherwise(lit(0.0)))\n",
    "cols_cat = ['job','marital','education','default','housing','loan']\n",
    "cols_num = ['age']\n",
    "# Tranh leakage: khong dua 'duration' vao features\n",
    "selected = cols_num + cols_cat + ['label']\n",
    "df = df.select(*selected).dropna()\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "pos = train.filter(col('label')==1.0).count(); neg = train.filter(col('label')==0.0).count(); tot = pos+neg\n",
    "w_pos = float(tot)/(2.0*pos) if pos>0 else 1.0; w_neg = float(tot)/(2.0*neg) if neg>0 else 1.0\n",
    "train = train.withColumn('weight', when(col('label')==1.0, lit(w_pos)).otherwise(lit(w_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexers = [StringIndexer(inputCol=c, outputCol=f'{c}_idx', handleInvalid='keep') for c in cols_cat]\n",
    "enc = OneHotEncoder(inputCols=[f'{c}_idx' for c in cols_cat], outputCols=[f'{c}_ohe' for c in cols_cat])\n",
    "assembler = VectorAssembler(inputCols=cols_num + [f'{c}_ohe' for c in cols_cat], outputCol='features')\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', weightCol='weight', maxIter=50, regParam=0.01, elasticNetParam=0.0)\n",
    "pipeline = Pipeline(stages=indexers + [enc, assembler, lr])\n",
    "model = pipeline.fit(train)\n",
    "pred = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.6584022402684366\n",
      "AUC-PR: 0.19510756061821388\n",
      "F1: 0.6665755868454523\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0| 4266|\n",
      "|  0.0|       1.0| 3060|\n",
      "|  1.0|       0.0|  320|\n",
      "|  1.0|       1.0|  532|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_roc = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "e_pr = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderPR')\n",
    "e_f1 = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "roc = e_roc.evaluate(pred); pr = e_pr.evaluate(pred); f1 = e_f1.evaluate(pred)\n",
    "cm = pred.groupBy('label','prediction').count().orderBy('label','prediction')\n",
    "print('AUC-ROC:', roc); print('AUC-PR:', pr); print('F1:', f1); cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence: 0.10418195157740279\n",
      "Best threshold by F1: {'threshold': 0.5, 'precision': 0.14810690423162584, 'recall': 0.6244131455399061, 'f1': 0.23942394239423942}\n",
      "Saved metrics and model to /home/hungfnguyen/Documents/1-bigdata-ml/marketing-bank-prediction/artifacts/\n"
     ]
    }
   ],
   "source": [
    "# Xac dinh PROJECT_ROOT de luu artifacts dung thu muc goc repo\n",
    "proj = None\n",
    "if os.environ.get('PROJECT_ROOT') and Path(os.environ['PROJECT_ROOT']).exists():\n",
    "    proj = Path(os.environ['PROJECT_ROOT'])\n",
    "if proj is None:\n",
    "    cwd = Path.cwd()\n",
    "    for base in [cwd] + list(cwd.parents):\n",
    "        if (base/'AGENTS.md').exists() or (base/'README.md').exists():\n",
    "            proj = base; break\n",
    "if proj is None:\n",
    "    proj = Path.cwd()\n",
    "artifacts_dir = proj / 'artifacts'\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tinh prevalence va quet cac nguong xac suat cho lop duong\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "get_p1 = udf(lambda v: float(v[1]), DoubleType())\n",
    "pp = pred.withColumn('p1', get_p1(col('probability')))\n",
    "total = pred.count()\n",
    "pos = pred.filter(col('label')==1.0).count()\n",
    "prevalence = (pos/total) if total else 0.0\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "best = {'threshold': None, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "for t in thresholds:\n",
    "    m = pp.withColumn('hat', (col('p1') >= lit(float(t))).cast('double'))\n",
    "    tp = m.filter((col('label')==1.0) & (col('hat')==1.0)).count()\n",
    "    fp = m.filter((col('label')==0.0) & (col('hat')==1.0)).count()\n",
    "    fn = m.filter((col('label')==1.0) & (col('hat')==0.0)).count()\n",
    "    prec = (tp/(tp+fp)) if (tp+fp)>0 else 0.0\n",
    "    rec = (tp/(tp+fn)) if (tp+fn)>0 else 0.0\n",
    "    f1_t = (2*prec*rec/(prec+rec)) if (prec+rec)>0 else 0.0\n",
    "    if f1_t > best['f1']:\n",
    "        best = {'threshold': float(t), 'precision': float(prec), 'recall': float(rec), 'f1': float(f1_t)}\n",
    "print('Prevalence:', prevalence)\n",
    "print('Best threshold by F1:', best)\n",
    "\n",
    "# Ghi metrics va model\n",
    "metrics = {\n",
    "    'auc_roc': float(roc),\n",
    "    'auc_pr': float(pr),\n",
    "    'f1_macro': float(f1),\n",
    "    'class_prevalence': float(prevalence),\n",
    "    'best_threshold_by_f1': best,\n",
    "    'w_pos': float(w_pos),\n",
    "    'w_neg': float(w_neg)\n",
    "}\n",
    "with open(artifacts_dir / 'hung_lr_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "model.write().overwrite().save(str(artifacts_dir / 'hung_lr_model'))\n",
    "print(f'Saved metrics and model to {artifacts_dir}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54361c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
